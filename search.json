[
 {
  "title": "Advanced SQL in Starburst",
  "url": "/videos/2020-07-29-advanced-presto-sql.html",
  "content": "# {{ page.title }}Hosts: {{ page.participants }}Video date: {{ page.date }} Running time: {{ page.length}}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping users understand how to run morecomplex and comprehensive SQL queries with {{site.terms.sb}}. Delivered by DavidPhillips, this session covers the following topics:* Using JSON and other complex data types* Advanced aggregation techniques* Window functions* Array and map functions* Lambda expressions* Many other SQL functions and features## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.* Welcome - 0:00* General SQL Features - [9:45](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=585s)   * Format function - [9:57](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=597s)   * Case expressions - [12:48](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=768s)   * Searched case expression - [13:34](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=814s)   * IF expression - [14:24](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=864s)   * TRY expression - [15:26](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=926s)   * Lambda expression overview[](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1061s)* Using JSON - [18:36](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1116s)   * JSON data type - [19:13](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1153s)   * Extraction using JSONPath - [23:11](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1391s)   * Casting from JSON - [26:01](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1561s)   * Parting casting from JSON - [27:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1627s)   * Formatting as JSON - [29:40](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1780s)* Advanced Aggregation Techniques - [32:50](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1970s)   * Counting distinct items - [33:06](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1986s)   * Approximate percentiles - [36:02](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2162s)   * Associated max value - [37:59](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2279s)   * Associated max value using a row type - [38:39](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2319s)   * Pivoting with conditional counting - [40:58](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2458s)   * Pivoting with filtering - [42:08](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2528s)   * Pivoting averages - [42:57](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2577s)   * Aggregating a complex expression - [43:55](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2635s)   * Aggregating into an array - [45:36](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2736s)   * Aggregating into a lambda - [46:28](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2788s)   * Order-insensitive checksums - [50:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3007s)   * ROLLUP with single - [52:32](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3152s)   * ROLLUP with multiple - [53:24](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3204s)   * CUBE - [55:52](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3352s)   * GROUPING SETS - [59:28](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3568s)* Array and Map Functions - [1:07:47](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4067s)   * Creating arrays - [1:08:40](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4120s)   * Accessing array elements - [1:09:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4147s)   * Sorting arrays - [1:11:25](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4285s)   * Matching elements - [1:13:26](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4406s)   * Filtering elements - [1:14:57](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4497s)   * Transforming elements - [1:16:25](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4585s)   * Converting arrays to strings - [1:17:44](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4664s)   * Computing array product - [1:19:45](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4785s)   * Unnesting an array - [1:22:16](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4936s)   * Unnesting an array with ordinality - [1:23:06](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4986s)   * Creating maps - [1:23:44](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5024s)   * Accessing map elements - [1:25:20](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5120s)   * Unnesting a map - [1:26:16](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5176s)* Window Functions - [1:28:00](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5280s)   * Window function overview - [1:28:58](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5338s)   * Row numbering - [1:30:04](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5404s)   * Row numbering order - [1:30:49](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5449s)   * Row numbering with limit - [1:31:39](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5499s)   * Rank - [1:32:30](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5550s)   * Rank with ties - [1:33:11](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5591s)   * Dense rank with ties - [1:33:54](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5634s)   * Ranking without ordering - [1:34:20](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5660s)   * Row numbering without ordering - [1:34:40](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5680s)   * Assigning rows to buckets - [1:36:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5767s)   * Percentage ranking - [1:37:12](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5832s)   * Partitioning - [1:37:53](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5873s)   * Partitioning on the same value - [1:39:26](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5966s)   * Accessing leading and trailing rows - [1:40:12](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6012s)   * Accessing leading and trailing rows with nulls - [1:42:56](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6176s)   * Accessing leading and trailing rows without nulls - [1:43:56](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6236s)   * Window frames - [1:45:13](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6313s)   * Accessing the first value - [1:47:05](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6425s)   * Accessing the last value - [1:47:35](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6455s)   * Accessing the Nth value - [1:47:49](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6469s)   * Window frame ROWS vs RANGE - [1:48:15](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6495s)   * Rolling and total sum - [1:50:39](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6639s)   * Partition sum - [1:52:00](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6720s)"
 },
 {
  "title": "Understanding and tuning Starburst query processing",
  "url": "/videos/2020-08-12-query-performance.html",
  "content": "# {{ page.title }}Hosts: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping users understand how{{site.terms.sb}} executes queries. That knowledge can help you improve queryperformance. For instance, the explain plan is a powerful tool. We explore howto access the explain plan and how to read it. We look at the work thecost-based optimizer performs and how you can potentially help run yourqueries even faster. Delivered by Martin Traverso, this session covers thefollowing topics:* Explain the EXPLAIN* Learn how queries are analyzed and executed* Understand what the optimizer does, including some of its limitations* Showcase the cost-based optimizer## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can skip tothat timestamp in the video player above.* Welcome - 0:00* Query lifecycle - [8:11](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=491s)    * Parsing - [9:44](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=584s)    * Analysis - [11:29](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=689s)    * Planning - [15:25](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=925s)    * Optimization - [16:44](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1004s)    * Scheduling and execution - [18:50](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1130s)* Explain the EXPLAIN - [20:18](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1218s)    * EXPLAIN command - [20:55](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1255s)    * EXPLAIN vs EXPLAIN ANALYZE - [23:07](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1387s)    * Fragment Structure - [24:06](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1446s)    * Distribution - [26:55](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1615s)    * Row layout - [29:37](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1777s)    * Estimates - [31:54](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1914s)    * Performance stats - [33:49](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2029s)    * Exchanges - [39:33](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2373s)* Optimization - [41:10](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2470s)    * Constant folding - [43:23](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2603s)    * Predicate pushdown - [51:07](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=3067s)    * Predicate pushdown into connectors - [55:43](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=3343s)    * Predicate pushdown into the Hive connector - [1:08:24](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4104s)    * Hive partition pruning - [1:10:18](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4218s)    * Hive bucket pruning - [1:17:12](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4632s)    * Row group skipping for ORC and Parquet - [1:19:17](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4757s)    * Limit pushdown - [1:22:31](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4951s)    * Partial limit pushdown - [1:26:11](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5171s)    * Aggregation pushdown - [1:28:26](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5306s)    * Skew - [1:33:41](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5621s)* Cost-based Optimizations - [1:39:01](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5941s)    * Partitioned join - [1:41:41](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6101s)    * Broadcast join - [1:44:01](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6241s)    * Join type selection - Partitioned - [1:46:02](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6362s)    * Join type selection - Broadcast - [1:47:07](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6427s)    * Disabling cost-based optimizations - [1:48:10](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6490s)    * Join reordering - [1:48:44](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6524s)    * Table statistics - [1:50:21](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6621s)    * Computing statistics - [1:51:19](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6679s)* Resources - [1:52:09](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6729s)"
 },
 {
  "title": "Securing Starburst Enterprise",
  "url": "/videos/2020-08-26-securely-deploy-presto.html",
  "content": "# {{ page.title }}Presenters: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping {{site.terms.sep_first}} userssecurely deploy {{site.terms.sep}} at scale. We cover how to secure{{site.terms.sep}} as well as access to your underlying data. Delivered by DainSundstrom, this session covers the following topics:* Authentication, including password &amp; LDAP Authentication* Authorization to access your data sources* Encryption including client-to-coordinator communication* Secure communication in the cluster* Secrets usage for configuration files including catalogs## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.* Welcome - 0:00* Tips and Notes - [5:06](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=306s)   * Process for securing {{site.terms.sep}} - [7:34](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=454s)   * What to secure - [11:02](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=662s)   * Verify HTTP with the Web UI - [13:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=803s)   * Verify HTTP with the CLI - [14:48](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=888s)      * CLI failures - [15:14](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=914s)   * Client to Server Encryption - [15:44](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=944s)      * Approaches for HTTPS - [15:58](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=958s)      * HTTPS proxy or load balancer - [17:33](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1053s)      * Add the SSL/TLS certificate to the coordinator - [20:28](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1228s)         * Inspect the PEM file - [22:40](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1360s)         * Verify the PEM file certificate - [23:45](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1425s)         * Verify the PEM private key - [26:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1568s)         * Verify the JKS file - [26:38](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1598s)         * Configure {{site.terms.sep}} - [27:59](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1679s)      * Verify HTTPS with the Web UI - [28:51](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1731s)      * Verify HTTPS with the CLI - [29:36](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1776s)         * CLI failures - [29:46](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1786s)         * CLI --insecure - [33:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2003s)   * Authentication - [34:57](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2097s)      * Password file authentication - [36:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2168s)      * LDAP Authentication - [41:19](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2479s)      * Kerberos Authentication - [50:24](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3024s)      * Client certificate authentication - [53:53](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3233s)      * JSON Web Token authentication - [55:03](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3303s)      * Multiple authenticators - [56:01](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3361s)      * User mapping - [58:14](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3494s)   * Authorization - [1:00:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3608s)      * File-based system access control - [1:02:54](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3774s)   * Client to server summary - [1:07:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4043s)   * Internal security and connector security - [1:18:14](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4694s)      * Securing the cluster itself - [1:18:30](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4710s)      * Shared secret - [1:20:29](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4829s)   * Internal HTTPS - [1:23:58](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5038s)   * Secrets Management - [1:27:53](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5273s)   * Management Endpoints - [1:30:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5423s)   * Hive Catalog Security - [1:33:29](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5609s)      * Hive Catalog Authorization - [1:34:45](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5685s)      * Hive Metastore Authentication - [1:38:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5888s)      * HDFS Authentication - [1:42:24](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6144s)      * Hive Kerberos Debugging - [1:43:31](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6211s)      * S3 Authentication - [1:45:53](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6353s)      * Google Cloud Authentication - [1:49:31](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6571s)"
 },
 {
  "title": "Starburst cluster sizing, and performance tuning",
  "url": "/videos/2020-09-09-cluster-sizing-and-performance-video.html",
  "content": "# {{ page.title }}Presenters: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping users tune and size theirdeployment for optimal performance. Delivered by Dain Sundstrom, this sessioncovers the following topics:* Cluster configuration and node sizing* Memory configuration and management* Improving task concurrency and worker scheduling* Tuning your JVM configuration* Investigating queries for join order and other criteria* Tuning the cost-based optimizer## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.* Welcome - 0:00* General Strategy - [5:45](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=345s)* Baseline Advice - [11:53](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=713s)* Cluster Sizing / CPU and Memory - [14:57](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=897s)* Machine Sizing - [34:46](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2086s)    * Memory - [34:58](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2098s)    * Memory Allocations - [39:55](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2395s)    * Shared Join Hash - [46:59](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2819s)    * Distributed Join - [49:11](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2951s)    * Skew - [50:58](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3058s)    * Use bigger machines - [55:03](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3303s)    * Machine Types - [58:40](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3520s)* Additional Thoughts - [1:03:20](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3800s)    * Hash join vs (sort) merge join - [1:03:25](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3805s)    * Spilling - [1:04:43](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3883s)    * Small Clusters - [1:07:54](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=4074s)* Tuning the Workload - [1:23:45](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5025s)    * Query Plan - [1:24:52](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5092s)    * Precomputing - [1:30:00](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5400s)    * Connectors - [1:34:53](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5693s)* Hive Data Organization - [1:39:27](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5967s)    * Organize the data for the Hive connector - [1:39:41](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5981s)    * Hive Partitioning - [1:42:13](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6133s)    * Hive bucketing - [1:43:59](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6239s)    * Orc and Parquet - [1:46:26](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6386s)    * File Size - [1:51:02](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6662s)    * Bad Parquet Files - [1:53:13](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6793s)    * Rewrite table with ORC writer - [1:54:29](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6869s)* Making Queries Faster - [1:55:37](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6937s)    * What to look for in a query - [1:57:09](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7029s)    * More hardware - [2:01:18](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7278s)    * Under-utilization - [2:02:32](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7352s)    * Hive Caching - [2:05:29](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7529s)* Sharing Resources / Resource Groups - [2:08:34](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7714s)"
 },
 {
  "title": "Privacera and Starburst - Enabling secure queries and consistent governance and compliance",
  "url": "/videos/2021-01-20-demos-privacera-starburst.html",
  "content": "# {{ page.title }}Hosts: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include youtubePlayer.html id=page.youtubeId %}Presented by Bill Brooks, Director of Solutions Engineering at Privacera, andClaudius Li, Product Manager for engine and security products at{{site.terms.sb}}. This webinar demonstrates how Privacera and {{site.terms.sb}}integrate to to build an enterprise-ready, elastic infrastructure at scale. Thissession covers the following topics:* {{site.terms.sb}} overview of compliance, governance, and secure queries* {{site.terms.sb}} demo* Privacera overview* Privacera demo## Detailed topics with timestampsClicking the timestamp links below takes you to YouTube, or you can skip tothat timestamp in the video player above.* Welcome - 0:00* {{site.terms.sb}} overview of compliance, governance, and secure queries - [1:04](https://youtu.be/z9XjKp1aXZo?t=64)* {{site.terms.sb}} demo - [7:55](https://youtu.be/z9XjKp1aXZo?t=474)* Privacera overview - [15:43](https://youtu.be/z9XjKp1aXZo?t=943)* Privacera demo - [20:55](https://youtu.be/z9XjKp1aXZo?t=1255)* Questions from the audience - [32:11](https://youtu.be/z9XjKp1aXZo?t=1931)"
 },
 {
  "title": "What Data Mesh Means for Data Analysts",
  "url": "/videos/2021-05-19-data-mesh-for-analysts.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;vhc0oa8rb2&#39; %}This video focuses on the increasingly popular data mesh approach to enterprisearchitecture which calls for distributed, domain-oriented ownership of data. Butwhat does data mesh actually mean for data analysts? Is the concept of datawarehouse at odds with the data mesh approach? Is there a paradigm shifthappening in the data architecture landscape?Our panelists answer these pressing questions in a discussion moderated by SeanZinsmeister, VP Product Marketing at ThoughtSpot. Panelists:* Zhamak Dehghani, Founder of Data Mesh* Daniel Abadi, Darnell-Kanal Professor of Computer Science at University of Maryland, College Park* Gareth Stevenson, Director, Senior Quantitative Analyst, Bank of America This fun and informative discussion is worth watching in its entirety."
 },
 {
  "title": "Mars - Enabling Profitable Growth by Leveraging Data and Analytics",
  "url": "/videos/2021-05-19-mars-data-analytics.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;kj2xv60u3y&#39; %}Mars, best known for its M&amp;M&#39;s and Snickers brands, is in the midst of a majordigital transformation with a very specific focus. In this session, Deepak Jose,Head of Business Strategy and Advanced Analytics at Mars, talks about puttingtheir data and analytics approach into practice, including designing the MarsDigital Engine, an effort broadly focused on leveraging data and analytics todrive growth. The talk provides a great inside look at how a large, storied brand isapproaching our data-driven future, and where data analysts fit. Deepak has somecool advice for those of you encountering colleagues with a stubborn traditionalmindset and why today’s data analysts need to stand their ground and support newtools if they want to deliver truly transformational results.  "
 },
 {
  "title": "SQL and Trino - The Golden Language Prevails",
  "url": "/videos/2021-05-19-sql-trino.html",
  "content": "# {{ page.title }}Presenters: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;3lc8nexqkz&#39; %}SQL remains the dominant language for analytics. SQL-based solutions like Trinohelp you gain fast access to data to make informed business decisions. If you’rerelatively new to this space and the value of distributed query engines, thissession has everything you need to get started.{{site.terms.sb}}’s Brian Luisi delivers an overview of why enterprises needsuch platforms in the first place and discusses how they work within largeorganizations. Tom Nats shows you a SQL and Trino demo. "
 },
 {
  "title": "The State of Data Analysts",
  "url": "/videos/2021-05-19-state-of-data-analysts.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;t97r90bswp&#39; %}Moderated by Andy Cotgreave, Technical Evangelist at Tableau and Co-Author of‘The Big Book of Dashboards,’ this session reviews how the role of the dataanalyst has evolved over time. The demands of modern analytics, the advent of AIand ML, and the emergence of SQL-based MPP query engines like Starburst havealtered the way analysts have traditionally operated.The panel features Charles Wilson, Senior Manager of Customer Insights andAnalytics at New Balance, Manav Gupta, Head of Analytics and Insights at AdobeStock, and Parker Dillon, a senior consultant at Slalom.The speakers make bold predictions about dashboard usage, examine whether AI isoverhyped, share advice for budding analysts, and even get into a somewhatphilosophical debate about whether data analysis is an art or science. Or is ita scientific art? The highlight of this discussion is the mix of participantsand perspectives, and it’s a must-watch for every data analyst."
 },
 {
  "title": "Lightning demo - Tableau and Starburst",
  "url": "/videos/2021-05-19-tableau-staburst-demo.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;yvgxn8b3fz&#39; %}Lightning demo of Tableau and {{site.terms.sb}} delivered by Ben Lumbert covershow to connect Tableau and {{site.terms.sb}}."
 },
 {
  "title": "Lightning demo - ThoughtSpot and Starburst",
  "url": "/videos/2021-05-19-thoughtspot-staburst-demo.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;3atxe3r3mo&#39; %}Lightning demo of ThoughtSpot and {{site.terms.sb}} delivered by ToddBeauchene covers how to connect ThoughtSpot and {{site.terms.sb}}."
 },
 {
  "title": "Accelerating data science with Trino",
  "url": "/videos/2021-07-14-accelerating-data-science-with-trino.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;keywshcupz&#39; %}This panel features Starburst Regional Manager, Brian Luisi, and Starburst CTOs,Dain Sundstrom and David Philips. They cover how Starburst and Trino assist inthe responsibilities of data scientists, including collecting data, interactingwith data, and making predictive models. They also discuss what data accessmeans for data scientists, Trino use cases, and best practices for datascientists. Data scientists need a data engine that can interact with all of their data andis able to quickly sift through it, which allows for easier profiling andexploration. Trino and Starburst allows data scientists to do this by providinga single point of access to large amounts of data. David describes Trino as“fast and distributed, so if you need to process data, you can process it wayfaster than you could on a Python script.” Trino’s SQL-based MPP query engineprovides great value to scientists looking to expedite their processing throughits ability to analyze large volumes of data, fast."
 },
 {
  "title": "Data lakehouse - a new architectural horizon",
  "url": "/videos/2021-07-14-data-lakehouse-new-horizon.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;vm18cdjbew&#39; %}A hot topic in the big data field, this discussion led by Paco Nathan, exploredthe pros and cons of the data lakehouse. Although the lakehouse combines a lotof the best aspects of a data warehouse and the best aspects of a data lake,there are some complications that come along with the adoption of the datalakehouse.The panelists, Adri Purkayasth from BNP Paribas, Anjali Samani from Salesforce,and Tom Nats of Starburst, debate what it means for organizations to implementthis data architecture including regulatory considerations, standardizations,and the struggles with blending old architecture with the new architecture.Anjali says, “To realize a lot of value out of your data science investments,you need access to alternative data sources and that’s where data lakehousearchitecture is very attractive.”"
 },
 {
  "title": "Demo - Assurance and Starburst",
  "url": "/videos/2021-07-14-demo-assurance-and-starburst.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;jv1k60ewlf&#39; %}Mitchell Poslums, Senior Data Scientist at Assurance, an online insurancedistribution platform, presents on how the implementation of Trino and Starburst“have really enabled [Assurance] to accelerate time to insights, improve ourconversion rates, and enable robust modeling all coalescing to achieve betterbusiness outcomes.”In order to provide the best service to their customer, Assurance needs the bestout of their data. For all technology businesses, insights can’t be achievedwithout accurate, timely data. With Starburst and Trino, Assurance was able tosolve their blockers to insights and effectively provided the best solutions totheir customers."
 },
 {
  "title": "Demo - Red Hat OpenShift data science",
  "url": "/videos/2021-07-14-demo-redhat-openshift-data-science.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;s0xu9bh93m&#39; %}Red Hat launched a new product, Red Hat OpenShift Data Science, which givesdata scientists the ability to produce production-ready models as quickly aspossible. It also easily supports a data mesh paradigm.Through a video demonstration, Karl Eklund, Principal Architect at Red Hat,shows how this tool provides a central location to explore data and buildmodels. There is also no vendor lock-in, so users can use other tools, likeStarburst, to better leverage this new technology. With these technologies,“data scientists have everything they need to be successful in consuming data,building models, and then deploying and monitoring them.”"
 },
 {
  "title": "The intelligent edge",
  "url": "/videos/2021-07-14-intelligent-edge.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;ex099yj2tt&#39; %}An insightful session on the ‘Intelligent Edge’ by leading data scientist, KirkBorne. With metaphors from Formula 1 racing and quotes from Yogi Berra, heexplains how the ability to use the intelligent edge and sensory data can allowdata scientists to make better analytical decisions.Data is at the forefront of everything: “Innovations are inspired by data,informed by data, enabled by data, and create value from data.” New intelligentedge technologies, like model monitoring, robotics, and drones, are powered byAI and ML. This allows data users to “see around the corner” and monitorimportant data, detect early signs of risk events, and discover the rightquestions to ask your data. In order to make the most accurate predictions, datascientists need the most accurate data."
 },
 {
  "title": "What data mesh means for data scientists",
  "url": "/videos/2021-07-14-what-data-mesh-means.html",
  "content": "# {{ page.title }}Participants: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;ko7g02j84c&#39; %}Data mesh is both a technical and organizational approach to managing andaccessing data. We were lucky enough to have Zhamak Dehghani, who coined theterm data mesh, join us for this discussion on how the data mesh impacts datascientists.By having this decentralized approach and treating data as a product, data isable to be exposed and shared with those that need it the most, including thedata scientists. Zhamak said, “To get value from data, particularly foranalytical purposes, when we want to make predictions or we want to discovertrends, we have to aggregate and centralize data in one place, under the controlof one set of technologies and specifically under the control of a centralizedteam.”The question of data ownership is a huge pain point for data scientists.However, with the data mesh architecture, scientists are able to access datawhere it lives and better contribute to the overall business goals. In aconversation moderated by Sophie Watkins of Red Hat, the panel comprising,Daniel Abadi, professor of Computer Science at University of Maryland, MaxSchultze of Zalando, and Zhamak Dehghani of Thoughtworks, discuss pain points,best practices, and the overall principles of data mesh."
 },
 {
  "title": "404",
  "url": "/404.html",
  "content": "                                  Oops ... 404        Don&#39;t panic. We brought a towel.        Try going home.        Or maybe you are looking for some specific documentation and resources:                  {{site.terms.sep_full}}          Starburst personas overview          Platform administrator          Data engineer          Data consumer                            &amp;nbsp;                                          Confused where you landed?          Go          straight to the {{site.terms.sep_full}} reference documentation!          A lot of new useful documentation is available here, but fear not,          all the comprehensive reference documentation for          {{site.terms.sep_full}} is ready and constantly improving as always.                      Go there now!                                                    "
 },
 {
  "title": "Azure Data Lake Storage catalogs",
  "url": "/starburst-galaxy/catalogs/adls.html",
  "content": "# {{page.title}}You can use an Azure Data Lake Storage (ADLS) catalog to configure access to a[Azure Data LakeStorage](https://azure.microsoft.com/en-us/services/storage/data-lake-storage/)object storage hosted on Microsoft Azure.The metadata about the objects and related type mapping needs to be stored in ametastore. You can use a Hive Metastore, or the built-in metastore.## Configure a catalogTo create an ADLS catalog, select **Catalogs** in the main navigation and click**Configure a catalog**. Click on the **Azure Data Lake Storage** button in the**Select a data source** screen.{% include_relative name-description.md %}{% include_relative read-only.md %}### Authentication to ADLSProvide the **GCS JSON key** to grant access to the object storage.The connection to the object storage requires the ABFS storage account name andaccess key.### Metastore configuration{% include_relative hms.md %}{% include_relative sgm.md %}{% include_relative save.md %}"
 },
 {
  "title": "Create an AKS cluster",
  "url": "/ecosystems/microsoft/aks/aks-cluster-creation.html",
  "content": "# {{page.title}}This page describes how to configure a new AKS cluster that ensures all{{site.terms.sep}} resources are co-located and follow best practices.{% include warning.html content=&quot;SEP has specific requirements for sizing,placement, and sharing of resources. You must ensure that your AKS cluster meetsall requirements described in our [cluster requirementssection](../../../latest/k8s/requirements.html#k8s-cluster-requirements).&quot; %}## PrerequisitesEnsure that you have the following tools, policies, and certificates beforecreating a Kubernetes cluster for {{site.terms.sep}} in AKS:* `helm`* `kubectl`* [Azure CLI (`az`)](https://docs.microsoft.com/cli/azure/)* Azure resource group for the SEP nodes* Virtual network assigned for the resource group* IAM policies for ADLS, S3, as desired* CA-signed certificate for HTTPS/TLS (for a domain such as  `starburst.example.com`) if using AD/LDAP authentication## Create your Azure clusterIt is strongly recommended to have your {{site.terms.sep}} coordinator andworkers share the same resource group. The following example[`az aks create`](https://docs.microsoft.com/cli/azure/aks?view=azure-cli-latest#az_aks_create)command creates the &quot;sep-example&quot; cluster in the &quot;example-rg&quot; resource group:```shell$ az aks create --kubernetes-version 1.20.7 --name sep-example --resource-group example-rg   --vnet-subnet-id /subscriptions/1234abcd-a1b2-c3d4-e5f6-example/resourceGroups/example-rg/providers/Microsoft.Network/virtualNetworks/example-network/subnets/default   --service-cidr 10.10.0.0/16   --dns-service-ip 10.10.0.10   --docker-bridge-address 172.16.0.1/16   --location eastus   --zones 1   --network-plugin azure   --node-vm-size standard_ds2_v2   --enable-aad   --aad-admin-group-object-ids aabbccdd-1a2b-3c4d-5d6f-example   --assign-identity /subscriptions/1234abcd-a1b2-c3d4-e5f6-example/resourcegroups/example-rg/providers/Microsoft.ManagedIdentity/userAssignedIdentities/example-identity-policy   --enable-cluster-autoscaler   --node-count 1   --min-count 1   --max-count 3   --nodepool-name systempool   --node-osdisk-size 64```## Establish nodepoolsThe best practice is to create one nodepool for your {{site.terms.sep}}coordinator and another for worker nodes. The following[`az aks nodepool add`](https://docs.microsoft.com/cli/azure/aks/nodepool?view=azure-cli-latest#az_aks_nodepool_add)command creates anodepool for a coordinator:```shell$ az aks nodepool add --cluster-name sep-example --resource-group example-rg   --name sep-coordinator   --labels apps=sep-coordinator   --priority Spot   --node-vm-size standard_d8s_v3   --eviction-policy Delete   --spot-max-price -1   --enable-cluster-autoscaler   --node-count 1   --min-count 1   --max-count 2   --node-osdisk-size 64   --node-osdisk-type Ephemeral   --no-wait```The following command creates a scaling nodepool for a minimum of two workers:```shell$ az aks nodepool add --cluster-name sep-example --resource-group example-rg   --name sep-workers   --labels apps=sep-workers   --priority Spot   --node-vm-size standard_d8s_v3   --eviction-policy Delete   --spot-max-price -1   --enable-cluster-autoscaler   --node-count 2   --min-count 2   --max-count 4   --node-osdisk-size 64   --node-osdisk-type Ephemeral   --no-wait```"
 },
 {
  "title": "Archive",
  "url": "/blog/archive.html",
  "content": "      {{ page.title}}    The complete list of posts from the Starburst team        {% for post in site.posts %}                              {{ post.date | date: &quot;%-d %B %Y&quot; }}          {{ post.title}}          {{ post.author }}                           {% endfor %}      "
 },
 {
  "title": "Audit log",
  "url": "/starburst-galaxy/account/audit-log.html",
  "content": "# {{page.title}}The **Audit log** section displays the audit trail of all administrative actionsperformed by all users in {{site.terms.sg}}. Use the filter above the list tonarrow down the displayed data.Each action contains the following details:* Operation* Target* What changed* User* Entity* Time of change"
 },
 {
  "title": "Enable autoscaling in SEP clusters",
  "url": "/platform-administrator/autoscaling.html",
  "content": "# {{page.title}}Enabling cluster autoscaling in {{site.terms.sep_first}} requires you to deployand configure a cluster autoscaler and to install a metrics server to report onworker CPU utilization. This document assumes that a cluster autoscaler iscurrently deployed for your cluster:* [EKS autoscaler](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html)* [AKS autoscaler](https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler)* [GCP autoscaler](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler)* [OpenShift autoscaler](https://docs.openshift.com/container-platform/4.1/machine_management/applying-autoscaling.html)## Configure the cluster for autoscalingTo prevent scale downs that interrupt in-flight query processing, you must setthe `safe-to-evict` annotation for the cluster:```shell$ kubectl -n kube-system annotate deployment.apps/cluster-autoscalercluster-autoscaler.kubernetes.io/safe-to-evict=&quot;false&quot;```The autoscaler manifest must be modified to include the following four lines inthe `spec.containers.command` YAML node with appropriate indentation:```yaml- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/- --balance-similar-node-groups- --skip-nodes-with-system-pods=false- --nodes=min:max:```To edit the manifest to include these lines, use the following command:```yaml$ kubectl -n kube-system edit deployment.apps/cluster-autoscaler```Set the [cluster-autoscalerversion](https://github.com/kubernetes/autoscaler/releases). The specifiedversion must be the major version of your Kubernetes cluster. In the followingexample, the version is set to `v1.18.3`:```yaml$ kubectl -n kube-system set image deployment.apps/cluster-autoscaler        cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v1.18.3```## Deploy the metrics serverDeploy the metrics server to report CPU utilization using the following command:```shell$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml```Verify that the metrics server is deployed and is running:```shell$ kubectl get deployment metrics-server -n kube-system```"
 },
 {
  "title": "Starburst Enterprise in AWS Marketplace",
  "url": "/ecosystems/amazon/aws-marketplace.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} is available directly through [Amazon AWSMarketplace](https://aws.amazon.com/marketplace/pp/prodview-pwnl3c6p2jycg#pdp-overview)to run on a variety of instance types. The AWS Marketplaceoffering allows you to easily set up a monthly contract, after which you candeploy {{site.terms.sep}} using the CloudFormation Template (CFT).Once deployed, {{sep}} is[fully customizable](../../latest/aws/configuration.html#custom-configuration),including connecting to services such as:* [Glue](../../latest/aws/configuration.html#hive-connector-options)* [Hive Metastore](../../latest/aws/metastore.html)* [Ranger and LDAP](../../latest/aws/configuration.html#ranger-and-ldap-user-synchronization)* [IAM](../../latest/aws/configuration.html#iam-instance)* [CloudWatch](../../latest/aws/cloudwatch.html)All of these service integrations are available in addition to{{site.terms.sb}}&#39;s suite of [enhanced and exclusiveconnectors](../../latest/connector/starburst-connectors.html).## Choose our deployment typeIn addition to our marketplace offerings, {{site.terms.sep}} is also availableto deploy on Amazon&#39;s Elastic Kubernetes Service (EKS). We strongly recommendthat you deploy using EKS unless there is a specific technical reason why youcannot.If you intend to use CFT as a proof-of-concept before moving to an EKSdeployment in production, please be aware that there is no direct path from aCFT deployment to an EKS deployment. Therefore, if you begin building aproof-of-concept using CloudFormation, you should expect to remain onCloudFormation for production.## Support for marketplace offerings{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:* Email-only support* Five [email issues to  awssupport@starburstdata.com](mailto:awssupport@starburstdata.com) per month* First response SLA of one business day* Support between 9 AM - 6 PM US Eastern Time## Set up your subscriptionBefore you begin, you must have an AWS login with the ability to subscribe toservices.To subscribe to {{site.terms.sep}} through the AWS Marketplace:1. Log in with your subscriber account and follow [this link](https://aws.amazon.com/marketplace/pp/prodview-pwnl3c6p2jycg#pdp-overview), or enter &quot;{{site.terms.sep_full}}&quot; in the marketplace search field and select   **Starburst Enterprise**.1. Scroll down to the **Estimating your costs** tool:    1. Select your desired **Region**.    1. Select &quot;CloudFormation Template** as your **Fulfillment option**. *Do not       select the Amazon Machine Image (AMI) option.*    1. Select the **Instance type**.1. Click **Continue to subscribe** at the top of the page.1. On the **Subscribe to this software** screen that appears:    1. Configure the contract, if desired.    1. Manage the license as per your organization&#39;s protocols using the       **Manage license** option, if desired.1. Click **Continue to configuration**.1. On the **Configure this software** page, select **CloudFormation Template**   option again in the **Delivery method** dropdown. *Do not select the Amazon   Machine Image (AMI) option.*1. In the &quot;Software version&quot; dropdown, confirm that the latest version is   selected.1. Confirm that the correct region is selected.1. Click **Continue to launch**.1. In the &quot;Launch this software&quot; screen, select the **Launch CloudFormation**   option in the **Choose Action** dropdown.1. Click **Launch**.## Create your stack and install {{site.terms.sep}}Once you have launched {{site.terms.sep}}, our [installguide](../../latest/aws/installation.html) provides instructions for creatingyour stack and specifying cluster details.## Next stepsRead our CFT [configuration section](../../latest/aws/configuration.html) tobegin customizing {{site.terms.sep}}. You can also set up these optionalitems:* [Glue](../../latest/aws/configuration.html#hive-connector-options)* [Hive Metastore Service](../../latest/aws/metastore.html)* [Apache Ranger](../../latest/aws/configuration.html#ranger-and-ldap-user-synchronization)* [CloudWatch](../../latest/aws/cloudwatch.html)The following pages introduce key concepts and features in {{site.terms.sep}}:* [Catalogs](../../data-engineer/catalogs.html) and  [Connectors](../../latest/connector/starburst-connectors.html)* [Security](../../platform-administrator/security.html)* [Performance](../../platform-administrator/performance-tuning.html)"
 },
 {
  "title": "Azure Database",
  "url": "/ecosystems/microsoft/data-sources/azure-database.html",
  "content": "# {{page.title}}You can query data stored on relational databases hosted on Azure Database with{{site.terms.sep_first}} and {{site.terms.sg}}.## {{site.terms.sep_full}}Create a catalog with the [MySQLconnector](../../../latest/connector/starburst-mysql.html) with the followingdatabases:* [Azure Database for MariaDB](https://azure.microsoft.com/en-us/services/mariadb/)* [Azure Database for MySQL](https://azure.microsoft.com/en-us/services/mysql/MySQL)Create a catalog with the [PostgreSQLconnector](../../../latest/connector/starburst-postgresql.html) with the followingdatabases:* [Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/services/postgresql/)## {{site.terms.sg}}Use a [MySQL catalog](../../../starburst-galaxy/catalogs/mysql.html) with thefollowing databases:* [Azure Database for MariaDB](https://azure.microsoft.com/en-us/services/mariadb/)* [Azure Database for MySQL](https://azure.microsoft.com/en-us/services/mysql/MySQL)Use a [PostgreSQL catalog](../../../starburst-galaxy/catalogs/postgresql.html)with the following databases:* [Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/services/postgresql/)"
 },
 {
  "title": "Get Starburst in Azure Marketplace",
  "url": "/ecosystems/microsoft/azure-marketplace.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} is available as a Microsoft preferred solution throughthe [AzureMarketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/starburstdatainc1582306810515.starburst-enterprise-presto?tab=Overview)to run on a base of either 216 vCPUs/hr or 1072 vCPUs/hour. Our AzureMarketplace offering allows you to easily set up a monthly contract to deployand operate {{site.terms.sep}} using AKS.Once deployed, {{site.terms.sb}} integrates with a number of Azure servicesincluding:* Azure Blob Storage* Azure Data Lake Storage* External Hive Metastore* Microsoft PowerBIAll of these services are available in addition to {{site.terms.sb}}&#39;s suite of[enhanced and exclusiveconnectors](../../../latest/connector/starburst-connectors.html).### Support for marketplace offerings{{site.terms.sb}} offers the following support for our marketplace subscribers:- Email-only support- Five [email issues to azuresupport@starburstdata.com](mailto:azuresupport@starburstdata.com) per month- First response SLA of one business day- Support hours between 9 AM - 6 PM US Eastern Time## Set up your subscriptionBefore you begin, you must have an Azure Marketplace login with the ability tosubscribe to services.{% include note.html content=&quot;If this is your first time using Azure, you areprompted to set up billing before you can subscribe.&quot; %}To subscribe to {{site.terms.sep}} through the Azure Marketplace:1. Log in with your subscriber account and [access the Starburst Enterprise offering directly](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/starburstdatainc1582306810515.starburst-enterprise-presto?tab=Overview), or enter &quot;{{site.terms.sep_full}}&quot; in the marketplace search field   and select   **{{site.terms.sep_full}} - Distributed SQL Engine**. Click **Get it now** in   the resulting screen.1. Select the plan you would like to subscribe to, either 216 v/CPUs/hr or 1072   vCPUs/hr.1. In the **Project details** screen, select your organization&#39;s   **Subscription** and **Resource group**. We strongly suggest that you use the   **Create new** link to create a separate resource group for your   {{site.terms.sep}} cluster.1. Give your {{site.terms.sep}} subscription a unique identifier with the   **Name** field.1. Select your billing frequency, and click **Review + subscribe**.1. Review the **Product + plan details** screen, adding any required   information and click **Subscribe**.## Get your licenseReach our to [Starburst Support](../../support.html) to get your license file.The license file unlocks the functionality in our exclusive connectors andextended security features.## Deploy {{site.terms.sep}}{{site.terms.sb}} offers two mechanisms to deploy with AKS, available in ourpublic GitHub repositories:* [Azure CLI deployment](https://github.com/starburstdata/starburst-deploy/tree/main/azure)* [Terraform-based scripts](https://github.com/starburstdata/starburst-terraform/tree/main/azure)## Next stepsAs you build out your proof-of-concept deployment and beyond, read our extensive[SEP configuration section](../../latest/k8s/sep-configuration.html), along with[examples](../../latest/k8s/sep-config-examples.html). You can also set up theseoptional items:* [Hive Metastore Service](../../latest/k8s/hms-configuration.html)* [Apache Ranger](../../latest/k8s/ranger-configuration.html)* [Starburst cache service](../../latest/k8s/cache-service-configuration.html)The following pages introduce key concepts and features in {{site.terms.sep}}:* [Catalogs](../../data-engineer/catalogs.html) and  [Connectors](../../latest/connector/starburst-connectors.html)* [Security](../../platform-administrator/security.html)* [Performance](../../platform-administrator/performance-tuning.html)"
 },
 {
  "title": "Azure Storage",
  "url": "/ecosystems/microsoft/data-sources/azure-storage.html",
  "content": "# {{page.title}}You can access object storage in Microsoft Azure Blob Storage and Azure DataLake Storage with {{site.terms.sep_first}} and {{site.terms.sg}}.## {{site.terms.sep_full}}Use the [Hive connector](../../../latest/connector/starburst-hive.html) with thededicated [configuration options](../../../latest/connector/hive-azure.html) toquery Azure Blob Storage and Azure Data Lake Storage.## {{site.terms.sg}}Use an [Azure Data Lake Storagecatalog](../../../starburst-galaxy/catalogs/adls.html)."
 },
 {
  "title": "Using the cache service",
  "url": "/data-engineer/cache-service.html",
  "content": "# {{ page.title }}[Starburst Enterprise platform](../starburst-enterprise/index.html) (SEP) offerscaching options through its [cache service](../latest/admin/cache-service.html)to help you reduce query time and costs.The cache service can automatically manage refreshes of materialized views [inHivecatalogs](../latest/connector/starburst-hive.html#automated-materialized-view-management).With [table scan redirections](../latest/admin/table-scan-redirection.html), youcan redirect data requests for a table in one catalog to a cached version of itin another catalog on a more performant system, reducing the query load on theoriginal data source. This redirection is transparent to the user, and thereforeprovides performance improvements without the need to modify user queries.This document provides an overview of the cache service, including links torelevant sections of our reference documentation.## Select the right cache strategyBoth materialized views and table scan redirections improve query performance.Find out which one is right for your organization in this guide.### Materialized viewsMaterialized views created in Hive catalogs are backed by the cache service andthe Hive Metastore and can be automatically refreshed. If you use{{site.terms.sep}} with Hive and the cache service, you already have thenecessary prerequisites to use automatically refreshed materialized views.Any query that runs successfully in {{site.terms.sep}} can be used to create amaterialized view. Users access the materialized view through the catalog it ismade available from, just as they would any other data source.Materialized views in Hive allow you to access the results of a query from anydata catalog you have defined in {{site.terms.sep}} in real time withoutre-processing the query, and with a refresh scheme that works best for yourorganization.### Table scan redirectionsTable scan redirections allow you to transparently redirect queries to a cachedversion in a location that incurs lower egress costs, is more performant, orboth. Your users get even faster performance without changing the vetted queriesthat they depend upon.Table scan redirection is available for nearly all {{site.terms.sb}} connectors.## Get startedThe information in this section helps you get {{site.terms.sb}} Cached Viewsup and running using the {{site.terms.sb}} cache service. It includes links tomore detailed information in our reference documentation. As with anyconfiguration change in {{site.terms.sep}}, you must restart the server toapply the changes.### Enable the cache serviceTo use table scan redirections or Hive materialized views, the cache servicemust first [be enabled](../latest/admin/cache-service.html#configuration). Ourreference documentation has[requirements](../latest/admin/cache-service.html#requirements) and instructionsfor doing so. Take the time to read about the [two types ofdeployments](../latest/admin/cache-service.html#installation), standalone andembedded, to see which is better suited to your cluster. If your organizationuses our Helm charts to deploy {{site.terms.sep}}, you can use the [Helm chartfor the cache service](../latest/k8s/cache-service-configuration.html) to deployit as standalone service.Before you begin, ensure that you have an [external databaseinstance](../latest/admin/cache-service.html#relational-database) available tostore information about materialized views and table scan redirections.When you are ready, there are a number of [configurationproperties](../latest/admin/cache-service.html#configuration) that must be set,shown in the following example:```textservice-database.user=aliceservice-database.password=test123service-database.jdbc-url=jdbc:mysql://mysql-server:3306/redirectionsstarburst.user=bobstarburst.jdbc-url=jdbc:trino://coordinator:8080rules.file=etc/rules.json```Where these get set depends on whether or not you are using a standalone orembedded cache service, and whether you are using Helm charts or not:* Standalone -  * via the cache service Helm chart in the ``config.properties`` node nested    under the top level ``config`` node.  * Other deployments in the ``etc/config.properties`` file on the cache service    server.* Embedded -  * via the SEP Helm-chart in the ``config.properties`` node nested under the    top level ``coordinator`` node in the ``config`` section node.  * Other deployments in the ``etc/cache.properties`` file on the coordinator    server.### Enable materialized viewsMaterialized views must be [enabled in the Hivecatalog](../latest/connector/starburst-hive.html#sb-hive-mv-catalog-config) thatthey are accessed from. In addition, you must specify a schema to containmaterialized view storage. We strongly recommend that you create a schemadedicated to materialized views for a given catalog. A namespace must also bespecified; it is used internally by the cache service to avoid inadvertent namecollisions. To create a new schema with a specified location, use a commandsimilar to the following, which creates the ``mymvstorage`` schema in the``myhive`` catalog:```sqlCREATE SCHEMA myhive.mymvstorage WITH (location = &#39;s3a://mymvstorage/&#39;);```Each Hive catalog must be configured to allow materialized views and use theschema you created for materialized views:```textmaterialized-views.enabled=truematerialized-views.namespace=mymvnamespacematerialized-views.storage-schema=mymvstoragecache-service.uri=http://:8180```Users with the necessary access privileges in a schema in the configured catalogcan [create materialized views in the usualmanner](../data-consumer/materialized-views.html).### Enable table scan redirectionsLike materialized views, table scan redirections are also managed by the cacheservice. The redirections are transparent to users, so no additional training,modifications of queries or new commands are needed to use them.Table scan redirection refreshes are configured in a [JSON-formatted rulesfile](../latest/admin/cache-service.html#cache-refresh-rules-for-redirections).There are numerous properties to govern both global defaults for all rules andrule- specific behaviors.Table scan redirections offer even finer-grained control for refreshes thando materialized views, including cleanup options."
 },
 {
  "title": "Configure a catalog",
  "url": "/starburst-enterprise/try/catalog.html",
  "content": "# {{ page.title }}Before you can connect to your data sources with {{site.terms.sep_first}} youmust define them in a catalog. Each catalog connects to a single data source,such as a specific instance of a PostgreSQL or Oracle database. Catalogs aredefined in your Helm chart under the `catalogs:` node, or if you are not usingour K8s deployment, in its own properties file the `etc/catalog` directory of{{site.terms.sep}}.The name of the catalog YAML entry or catalog file becomes the name used toaccess the data in a query. There is a small set of required properties forevery catalog. While properties can vary, they minimally define the connectionto the data source. Depending on your data source type, there are anumber of different configuration properties available.This document teaches you how to create a minimum catalog configuration.## Gather your connection informationYou will need the following minimum information to connect to a data source:* Connection URL* Username* Password## Create your catalog propertiesThe catalog properties file content consists of `key=value` pairs. Every catalogmust include the `connector.name` property, and the[connector](../../../latest/connector/starburst-connectors.html) name must matchthe data source.In the `values.yaml` file in your K8s deployment, you must create a catalogentry under the `catalogs:` top-level node. The following example assumes thatthe `salesdata` catalog resides in a PostgreSQL database:```yamlcatalogs:  salesdata: |-    connector.name=postgresql    connection-url=jdbc:postgresql://example.net:5432/database    connection-user=root    connection-password=secret```If you are not using K8s, you must create the minimum contentfor a PostgreSQL catalog file. In the following example, the contents arestored as `etc/catalog/salesdata.properties`:```propertiesconnector.name=postgresqlconnection-url=jdbc:postgresql://example.net:5432/databaseconnection-user=rootconnection-password=secret```## Apply changesTo use the a newly-defined catalog, you must restart {{site.terms.sep}} for thechanges to take effect. Refer to the documentation for your deployment type forrestart instructions.## Next stepsTo learn more about catalogs and connectors in {{site.terms.sep}}:* Read our data engineering [guide to creating  catalogs](../../data-engineer/catalogs.html).* Familiarize yourself with the available {{site.terms.sep}}  [connectors](../../../latest/connector/starburst-connectors.html)"
 },
 {
  "title": "Configure and define catalogs",
  "url": "/data-engineer/catalogs.html",
  "content": "# {{ page.title }}You need to understand data sources and how they are connected to{{site.terms.sep_first}}, to take advantage of the query performance availableto your data consumers. The following content provides an overview of datasources and catalogs and how they work with connectors in {{site.terms.sep}}.## DefinitionsBefore you begin, here are definitions and explanations the key concepts of for*data sources*, *catalogs*, and *connectors*.### Data sourcesA data source is a system where data is retrieved from. You can query systemssuch as distributed object storage, RBDMSs, NoSQL databases, document databases,and many others.In {{site.terms.sep}}, you must connect to a data source so you can query fromthat source. To query those sources, you need to create a *catalog propertiesfile* to define a catalog.### CatalogsCatalogs define and name the configuration to connect to and query a datasource. They are a [key concept](../../latest/overview/concepts.html) in{{site.terms.sep}}.Without catalogs there is nothing to query.The catalog name is defined by the name of the catalog properties file, locatedin ``etc/catalog``. You can have as many catalogs as you want, and there are norestrictions for naming outside of valid character types. For example, afilename of ``etc/catalog/mydatabase.properties`` results in the catalog name``mydatabase``.The catalog properties file content defines the access configuration to the datasource. Properties are ``key=value`` pairs.Each catalog defines one and only one connector using the required``connector.name`` property. You must select the correct connector for your datasource. For example, the PostgreSQL connector is defined by using``connector.name=postgresql``, and enables the catalog to access a PostgreSQLdatabase. Another example is the Hive connector defined by``connector.name=hive``. It can enable a catalog to access Hadoop,Amazon S3 and many other object storage systems. {% include note.html content=&quot;While a catalog can have only one connector, asingle connector may be used by multiple catalogs.&quot; %}Access a list of catalogs with the[``SHOW CATALOGS``](../latest/sql/show-catalogs.html) command.#### Configuration propertiesEach connector has a small set of required properties. While properties canvary, they minimally define the connection to the data source. Depending on yourconnector and data source, there are a number of different configurationproperties available.Optional properties enable further configuration of the catalog in areas such assecurity, performance, and query behavior. These connector-specific propertiesare defined in the documentation for each connector. For more information onconnector-specific configuration properties, start with the [list of allconnectors](../latest/connector.html).#### Catalog session propertiesYou can further customize the behavior of the connector in your catalog usingcatalog session properties. A session is defined by a specific user accessing{{site.terms.sep}} with a specific tool such as the CLI. Catalog sessionproperties can control resource usage, enable or disable features, and changequery processing.Most of the session properties are similarly named to their config propertiescounterparts in a catalog file, mostly differing by the use of underscores (_)in the name to be SQL compliant. Session properties override catalog propertiesin certain circumstances.You can view current session properties using the[``SHOW SESSION``](../latest/sql/show-session.html) command. Thenimplement your session properties using[``SET SESSION``](../latest/sql/set-session.html).### ConnectorsA connector is specific to the data source it supports. It transforms theunderlying data into the {{site.terms.sep}} concepts of schemas, tables,columns, rows, and data types.Connectors provide the following between a data source and {{site.terms.sep}}:- Secure communications link- Translation of data types- Handling of variances in the SQL implementation, adaption to a provided API,  or translation of data in raw files## Setup of connectorMost {{site.terms.sep}} connectors include their configurations and anythingelse you might need by default, and therefore no setup is required.If you&#39;re using a connector that requires additional set up, such as theaddition of a proprietary JDBC driver, you find that documented with the[specific connector](../latest/connector.html).### Create a catalog properties fileYou can create catalog to access a data source with a few simple steps:- Create the catalog properties file in ``etc/catalog/``, for example  ``etc/catalog/mydatabase.properties``- Specify the required connector with ``connector.name=`` in file, for example  ``connector.name=postgresql``- Add any other properties required by the connector- Add any optional properties as desired- Copy the file onto the coordinator and all worker nodes- Restart the coordinator and all workers- Confirm the catalog is available with ``SHOW CATALOGS;``- List the available schemas with ``SHOW SCHEMAS FROM mydatabase;``- Start writing and running the desired queriesMany connectors use the similar properties in catalog properties files. Forexample, most JDBC-based connectors require these minimum properties for theircatalog files:```propertiesconnector.name=[connectorname]connection-url=[connectorprotocol]//:;database=connection-user=rootconnection-password=secret```"
 },
 {
  "title": "Connection parameters in CFT",
  "url": "/ecosystems/amazon/cft-connection-info.html",
  "content": "# {{page.title}}To run queries in {{site.terms.sep_first}}, you need the following connectionparameters described in our [client overviewpage](../../data-consumer/clients/index.html#connection-information).## Locating the connection informationTo locate the IP address, protocol, and port of your coordinator, complete thefollowing steps:1. Navigate to the **CloudFormation console** from **Service &gt; Management Tools**.2. Select your stack name and click the associated tab labeled **Outputs**.3. Note the value for the ``StarburstCoordinatorURL``.## Accessing the coordinatorThe ``StarburstCoordinatorURL`` is only accessible from within the same VPC as{{site.terms.sep}}. You must run your client tools from within the VPC, orwith connectivity to the VPC. Your IT department can help you with connectivityissues.Any easy way to test connectivity from a network location is to access the [WebUI](../../latest/admin/web-interface.html) using the``StarburstCoordinatorURL`` in your browser.## Connecting clients to {{site.terms.sep}}Once you have gathered the connection information, you can use [anyclient](../../data-consumer/clients/index.html) supported by {{site.terms.sep}}to connect and query.For example, to connect and query with the Command Line Interface (CLI):1. Download and install the [CLI](../../data-consumer/clients/cli.html)2. Use the parameters determined in the preceding section.3. Run the CLI with the URL and username supplied and activate the password   dialog.Once connected you can check for available catalogs with ``SHOW CATALOGS``, orrun any other query."
 },
 {
  "title": "Choosing the right deployment",
  "url": "/get-started/choosing-the-right-deployment.html",
  "content": "# {{ page.title }}[Starburst Enterprise](../starburst-enterprise/index.html) is available foron-premise usage or in private clouds, and can be run on bare metal servers,virtual machines and containers, all managed by you.You can also run {{site.terms.sep_full}} on public cloud provider systems, andtheir virtual machine or container offerings, or for further simplicity andconvenience [use their marketplaces](../ecosystems/index.html).That’s a lot of choice! This guide helps you select the best solution for yourorganization.The deployment you choose depends on a couple of key factors:* **People** and available skills in your organization* Variety and location of **data sources*** **Location** of your computing resources* **Security** and governance requirements## PeopleAs you decide the best {{site.terms.sb}} product and deployment style for yourorganization, the skill sets available in your organization is the mostimportant factor. You need the expertise of [platformadministrators](./starburst-personas.html#platform-administrator) and [dataengineers](./starburst-personas.html#data-engineer) to create, configure andmaintain clusters capable of running {{site.terms.sep_full}}. Your organizationmust also have people who can connect the desired data sources and ensure thatdata security and governance requirements are met. Typically, people with theseskill sets sit within your IT organization.[Starburst Enterprise](../starburst-enterprise/index.html) offers the mostcontrol of your deployments at the cost of being the most complex to install andmaintain.With limited availability using a public cloud provider or even [marketplaceoffering](../ecosystems/index.html) can help reduce the workload.## Data sourcesThe data sources you plan to query have a large impact on your choice. You needto understand what databases, object storage or others systems your users needto query. In addition, you need to know where these system are deployed toensure that {{site.terms.sep_full}} or {{site.terms.sg}} can access them withsufficient network performance and capacity.For example, if all your data is stored in your private network and data center,you should run {{site.terms.sep_full}} there as well. However, if all your datais hosted on a public cloud provider, you can choose to run{{site.terms.sep_full}} on the same cloud provider yourself, use a marketplaceoffering or even use {{site.terms.sg}}.{{site.terms.sep_full}} includes a very large variety of[connectors](../latest/connector.html) to support the most common datasources. They include many RDBMSs, Hadoop/Hive and other object storage systems,and commercial platforms such as Snowflake or Teradata. In addition,{{site.terms.sep_full}} can be operated anywhere. If your organization needs toquery large collection of data sources, {{site.terms.sep_full}} is the rightchoice for you. You can choose various options on how to run it, and ensure itis located closely to your data sources.## LocationIf you run exclusively on-prem, then your choice is easy -{{site.terms.sep_full}}. You can run it on bare metal servers, on virtualmachines, in private clouds, or even in your Kubernetes clusters on supportedcloud platforms. Your choice depends entirely on your requirements, datasources, people and skills.If you are using a single cloud provider, you can run {{site.terms.sep_full}},again with the options to manage the virtual machines yourself and use the`tar.gz` or RPM archive.You can also use a Kubernetes offering from cloud providers, and combine it withthe our [Kubernetes support with Helm charts](../latest/k8s.html).Alternatively, you can use cloud-specific offerings such our [AmazonCloudformation support](../latest/aws.html).{{site.terms.sb}} also makes {{site.terms.sep_full}} available in all major[marketplaces](../ecosystems/index.html).If you use a multi-cloud or hybrid cloud strategy, the locality of your datashould be your first consideration to reduce data transfer costs. You can evenrun multiple {{site.terms.sep_full}} clusters, and potentially connect them with[Starburst Stargate](../latest/connector/starburst-stargate.html).Here are some things to consider in choosing a location for your{{site.terms.sep_full}} cluster, keeping minimizing intra-cloud data transfer aslow as possible front-of-mind:* Can you place your cluster in a cloud that contains multiple data sources?* What intra-cloud data sources are most likely to be federated? Where is the  largest of them?* Is one of your cloud provider pricing models more favorable than the others?The closer you can put {{site.terms.sep_full}} to the bulk of your data, themore you can reduce the amount of data being returned, and save on data transfercosts.## SecurityWhat does data security and governance look like at your organization?{{site.terms.sep_full}} supports a wide range of security features.* Different authentication platforms such as LDAP, Kerberos or OAuth* Data access control with user impersonation, credential passthrough and others* Authorization management with Ranger or Privacera platform* Event logger for auditing and tracking* and othersOur [securityguide](https://www.starburst.io/info/starburst-enterprise-security-guide/)provides a good introduction.The location and mode of running {{site.terms.sep_full}} specifically determineswhich security features can be used. You need to specifically verify if afeature can be used based on your determined location.For example, if your company relies on [ApacheRanger](../latest/security/ranger-overview.html) or [Privaceraplatform](../latest/security/global-privacera.html) to secure your data,then a self-managed {{site.terms.sep_full}} in a private cloud or on-prem ismost likely the right choice for you.Depending on your particular security needs, one or more marketplace offeringsmay also work. {{site.terms.sep_full}} includes [Helm charts for Apache Rangerinstallation](../latest/k8s/ranger-configuration.html) and usage onKubernetes.## {{site.terms.sg}}As our most convenient solution, {{site.terms.sb}} now offers a hosted andmanaged solution - [{{site.terms.sg}}](../starburst-galaxy/index.html).{{site.terms.sg}} is the most hands-off approach as it provides asimple user interface and most complexity, including deployment and upgrade,is completely taken care of for you by {{site.terms.sb}}."
 },
 {
  "title": "CLI",
  "url": "/data-consumer/clients/cli.html",
  "content": "# {{page.title}}The {{site.terms.oss}} command line interface (CLI) provides a terminal-based,interactive shell for running queries and inspecting catalog structures in any{{site.terms.sep}} or {{site.terms.sg}} cluster. The CLI is distributed as anexecutable JAR file.## RequirementsThe {{site.terms.oss}} CLI requires Java 8 or newer, installed and availablethrough the ``PATH`` defined in your terminal. Run the ``java --version``command to verify that a compatible Java version is installed.## Download and installDownload the latest {{site.terms.oss}} CLI as an executable JAR file fromthe [reference documentation](../../latest/installation/download.html#clients).The latest CLI version is recommended, as the CLI is backwards-compatible witholder {{site.terms.sep}} versions.The JAR file is usable as-is, but for ease-of-use we recommend movingthis file to a directory in your shell&#39;s ``PATH``, renaming it, and making itexecutable. For example, in Linux:```shellcd /usr/local/bincp -p /home//Download/trino-cli-*-executable.jar .mv trino-cli-*-executable.jar trinochmod +x trino```You can run the same commands in macOS by replacing the directory in line twowith ``/Users//Download/trino-cli-*-executable.jar``.Next, to verify that the installation works, run the following command:```shelltrino --version```This command outputs a string with the CLI version you downloaded:```shell$ trino --versionTrino CLI 360```## ConnectionUse the following steps to access your cluster with the {{site.terms.oss}} CLIclient:1. Get the necessary [connection   information](./index.html#basic-connection-information) for your cluster.2. Launch the CLI with the ``--server`` option, specifying the host for your   cluster. You can specify a custom port by appending ``:`` to the   host.   ```shell   trino --server=https://cluster.example.com   ```   If the cluster requires password authentication, the client prompts you for   your password. By default the CLI client uses your operating   system username to authenticate. You can override this default and specify a   custom username with the ``--user`` option.   ```shell   trino --server=https://cluster.example.com --user=testuser   ```   On a successful connection, a ``trino&gt;`` prompt is displayed   indicating that you are currently in a {{site.terms.oss}} CLI shell.For more advanced authentication options and additional information, see the[{{site.terms.oss}} CLI reference documentation](../../latest/installation/cli.html).## QueryingIn the CLI command prompt, you can issue [SQL queries](../starburst-sql.html)against the cluster. You can review all available commands in the CLI shell withthe ``HELP;`` command.{% include note.html  content=&quot;All commands in the CLI shell must end with a semicolon.&quot;%}The following example commands demonstrate some common operations you can do inthe client:*  To see the list of catalogs configured in the cluster, run a [``SHOW   CATALOGS``](../../latest/sql/show-catalogs.html) statement:   ```shell   SHOW CATALOGS;   ```   See the following example output:   ```shell   trino&gt; SHOW CATALOGS;    Catalog   -----------    jmx    memory    system    tpcds    tpch   (5 rows)   ```*  To see the available schemas in a catalog, run a [``SHOW   SCHEMAS``](../../latest/sql/show-schemas.html) statement:   ```shell   SHOW SCHEMAS FROM tpch;   ```   See the following example output:   ```shell   trino&gt; SHOW SCHEMAS FROM tpch;          Schema   --------------------    information_schema    sf1    sf100    sf1000    sf10000    sf100000    sf300    sf3000    sf30000    tiny   (10 rows)   ```*  To specify a catalog and schema for following queries, run a   [``USE``](../../latest/sql/use.html) statement:   ```shell   USE tpch.sf100;   ```   See the following example output:   ```shell   trino&gt; USE tpch.sf100;   trino:sf100&gt;   ```*  To see the available tables in a schema, run a [``SHOW   TABLES``](../../latest/sql/show-tables.html) statement:   ```shell   SHOW TABLES FROM tpch.sf100;   ```   See the following example output:   ```shell   trino:sf100&gt; SHOW TABLES;     Table   ----------    customer    lineitem    nation    orders    part    partsupp    region    supplier   (8 rows)   ```*  To see the structure of a table, run a [``SHOW   COLUMNS``](../../latest/sql/show-columns.html) statement:   ```shell   SHOW COLUMNS FROM tpch.sf100.customer;   ```   See the following example output:   ```shell   trino:sf100&gt; SHOW COLUMNS FROM customer;      Column   |     Type     | Extra | Comment   ------------+--------------+-------+---------    custkey    | bigint       |       |    name       | varchar(25)  |       |    address    | varchar(40)  |       |    nationkey  | bigint       |       |    phone      | varchar(15)  |       |    acctbal    | double       |       |    mktsegment | varchar(10)  |       |    comment    | varchar(117) |       |   (8 rows)   ```*  To run a query, enter it as a complete SQL statement:   ```shell   SELECT custkey, name, phone, acctbal FROM tpch.sf100.customer LIMIT 7;   ```   See the following example output:   ```shell   trino:sf100&gt; SELECT custkey, name, phone, acctbal FROM customer LIMIT 7;   &quot;937501&quot;,&quot;Customer#000937501&quot;,&quot;21-593-223-9096&quot;,&quot;-543.06&quot;   &quot;937502&quot;,&quot;Customer#000937502&quot;,&quot;15-558-441-5619&quot;,&quot;764.44&quot;   &quot;937503&quot;,&quot;Customer#000937503&quot;,&quot;22-672-434-9488&quot;,&quot;8855.01&quot;   &quot;937504&quot;,&quot;Customer#000937504&quot;,&quot;12-286-528-4612&quot;,&quot;5136.01&quot;   &quot;937505&quot;,&quot;Customer#000937505&quot;,&quot;21-520-144-4196&quot;,&quot;1892.29&quot;   &quot;937506&quot;,&quot;Customer#000937506&quot;,&quot;31-167-767-9014&quot;,&quot;679.91&quot;   &quot;937507&quot;,&quot;Customer#000937507&quot;,&quot;27-398-220-4780&quot;,&quot;7159.44&quot;   ```As demonstrated in these example commands, the {{site.terms.oss}} CLI usessupported [SQL statements](../starburst-sql) to review and query data sourcesconnected to {{site.terms.sep}} or {{site.terms.sg}}.To exit the CLI shell, run ``EXIT;``:```shelltrino:sf100&gt; EXIT;```### Querying outside of the CLI shellIn addition to launching the {{site.terms.oss}} CLI and running queries from itsshell, you can execute queries directly from your terminal with the``--execute`` option and a query string:```shelltrino --server cluster.example.com:8080 --execute &#39;SELECT custkey, name, phone, acctbal FROM tpch.sf100.customer LIMIT 7&#39;&quot;937501&quot;,&quot;Customer#000937501&quot;,&quot;21-593-223-9096&quot;,&quot;-543.06&quot;&quot;937502&quot;,&quot;Customer#000937502&quot;,&quot;15-558-441-5619&quot;,&quot;764.44&quot;&quot;937503&quot;,&quot;Customer#000937503&quot;,&quot;22-672-434-9488&quot;,&quot;8855.01&quot;&quot;937504&quot;,&quot;Customer#000937504&quot;,&quot;12-286-528-4612&quot;,&quot;5136.01&quot;&quot;937505&quot;,&quot;Customer#000937505&quot;,&quot;21-520-144-4196&quot;,&quot;1892.29&quot;&quot;937506&quot;,&quot;Customer#000937506&quot;,&quot;31-167-767-9014&quot;,&quot;679.91&quot;&quot;937507&quot;,&quot;Customer#000937507&quot;,&quot;27-398-220-4780&quot;,&quot;7159.44&quot;```You can also execute SQL script files with the ``--file`` option:```shelltrino --server cluster.example.com:8080 --file &#39;nations.sql&#39;USE&quot;ALGERIA&quot;&quot;ARGENTINA&quot;...&quot;UNITED KINGDOM&quot;&quot;UNITED STATES&quot;```This example uses a script, ``nations.sql``, downloaded from the[{{site.terms.oss}}: The Definitive Guide GitHubrepository](https://github.com/trinodb/trino-the-definitive-guide).## Next stepsThis guide explained basic connection instructions and example queries for the{{site.terms.oss}} CLI client. To learn more about what you can do with the CLIclient, see the following resources:* Learn more about SQL statements that you can use with {{site.terms.sb}} in the  [SQL guide](../starburst-sql.html).* Learn more about the CLI client in the  [reference documentation](../../latest/installation/cli.html)."
 },
 {
  "title": "Using clients",
  "url": "/starburst-galaxy/clients.html",
  "content": "# {{page.title}}{{site.terms.sg}} includes [Query editor](./query-editor/index.html). It allows youto write and execute SQL statements in all clusters and the configuredcatalogs.You can access the connection details for a specific cluster by selecting the**Get connection** item in the  options in the[list of clusters](./clusters/index.html#cluster-list).{% include image.html  url=&#39;../assets/img/sg/connection-info.png&#39;  img-id=&#39;connection-info&#39;  alt-text=&#39;Connection info display for a cluster&#39;  descr-text=&#39;Connection info display for a cluster&#39;  screenshot=&#39;true&#39;%}The dialog displays the following information:* Connection string to use with the [JDBC driver](../data-consumer/clients/jdbc.html)* [Trino CLI](../data-consumer/clients/cli.html) start command* User, host, and port information for other clientsThe user string concatenates the username, provided as email address, and thecurrent role used by the logged in user.The host is specific for each cluster and includes the account name and thecluster name.The port is 443, as used by default for TLS connections. The certificate used by{{site.terms.sg}} to secure the connection with TLS is a globally trustedcertificate.Use this information to connect to your cluster in {{site.terms.sg}} with thefollowing supported and tested clients:* [Trino CLI](../data-consumer/clients/cli.html)* [DBeaver](../data-consumer/clients/dbeaver.html)* [Microsoft Power BI](../data-consumer/clients/powerbi.html)* [Tableau](../data-consumer/clients/tableau.html)[Other clients](../data-consumer/clients/index.html) are not explicitly tested,but are expected to work as well."
 },
 {
  "title": "Starburst Enterprise cluster basics",
  "url": "/platform-administrator/cluster.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is powerful and highly configurable. We have extensivedocumentation to help you ensure that {{site.terms.sep}} works as efficientlyand securely as possible in your environment.## ArchitectureA {{site.terms.sep}} cluster consists of a coordinator and many workers. Usersconnect to the coordinator with their SQL query tool. The coordinatorcollaborates with the workers. The coordinator as well as all the workers accessthe connected data sources. This access is configured in[catalogs](../../data-engineer/catalogs.html).You can learn more in the [conceptssection](../latest/overview/concepts.html) of the reference documentation.Processing each query is a stateful operation. The workload is orchestrated bythe coordinator and spread parallel across all workers in the cluster. Each noderuns {{site.terms.sep}} in one JVM instance, and processing is  parallelizedfurther using threads.## Memory and CPU resource considerationsTypical workloads for {{site.terms.sep}} require large amounts of memory and CPUfor processing. For optimal scheduling all workers need to have the same largeamount of memory allocated.## OS and software requirements{{site.terms.sep}} requires Linux, Java, and Python. Specific details vary foreach version and deployment platform.## Networking{{site.terms.sep}} has a few networking aspects you need to consider:* Access to the coordinator for users requires HTTP/HTTPS access* Access from the cluster to any external authentication system such as LDAP* Access from the all cluster nodes to any queried data sources## Securing{{site.terms.sb}} has an array of powerful, comprehensive security features toensure that your data governance and security are top-notch. We strongly suggestyou begin by watching the training video below. After that, our extensivesecurity documentation will help you get started securing your data with{{site.terms.sep}}.* [Securing Starburst training video](./security.html)* [Security documentation](../../latest/security.html)## Configuration basicsWhen you are ready to install, we&#39;ve got you covered with detailed referencedocumentation covering everything from deployment to setting up data sourceconnections:* [Coordinator and worker configuration](../../latest/installation/deployment.html#config-properties)* [Node properties](../../latest/installation/deployment.html#node-properties)* [JVM configuration](../../latest/installation/deployment.html#jvm-config)* [Catalog properties introduction](../../latest/installation/deployment.html#catalog-properties)## Deployment options{{site.terms.sep}} can be deployed in several ways, depending on yourorganization&#39;s infrastructure, skills, and existing tooling. Follow the linkmost appropriate to your environment to learn more about deploying{{site.terms.sep}}:We highly suggest using a cloud provider marketplace offering, and self-managedKubernetes deployments in the cloud for production clusters:* [Google Cloud Marketplace](../../ecosystems/google/index.html)* [Red Hat Marketplace](../../ecosystems/redhat/index.html)* [Microsoft Azure Marketplace](../../ecosystems/microsoft/index.html)* [Self-managed Kubernetes deployment](../latest/k8s.html) on any  cloud provider platform including AKS, EKS, GKE and OpenShift.The following options are available for baremetal servers or virtual machines inyour own data center or in a cloud environment:* [RPM](../../latest/installation/rpm.html) with your own custom tooling* [Tarball](../../latest/installation/tarball.html) with your own custom tooling* RPM or tarball deployments managed with  [Starburst Admin](../starburst-enterprise/starburst-admin/index.html)* AWS deployment with [AMI and CFT](../../latest/aws.html)"
 },
 {
  "title": "Connectors",
  "url": "/data-engineer/connectors.html",
  "content": "# {{ page.title }}{{site.terms.sep}} comes with [supported enterpriseconnectors](../latest/connector.html) that allow you to configure[catalogs](./catalogs.html) that provide access to all your data sources.{{site.terms.sep}}’s architecture fully abstracts the data sources it canconnect to, compute and storage are separated. You can scale your query engineand performance separately from your data storage.Exposing all this data in one place creates an accessible data mesh, ready foryour analysis.## Developing custom connectors{{site.terms.sep_first}}  comes with an array of built-in connectors for avariety of cloud-based and on-prem data sources.That separation means that you can use the {{site.terms.sep}} connector serviceprovider interface (SPI) to build plugins for file systems and object stores,NoSQL stores, relational database systems, and custom services without anoff-the-shelf connector. As long as you can map data into relational conceptssuch as tables, columns, and rows, it is possible to create your own{{site.terms.sep}} connector.To learn more, read our latest [developerdocumentation](../latest/develop.html)."
 },
 {
  "title": "Query multiple sources",
  "url": "/data-consumer/data-mesh.html",
  "content": "# {{ page.title }}{{site.terms.sep_full}} and {{site.terms.sg}} let data consumers queryanything, anywhere, and get the data they need in a single query, no matterwhere it lives in your company&#39;s [datamesh](https://www.starburst.io/info/distributed-data-mesh-resource-center/).Specifically, they support queries that combine data frommany different data sources in the data mesh at the same time. These datasources are [defined as catalogs which expose data in schemas andtables](../data-engineer/catalogs.html).You can combine, for instance, historical data from HDFS or object stores withthe most recent incoming data from PostgreSQL in one query.Combining data is straightforward: use the fully-qualified name of thetables in your `FROM` clause. Table names are fully-qualified when they includethe catalog and schema name:```text..```A *catalog* defines the schemas in a data source such as Snowflake,Oracle and Hive.Here&#39;s an example of data from two different sources, Hive and MySQL, combinedinto a single query:```sqlSELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &gt; 300LIMIT 2;```This query uses data from the following sources:* The `orders` table in the `order_entries` schema defined in the `hive_sales` catalog* The `customer_master` table in the `sf_history` schema defined in the `mysql_crm` catalog"
 },
 {
  "title": "JetBrains DataGrip",
  "url": "/data-consumer/clients/datagrip.html",
  "content": "# {{ page.title }}[DataGrip](https://www.jetbrains.com/datagrip/) by JetBrains is an IDE for databases.It is designed to work with databases installed locally, on a server, or in thecloud. It is installed as a local application on your workstation.You can use the [JDBC driver](./jdbc.html) with DataGrip to access {{site.terms.sep_full}}.The setup steps are similar for versions of[IntelliJ IDEA](https://www.jetbrains.com/idea/) and other tools from JetBrains.  ## RequirementsThe Trino JDBC driver and Datagrip 2021.2 work with {{site.terms.sep}} 354-e or newer.## ConnectionUse the following steps to prepare DataGrip to access your cluster:1. Get the necessary [connection   information](./index.html#connection-information) for your cluster.2. Start DataGrip.3. In the **Welcome to DataGrip** window, click the plus sign icon to start a   new project.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img//general/datagrip-new-project.png&#39;   alt-text=&#39;DataGrip new project&#39;   descr-text=&#39;DataGrip new project&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}4. Name your project.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img//general/datagrip-name-project.png&#39;   alt-text=&#39;DataGrip name project&#39;   descr-text=&#39;DataGrip name project&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}## Connect to a data source5. In the **Database** tool window on the left side of the screen, click the plus   sign, and select **Data Source &gt; Trino**.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-add-data-source.png&#39;   alt-text=&#39;DataGrip add data source&#39;   descr-text=&#39;DataGrip add data source&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}6. In the **General** tab of the **Data Sources and Drivers** dialog:   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-data-sources-general.png&#39;   alt-text=&#39;DataGrip general tab&#39;   descr-text=&#39;DataGrip general tab&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}   a. Name your database.   b. Enter the URL of the Starburst cluster in the **Host** field, and its port   in the **Port** field.   c. Leave the **Authentication** dropdown as is with **User &amp; Password** already   selected.   d. Enter your username in the **User** field.   e. If your cluster is TLS-enabled, you must enter a password, and follow the   upcoming steps for the **Advanced** tab.   f. Do not write in the **URL** field. It is constructed automatically.   G. Click **Apply**7. In the **Advanced** tab of the **Data Sources and Drivers** dialog:   For TLS-enabled clusters, you must configure the JDBC driver property   `SSL=true`.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-data-sources-advanced.png&#39;   alt-text=&#39;DataGrip advanced tab fields&#39;   descr-text=&#39;DataGrip advanced tab fields&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}8. Test the connection by clicking **Test Connection** at the bottom of the   dialog.## Configure the driver9. In the **Drivers** tab, choose **Trino** from the menu.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-driver-general.png&#39;   alt-text=&#39;DataGrip advanced tab fields&#39;   descr-text=&#39;DataGrip advanced tab fields&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}10. In the **General** tab, click the plus sign in the **Driver Files** field.    Navigate to the location of the Trino JDBC driver JAR file, and select it.    &amp;nbsp; {% include image.html    url=&#39;../../assets/img/general/datagrip-select-driver.png&#39;    alt-text=&#39;DataGrip driver general tab fields&#39;    descr-text=&#39;DataGrip driver general tab fields&#39;    screenshot=&#39;true&#39;    pxwidth=&#39;421&#39;    screenshot=&#39;true&#39;    %}11. Click **Apply**, then **Okay**## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is also required touse [TLS/HTTPS](../../latest/security/tls.html). If you&#39;re usingglobally-trusted certificate best practices, use the cluster&#39;s HTTPS URL in theconnection string as shown in the steps above.If you are not using a globally-trusted certificate, you may have to configurethe trust store on your client machine. Consult your site&#39;s networkadministrators for guidance.To use TLS, specify the JDBC parameter setting `SSL=true` as shown in theproceeding section. Alternatively, you can make the same setting appended to theJDBC connection string in the form `?SSL=true`.## QueryingTo reveal the databases and schemas in your data source, expand the nodenext to your data source, then click the three dots.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-choose-databases-schemas.png&#39;   alt-text=&#39;DataGrip choose databases and schemas&#39;   descr-text=&#39;DataGrip choose databases and schemas&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}Run your queries in the query console.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-run-query.png&#39;   alt-text=&#39;DataGrip run query&#39;   descr-text=&#39;DataGrip run query&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}"
 },
 {
  "title": "DBeaver",
  "url": "/data-consumer/clients/dbeaver.html",
  "content": "# {{ page.title }}The open source tool [DBeaver Community](https://dbeaver.io/) is a powerful SQLeditor and universal database tool. It is installed as a local application onyour workstation. You can use it to access clusters in {{site.terms.sep}} or{{site.terms.sg}}, because it uses the [JDBC driver](./jdbc.html).The same setup steps apply to the commercial versions of[DBeaver](https://dbeaver.com).  ## RequirementsUsers of {{site.terms.sg}} and {{site.terms.sep_full}} 354-e or newer, must useDBeaver 21.0 or newer.## ConnectionUse the following steps to prepare DBeaver to access your cluster:1. Get the necessary [connection   information](./index.html#connection-information) for your cluster.2. Start DBeaver.3. Right-click in the **Database Navigator** panel and select **Create &gt;   connection**.4. In the **Connect to a database** dialog, select **All** in the left column.5. Enter `trino` in the search field. Select the Trino logo and click **Next**.   (If you are connecting to a {{site.terms.sep}} cluster with release 350-e or   older, instead enter `prestos`. Select the PrestoSQL icon, then **Next**.)   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-connection.png&#39;   alt-text=&#39;DBeaver select driver dialog&#39;   descr-text=&#39;DBeaver select driver dialog&#39;   pxwidth=&#39;699&#39;   screenshot=&#39;true&#39;   %}6. In the **Main** tab of the **Connect to a database** dialog:   a. The initial configuration is set for a server running at `localhost:8080`      with no security settings.   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-connect-to-database.png&#39;   alt-text=&#39;DBeaver connect to database dialog&#39;   descr-text=&#39;DBeaver connect to database dialog&#39;   pxwidth=&#39;575&#39;   screenshot=&#39;true&#39;   %}   b. The **JDBC URL** field is constructed automatically. Do not write in this      field.   c. In the **Host** field, enter the FQDN or IP address of your cluster&#39;s      coordinator (or its load balancer or proxy). Do not include the protocol      at the start of the string or a database name on the end.   d. In the **Port** field, enter the port on which your cluster is listening      for connections. You must fill in a port number, which your network      administrator can provide, or enter one of the [default      ports](./index.html#default-ports) for {{site.terms.sep}} clusters.      Use 443 for {{site.terms.sg}}.    e. For your initial connection test, do not use the **Database/Schema**       field. You can come back later to narrow this connection entry to open a       particular database among those managed by your cluster.    f. In the **Username** field, for a cluster without security, enter any name.       For a TLS-enabled cluster, enter a valid username for the authentication       type in use on your cluster, such as LDAP. {{site.terms.sg}} provides the       correct value in the User field in the connection info dialog.    g. In the **Password** field, leave blank for a cluster without security.       For a TLS-enabled cluster, enter the valid password for the username       entered.7. For a TLS-enabled cluster, continue into the **Driver properties**   tab.   a. In the **User Properties** grid, right-click and select **Add new      property**.   b. Add a property named `SSL` with value `true`.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-ssl-true.png&#39;   alt-text=&#39;DBeaver driver properties&#39;   descr-text=&#39;DBeaver driver properties&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;486&#39;   screenshot=&#39;true&#39;   %}8. Click the **Test Connection** button in the bottom left of the dialog.   If the JDBC driver is not already installed, this opens the **Download driver   files** dialog showing the latest available JDBC driver. Select that line and   click **Download**.9. The connection test continues. Look for a success dialog like the following:   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-connection-test.png&#39;   alt-text=&#39;DBeaver connection test&#39;   descr-text=&#39;DBeaver connection test&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}   If you instead receive an error dialog, go back through your settings or try   different port numbers. Make sure you are using the exact connection   information provided by your network administrator.   {% include note.html      content=&quot;If you encounter an error stating &quot;Session properties cannot be      overridden once a transaction is active&quot;, enable **Auto-commit**. You can      find this setting on the **Edit connection** menu under **Connection      settings &gt; Initialization**.&quot;   %}12. After a successful test, click **Finish**. This places an entry for this    connection in the **Database Navigator** panel. Open this entry to connect    to the cluster and see a list of its catalogs.    {% include note.html content=&quot;You can select the new entry, right-click, and      select **Rename** to give this connection your preferred name.&quot; %}## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is also required touse [TLS/HTTPS](../../latest/security/tls.html). If you&#39;re using globallytrusted certificate best practices, use the cluster&#39;s HTTPS URL in theconnection string as shown in the steps above.{{site.terms.sg}} uses globally trusted certificates, so the host and portprovided by the connection dialog are sufficient.If you&#39;re not using a globally trusted certificate, you may have to configurethe trust store on your client machine. Consult your site&#39;s networkadministrators for guidance.To use TLS, specify the JDBC parameter setting `SSL=true` as shown in the stepsabove. As an alternative, you can make the same setting appended to the JDBCconnection string in the form `?SSL=true`.## QueryingYou can click on the defined connection in the **Database Navigator** toconnect. The initial connection downloads all metadata about catalogs, schema,tables, columns and more. You can browse the information as it loads.{% include image.html  url=&#39;../../assets/img/general/dbeaver-navigator.png&#39;  alt-text=&#39;DBeaver database navigator&#39;  descr-text=&#39;DBeaver database navigator&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}Open **SQL Editor &gt; Open SQL script** (or press F3) to write and execute yourqueries and inspect the returned results.{% include image.html  url=&#39;../../assets/img/general/dbeaver-editor.png&#39;  alt-text=&#39;DBeaver SQL editor&#39;  descr-text=&#39;DBeaver SQL editor&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}"
 },
 {
  "title": "Deploy Ranger Kubernetes clusters",
  "url": "/platform-administrator/deploy-ranger.html",
  "content": "# {{page.title}}[Global access control](../../latest/security/global-ranger.html) in{{site.terms.sep_first}} allows you to create, manage, and store access controlpolicies for objects that {{site.terms.sep}} has access to, including:* Catalogs* Schemas* Views* Tables* Columns* Rows* Procedures* Session PropertiesRanger access control policies can grant or revoke access to any of the abovementioned objects, and are stored in a backing PostgreSQL database.Ranger is deployed via a separate Kubernetes (K8s) Helm chart. The chart hasseveral key sections:* Registry credentials* A required administration container* The required backing database* An optional usersync containerYou can configure multiple {{site.terms.sb}} clusters to use the same Rangerinstance. Once deployed, Ranger is available within the cluster and can beaccessed internally using `http://ranger:6080`. External access is controlledvia the DNS configured for Ranger ingress.This document assumes you are familiar with Ranger, as well as Helm charts andK8s tools such as `kubectl`. We recommend that you review the following beforedeploying Ranger:* {{site.terms.sep}} K8s [best practices](https://docs.starburst.io/latest/k8s/overview.html#customization-best-practices)* {{site.terms.sep}} K8s [requirements](https://docs.starburst.io/latest/k8s/requirements.html)## Edit the Helm chartOur [referencedocumentation](https://docs.starburst.io/latest/k8s/ranger-configuration.html)provides details about the content of the Ranger Helm chart, including yamlsections not discussed here.### Provide your registry credentialsWe recommend that you use a separate `registry-access.yaml` file across all Helmcharts as described in our SEP [K8s installationinstructions](../../../latest/k8s/installation.html#create-this-one-file-before-you-begin).Alternatively, you can edit the `registryCredentials:` node of the Ranger Helmchart to include them.### Configure the Ranger serverThe following values must be defined in the `admin:` node of the Ranger Helm chart:* CPU resources for requests and limits - The defaults are sufficient for most  environments; however, they must work with the instance type you are using.* Memory resources for requests and limits - The defaults are sufficient for  most environments; however, they must work with the instance type you are  using.* Passwords - You must supply all passwords in the `passwords:` node.You can read more about the `admin:` top-level node in our [referencedocumentation](../../../latest/k8s/ranger-configuration.html#ranger-server).### Configure UsersyncUsersync automates the process of adding users to Ranger for policy enforcementby allowing the synchronization of users and groups from LDAP and ActiveDirectories.At a minimum, the `env:` properties in the top-level `usersync:` node must bedefined correctly for your environment. For all Ranger Helm chart usersyncproperties, see our [referencedocumentation](../../../latest/k8s/ranger-configuration.html#ldap-user-synchronization-server).### Configure the PostgreSQL backing databaseThe configuration properties for the PostgreSQL database which stores policyinformation are found in the `database:` top-level node. As a minimalcustomization, you must ensure that the following are set correctly for yourenvironment:```yamldatabase:  type: &quot;internal&quot;  internal:    port: 5432    databaseName: &quot;ranger&quot;    databaseUser: &quot;ranger&quot;    databasePassword: &quot;RangerPass123&quot;    databaseRootUser: &quot;rangeradmin&quot;    databaseRootPassword: &quot;RangerAdminPass123&quot;```You may also configure `volume:` persistence and resources, as well as the`resources:` for the backing database itself in the `database:` node. For acomplete list of available backing database properties, see our [referencedocumentation](../../../latest/k8s/ranger-configuration.html#internal-backing-database-server).### Configure TLS (optional)If your organization uses TLS, you must enable and configure Ranger to work withit. The most straightforward way to handle TLS is to terminate TLS at the loadbalancer or ingress, using a signed certificate. This method requires noadditional configuration for Ranger.[Ranger](https://cwiki.apache.org/confluence/display/RANGER/Index) can also beconfigured to listen on HTTPS directly.If you choose not handle TLS using those methods, you can instead configure itin the[`usersync:`](../../../latest/k8s/ranger-configuration.html#ldap-user-synchronization-server)and [`expose:`](../../../latest/k8s/ranger-configuration.html#exposing-the-cluster-to-outside-network) top-level nodes of the Ranger Helm chart.{% include note.html content=&quot;This is separate from configuring TLS on SEPitself.&quot; %}## Deploy RangerWhen Ranger is configured as desired for your organization, run the following command to deploy it:```yaml$ helm upgrade -i ranger starburst/starburst-ranger -f ranger-values.yaml```"
 },
 {
  "title": "Deploying and updating Starburst Enterprise",
  "url": "/platform-administrator/deploy-update.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} can be deployed in several ways, depending on yourorganization&#39;s infrastructure. Most organizations choose to install, configureand deploy {{site.terms.sep}} on Kubernetes. Follow the link most appropriate toyour environment to learn more about deploying {{site.terms.sep}}:* [Directly](../latest/k8s/overview.html) into Google Cloud, or through  [Google Cloud]({{ site.baseurl }}/ecosystems/google/index.html) using Helm  charts* [Directly](../latest/k8s/overview.html) into OpenShift, or through  [Red Hat Marketplace](../ecosystems/redhat/index.html) using  our Kubernetes operator* On AWS [using our AMI](../latest/aws/overview.html) or [using  EKS](../latest/k8s/overview.html)* On [Microsoft Azure](../ecosystems/microsoft/index.html)See our [latest reference manual](../latest/index.html) forcomprehensive documentation on installing and configuring{{site.terms.sep_full}} in your environment."
 },
 {
  "title": "Disclaimers",
  "url": "/disclaimers.html",
  "content": "# {{ page.title }}The following information applies to this site hosted at[https://docs.starburst.io](https://docs.starburst.io).## Copyright© 2017 - 2021, Starburst Data, Inc. Starburst, and Starburst Data are registeredtrademarks of Starburst Data, Inc. All rights reserved.## PrivacyThe [privacy policy](https://www.starburstdata.com/privacy-policy/) of Starburstapplies to all content on and usage of the site.## TrademarksProduct names, other names, logos and other material used on this site areregistered trademarks of various entities including, but not limited to, thefollowing trademark owners and names:  Apache Software Foundation  Apache Hadoop, Apache Hive, Apache Kafka, and other names  Amazon  AWS, S3, Glue, EMR, and other names  Azul Systems Inc  Zulu  Docker Inc.  Docker  Google  Google Cloud, GKE, YouTube, and other names  Microsoft  Azure, AKS, and others  Oracle  Java, JVM, OpenJDK, and other terms  The Linux Foundation  Kubernetes, Helm, Presto, Linux, and other names  Starburst Data  Starburst, Starburst Data, {{site.terms.sg}}, and other names"
 },
 {
  "title": "Try Starburst Enterprise with Docker",
  "url": "/starburst-enterprise/try/docker.html",
  "content": "# {{ page.title}}You can install {{site.terms.sep_first}} in a Docker container and runit on any Docker host. This is especially useful as a test or demo environmentor to evaluate {{site.terms.sep}}.{% include note.html content=&quot;You can run SEP with Docker on Mac or Windows computers for evaluation purposes only. Understand that these are likely to be memory-limited devices that cannot run complex queries. Production implementation of a SEP cluster on Docker is supported only on Linux servers.&quot; %}This page presumes you have Docker installed and running, and that you have somefamiliarity with Docker commands. For Mac and Windows evaluations, make sure youhave Docker Desktop installed and running.With custom configuration of the containers for the {{site.terms.sep}}coordinator and worker nodes, you can run a set of Docker containers toimplement a Linux-hosted {{site.terms.sep}} cluster on your local network or ona cloud provider. However, this is **not recommended**. Instead, Starburstoffers a full [Kubernetes deployment option](./k8s.html), including Dockercontainers and Helm charts for {{site.terms.sep}}, Apache Ranger, and HiveMetastore Service.## Docker image featuresThe default {{site.terms.sep}} image has the following characteristics:* There is *no* {{site.terms.oss}} CLI command installed in the Docker image  itself. You can, of course, run the CLI on the host.* The `jvm.config` file is set to use 1G maximum.* The following catalogs are installed:  * jmx  * memory  * system  * tpcds  * tpch## Initial run of {{site.terms.sep}} on Docker{{site.terms.sb}} provides a Docker image on Docker Hub that contains a trialconfiguration of {{site.terms.sep}}. Use the following command to download andrun this image:```shelldocker run -d -p 8080:8080 --name sepdock starburstdata/starburst-enterprise:latest```If you know you need a particular release, substitute its number for `latest`.For example, `starburstdata/starburst-enterprise:354-e`. For release 350 andearlier, you must use the identifier `starburstdata/presto:350-e`.The `docker run` options have the following meanings:| Option   | Meaning| ---------|---------------------------------------|| `–d`     | Detach                                || `–p`     | Map ports as `hostport:containerport` || &amp;#8209;&amp;#8209;`name`&amp;nbsp;| Provide a name for the image to use in subsequent Docker commands |{% comment %}Preserve the non-breaking hyphens and forced space in the last rowabove to prevent the first column wrapping before &quot;name&quot;.{% endcomment %}To make sure the server starts in Docker, continually view the Docker logs untilyou see &quot;======== SERVER STARTED ========&quot;.```shelldocker logs sepdockdocker logs sepdock...```Use the `docker ps` command to verify that the *sepdock* service isrunning. Run `docker ps -a` to locate a server that failed to start.### Verify the serverTo verify that your Docker-hosted {{site.terms.sep}} server is operating asexpected, run the Web UI as described in [Verify the server](./verify.html).### Run queriesTo run queries against your server, use the {{site.terms.oss}} CLI as describedin [CLI](../../data-consumer/clients/cli.html).## Add custom configurationThe {{site.terms.sb}}-provided Docker image gives you a certain set ofconfiguration files. But the whole point of running {{site.terms.sep_full}} isto query your own data sources. How do you provide your own configuration filesto the Docker-hosted server?{{site.terms.sb}} does *not* recommend going into the Docker image to changeconfiguration files there. Those changes would be lost the next time{{site.terms.sb}} updated the public Docker image to a new version, which yournext `docker run` command would automatically download and use.Instead, you can map the `etc` directory used by the {{site.terms.sep}} instancerunning in Docker to a local directory. Once configured, the Docker-hosted{{site.terms.sep}} uses the local `etc` directory as its primary source ofconfiguration files, and ignores the default settings in the Docker image.To do this requires one extra `docker run` option. If your Docker-hosted{{site.terms.sep}} is running now, first stop and remove it:```shelldocker stop sepdockdocker rm sepdock```### Populate the local etc directoryCreate a local directory to contain your custom {{site.terms.sep}} configurationfiles. For example:```shellmkdir -p /home (or /Users)//sepdock```Substitute your ** in this command.The simplest way to test your local `etc` directory is to use the set ofconfiguration files provided as examples for the O&#39;Reilly book [Trino: TheDefinitive Guide](https://www.starburst.io/info/oreilly-trino-guide/).Download these samples from their [GitHublocation](https://github.com/trinodb/trino-the-definitive-guide) either as azip file or a git clone.Unzip or clone the files into a local directory, such as `/home (or/Users)//bookfiles`. From there, copy the entire `etc` directory fromthe `single-installation` sample to your `/home (or /Users)//sepdock`directory. For example:```shellcd /home//bookfiles/single-installationrsync -av etc /home//sepdock/```or```shellcd /Users//bookfiles/single-installationrsync -av etc /Users//sepdock/```### Edit local configuration filesNavigate to your local `etc` directory. For example:```shellcd /home (or /Users)//sepdock/etc```Inspect the configuration files provided there for suitability with Docker. Forexample, the `jvm.config` file sets `-Xmx16G`, which might be too large forDocker running on Docker Desktop on Mac or Windows. Set that to a lower valuesuch as `-Xmx2G`.{% include note.html content=&quot;Make sure Docker is configured to reserve enoughmemory to handle the -Xmx setting you choose for SEP. For example, DockerDesktop&#39;s default RAM setting is 2G. Use Docker Desktop Preferences, Resourcestab to specify enough RAM to allow the Docker-hosted server to start and run.&quot;%}### Restart to use the local etc directoryThe `docker run` command has another useful option:| Option     | Meaning| -----------|---------|| `‑‑volume` | Specify a directory in the container to mount to a host directory |The syntax for the `‑‑volume` option is:`localPath:containerPath:options`Specify _localPath_ first, followed by _containerPath_, separated by a colon. No_options_ are needed. Options are described in [Dockerdocumentation](https://docs.docker.com/storage/volumes/#choose-the--v-or---mount-flag).If your Docker-hosted server is running, stop and remove it:```shelldocker stop sepdockdocker rm sepdock```Make sure you&#39;re still in the `/home (or /Users)//sepdock/etc`directory. Then rerun the Docker image, this time including a `‑‑volume` optionthat maps the current directory to the `etc` directory inside the Docker image,`/usr/lib/presto/etc`.```shellcd /home (or /Users/username/sepdock/etcdocker run -d -p 8080:8080 --volume $PWD:/usr/lib/presto/etc --name sepdock starburstdata/starburst-enterprise:latest```Continually view the Docker logs as before, waiting to see &quot;======== SERVERSTARTED ========&quot;:```shelldocker logs sepdock...docker logs sepdock...```Verify that the server is running by using the Web UI as described in [Verifythe server](./verify.html).To run queries, connect the {{site.terms.oss}}[CLI](../../data-consumer/clients/cli.html) to the Docker-hosted{{site.terms.sep}} instance. Run the `show catalogs;`, `show schemas;`, and`show tables;` commands to confirm the assets of the server.### Configure custom data sourcesThe local `etc` directory feature described in the previous section lets youwork locally to make changes to your Docker environment.Continue adding new catalogs and configuring features that customize connectionsto the data sources you need {{site.terms.sep}} to see.## Next stepsYou need a cluster of servers to use {{site.terms.sep}} in production or evenfor better testing. Do not attempt this with the Docker container only. Insteaduse Kubernetes and the [Starburst Kubernetes deploymentsupport]({../../latest/k8s.html), including Docker containers and Helm chartsfor {{site.terms.sep}}, Apache Ranger, Hive Metastore Service, and our [cacheservice](../../latest/admin/cache-service.html)."
 },
 {
  "title": "EKS cluster creation",
  "url": "/ecosystems/amazon/eks/eks-cluster-creation.html",
  "content": "# {{page.title}}AWS requires EKS clusters to have a minimum of two availability zones (AZs), but{{site.terms.sep_first}} [best practices requires that the coordinator andworkers reside in a singleAZ](./eks-networking.html#best-practice-use-a-single-availability-zone-inside-your-existing-vpc),in a single subnet.{% include warning.html content=&quot;SEP has specific requirements for sizing,placement, and sharing of resources. You must ensure that your EKS cluster meetsall requirements described in our [cluster requirementssection](../../../latest/k8s/requirements.html#k8s-cluster-requirements). In addition, you must use only EC2-based instance types.&quot; %}This document covers how to configure a new EKS cluster to ensure that all{{site.terms.sep}} resources are co-located and follow best practices.## PrerequisitesThe following tools, policies, and certificates are required to create an{{site.terms.sep}} cluster in EKS:* `helm`* `kubectl`* `eksctl` version 0.54.0 or later* IAM Polices for Glue, S3, as desired* CA-signed certificate for HTTPS/TLS (e.g., `for starburst.example.com`)  if using AD/LDAP authenticationThe following example IAM addon policies must be present for each node group:managedNodeGroups:  - name:     iam:      withAddonPolicies:        externalDNS: true        albIngress: true## Create your `sep_eks_cluster.yaml` fileYour YAML file should start with the following two lines:```yamlapiVersion: eksctl.io/v1alpha5kind: ClusterConfig```Next, add the `metadata:` section to describe your cluster. The followingexample shows the minimum required fields, as well as suggested tags for astaging cluster running in `us-east-2`:```yamlmetadata:  name: my-sep-cluster  region: us-east-2  version: &quot;1.20&quot;  tags:    cloud: aws    environment: staging    info: &quot;EKS cluster for Starburst Enterprise staging environment&quot;    user: your.username```## Specify your networkingAdd the two required AZs, and the existing subnets associated with them:```yamlvpc:  subnets:     private:       us-east-2a:         id: subnet-0Subnet1ID8String2       us-east-2b:         id: subnet-0Subnet0ID2String3```For purposes of this example, we set up {{site.terms.sep}} in the `us-east-2a`AZ.Follow the instructions in the [EKS networking guide](./eks-networking.html) toconfigure:* DNS* Ingress* Application load balancer (ALB) controllerWe strongly suggest that you read the best practices section of thenetworking guide to ensure you follow best practices forperformance and cost controls.## Create your EKS `managedNodeGroups:`{{site.terms.sb}} recommends using `managedNodeGroups:` to create the pools ofinstances available to {{site.terms.sep}} and its associated services.`managedNodeGroups:` in EKS have the additional benefit of automating SIGTERMdelivery to {{site.terms.sep}} workers and the coordinator when a Spot instanceis removed to enable graceful shutdown. With `nodeGroups:` additionaldevelopment must be done, outside of {{site.terms.sep}}, to allow for gracefulshutdown.{% include caution.html content=&quot;Sudden removal of an expected resource cancause SEP to fail. You must carefully consider [whether to useSpot instances](./eks-networking.html#spot-instances-and-single-azs).&quot; %}In this example, a managed node group called `sep_support_services` is createdalongside the `sep` managed node group. The support services managed node groupruns HMS and Ranger, if those services are required in your environment.The coordinator is required for a functioning {{site.terms.sep}} cluster, and{{site.terms.sep}} architecture only allows for one coordinator at a time. Youmust ensure that resource availability is maximized. This can be done in twoways:* **Create a separate node group for the coordinator with two or more instances  on cold standby.** In the case that the instance the coordinator is running on  goes down, having two or more instances allows the coordinator pod to be  restarted on the unaffected instance.* **Define a priority class for the coordinator.** This allows the cluster to  eject a worker pod and start on that machine without waiting for a new node to  start if the coordinator’s instance of the same size is lost. The worker’s  termination grace period, which is five minutes by default, is still  respected.{% include warning.html content=&quot;AWS best practices typically recommend creating acluster with three node groups, each in a different availability zone (AZ) totolerate the loss of an AZ. This approach comes with significant performance andcost penalties for SEP, as internode traffic crosses availability zones.We [strongly recommend all node groups remain within the sameAZ](./eks-networking.html#best-practice-use-a-single-availability-zone-inside-your-existing-vpc),with failure tolerance handled by using multiple clusters.&quot; %}EKS requires the first three IAM policy ARNs shown in the following example forboth node groups. The fourth policy ARN for Glue illustrates usingan additional policy to allow access to EKS, S3, and Glue in the account used forthe EKS cluster without supplying credentials. It is not required.{% include note.html content=&quot;You must repeat all tags specified in themetadata tags in both entries in `managedNodeGroups:`.&quot; %}```yamlmanagedNodeGroups:  - name: sep    tags:      cloud: aws      environment: staging      info: &quot;EKS cluster for Starburst Enterprise staging environment&quot;      user: your.username    availabilityZones: [us-east-2a]    labels:      allow-workers: workers    instanceTypes: [&quot;m5.xlarge&quot;, &quot;m5a.xlarge&quot;, &quot;m5ad.xlarge&quot;]    desiredCapacity: 2    minSize: 2    maxSize: 4    privateNetworking: true    ssh:      allow: true      publicKeyName: en-field-key    iam:      attachPolicyARNs:        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy        - arn:aws:iam::47ID9String78:policy/EKS-S3-Glue  - name: sep_support_services    tags:      cloud: aws      environment: staging      info: &quot;EKS cluster for Starburst Enterprise staging environment&quot;      user: your.username    availabilityZones: [us-east-2b]    spot: true    instanceTypes: [&quot;m5.large&quot;, &quot;m5a.large&quot;, &quot;m5ad.large&quot;]    desiredCapacity: 1    minSize: 1    maxSize: 1    privateNetworking: true    ssh:      allow: true      publicKeyName: en-field-key    iam:      attachPolicyARNs:        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy        - arn:aws:iam::47ID9String78:policy/EKS-S3-Glue```## Save your file and create your clusterWhen you are finished adding the required content to your `sep_eks_cluster.yaml`file, save it and use it to create your cluster with `eksctl`:```shell$ eksctl create cluster -f sep_eks_cluster.yaml```When the command completes successfully, the following message appears inyour terminal:```shell2021-07-14 14:28:56 [✓] EKS cluster &quot;my-sep-cluster&quot; in &quot;us-east-2&quot; region is ready```{% include note.html content=&quot;If you get an error about not being able to deployKubernetes 1.20, ensure you are running `eksctl` version 0.54.0 or later.&quot; %}"
 },
 {
  "title": "Configure IAM in EKS clusters",
  "url": "/ecosystems/amazon/eks/eks-iam.html",
  "content": "# {{page.title}}The best practice for implementing [Amazon IAM](https://aws.amazon.com/iam/) inan EKS cluster is to use an EKS service account. This setup works with the`hive.s3.iam-role` catalog property, or with an [S3 securitymapping](../../../latest/connector/hive-s3.html#s3-security-mapping).Using S3 security mapping with EKS cluster versions prior to version 1.19requires a `securityContext:` definition in the {{site.terms.sep_first}} Helmchart as in the following example:```yamlsecurityContext:  fsGroup: 65534```The {{site.terms.sep}} chart uses the default account for its defined namespace,therefore you must create the service account with the name &quot;default&quot;, asin the following example `eksctl` command:```shell$ eksctl create iamserviceaccount  --name default  --namespace dataservices  --cluster sepstaging  --attach-policy-arn arn:aws:iam:::policy/eks_service_account_wm  --approve  --override-existing-serviceaccounts ```"
 },
 {
  "title": "AWS EKS networking",
  "url": "/ecosystems/amazon/eks/eks-networking.html",
  "content": "# {{page.title}}This document helps you to understand networking requirements and configurationfor {{site.terms.sep_first}} on AWS EKS.## Best practice: Use a single availability zone inside your existing VPCWhen deploying {{site.terms.sep}} into an AWS EKS cluster, it is importantto ensure that your cluster resources are located with the following in mind:* cluster communications latency* data ingress/egress costs* IP address availabilityUsing your existing VPC is a key cost control measure. It ensures that costsassociated with data ingress and egress are kept to a minimum. Every new VPCcomes with a NAT gateway. And, while NAT gateways are inexpensive, costs fordata transferred through that gateway add up very quickly. As a best practice,placing your {{site.terms.sep}} inside of your existing VPC, co-resident with asmany of your data sources as possible not only keeps costs down, it also greatlysimplify networking and security.Equally as important is performance. No matter if you use an existing or newVPC, {{site.terms.sep}} must run in a single availability zone (AZ) to ensurethe best performance possible. To accomplish this, use node groups. The nodegroups are then tied to a single AZ using affinity or node selection rules.For {{site.terms.sep}}, two [managed nodegroups](./eks-cluster-creation.html) are required. The {{site.terms.sep}}coordinator and workers are deployed to one group while support services, suchas HMS, Ranger and nginx, are deployed to the second node group.{% include warning.html content=&quot;You must use an existing VPC in order toenforce the use of a single AZ. Do not use eksctl to create a new VPC at clustercreation time.&quot; %}### Spot instances and single AZsAWS EC2 [Spotinstances](https://docs.aws.amazon.com/es_es/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html)are sometimes desireable to keep costs down. However, they are inherentlyunreliable, as they can be recalled by the EC2 platform at any time. Because ofhow {{site.terms.sep}} drives performance by dividing query processing for anysingle query, sudden removal of an expected resource can cause failures.Further, when using a single AZ, Spot instance rebalancing is diminished, aspools in only the single AZ are available to it. Using Spot instances may stillbe desirable in some cases. In this case, to use Spot instances for the{{site.terms.sep}} workers, create a third node group to contain them,separating the non-Spot coordinator node group from the Spot worker node group.The remaining node group contains support services as before.### IP address requirementsAn important consideration in using an existing VPC is IP address availability.As part of standing up your {{site.terms.sep}} cluster, you must ensure thatsufficient IP addresses are reliably available for use by your{{site.terms.sep}} instances.In EKS clusters, AWS creates one subnet per availability zone. Usually, theseare configured as /20 Classless Inter-Domain Routings (CIDR) with 4,091 IPaddresses available for use (an additional 5 are reserved by the clusteritself). {{site.terms.sep}} requires that all hosts, both workers andcoordinators, are sized identically. Each of these instances has a maximumnumber of IP addresses that can be assigned to it, and EKS reserves twice thatnumber of addresses for it.For purposes of this discussion, we assume an {{site.terms.sep}} deploymentinvolving six m5.xlarge workers and one m5.xlarge coordinator. Each of thisinstances can have a maximum of 15 IP addresses in use, one per each of the 15interfaces it comes with. An additional 15 are reserved, for a total of 30 IPaddresses needed per instance. Those seven instances together then require 210IP addresses:```shell  ( 7 m5.xlarge instances)x ( 15 interfaces per instance )x ( 2 IPs per interface )= 210 IP addresses needed```In this example, you must ensure that a minimum of 210 IP addresses arereliably available for use by your {{site.terms.sep}} instances at all times.{% include note.html content=&quot;If you are taking advantage of AWS EKSautoscaling, you must calculate the number of addresses needed on the maximumallowable number of workers.&quot; %}### Using subnetsYou can use an existing subnet or create a new one. It must be configured with aroute out to the Internet either via a NAT gateway or an IGW to allow your EKScluster to communicate with the AWS EKS management backplane. Costconsiderations for these communications are minimal.### Considerations if you must use VPC peeringIf you cannot place {{site.terms.sep}} within your current VPC because of ascarcity of IP addresses, as an alternative you can create a peering connectionwith the new EKS cluster’s VPC to avoid the often cost-prohibitive operation ofall data passing through the NAT gateway. VPC peering requires additional setup,and comes with potential downsides:* Does not scale well.* Transitive routing is not available.* Peering connections are a resource that must be managed.* Firewall rules must be carefully managed.Additionally, you must ensure that the CIDR you set for the new{{site.terms.sep}} VPC does not match or overlap with your existing VPC’s CIDR.VPCs with overlapping CIDRs cannot create peering connections.The following are required when using internet-facing ALBs:* Subnets tagged with kubernetes.io/cluster/cluster-name → shared* Subnets tagged with kubernetes.io/role/elb → 1The following is required when using ALBs that are not internet-facing:* kubernetes.io/role/internal-elb → 1## Configure DNSInstall the[ExternalDNS](https://artifacthub.io/packages/helm/bitnami/external-dns#parameters)add-on to automatically add DNS entries to [Amazon Route53](https://aws.amazon.com/route53/) once ingress resources are created, asshown in the following example `external-dns.yaml` file:```yamlprovider: awsaws:  zoneType: public  region: us-east-2txtOwnerId:    # Identify thistxtPrefix:      # external DNS instancezoneIdFilters:  -  # ID of AWS R53 Hosted Zone# This allows external-dns to delete entries as wellpolicy: sync```Run the following commands to complete the install. Be sure to use the latestartifact of[external-dns](https://artifacthub.io/packages/helm/bitnami/external-dns) andspecify its version in the command:```shell$ helm repo add bitnami https://charts.bitnami.com/bitnami$ helm repo update$ helm upgrade external-dns bitnami/external-dns --namespace kube-system --install --version    --values external-dns.yaml```## Best practice: Configure ingress access for {{site.terms.sep}} and Ranger through a controllerAccessing your {{site.terms.sep}} cluster from external clients requires accessto the coordinator and Ranger pods within the K8s cluster. This can be handledeither through an AWS layer 7 [load balancer (ALB) ingresscontroller](https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html),or by deploying an Nginx ingress controller behind a classic layer 4 loadbalancer.There are technical considerations for either approach:*  Using an ALB ingress controller   * TLS termination is taken care of in the load balancer.   * Easy to integrate AWS Certificate Manager for publicly-signed     certificates.   * A separate load balancer resource is be created for each service     (Starburst, Ranger, etc.)* Using an Nginx ingress controller   * Nginx can route traffic to different services using a single load balancer.   * Internal or alternate CA issue certificates, as it is not possible to use     Amazon Certificate Manager with an Nginx pod.For either approach, you must first configure and apply changes to yourcluster&#39;s `ingress.yaml` file. When you have verified that the ingress for thecluster is working properly, proceed with configuring and deploying your ingresscontroller of choice.{% include caution.html content=&quot;While it is possible to set up ingress directlyin Starburst Helm charts, it is not recommended. Using standard K8s controllersas outlined here avoids complex networking and certificate issues inherent inconnecting directly through application-specific ingress configurations. &quot; %}### Configure `ingress.yaml`The following `ingress.yaml` file provides the necessary networkingconfiguration for both {{site.terms.sep}} and Ranger. You can safely ignore theRanger configuration if your organization does not use it:```yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: starburst  annotations:    kubernetes.io/ingress.class: alb    alb.ingress.kubernetes.io/group.name: sep    alb.ingress.kubernetes.io/scheme: internet-facing    alb.ingress.kubernetes.io/listen-ports: &#39;[{&quot;HTTP&quot;: 80},{&quot;HTTPS&quot;: 443}]&#39;    alb.ingress.kubernetes.io/ssl-redirect: &#39;443&#39;    alb.ingress.kubernetes.io/success-codes: 200,303    alb.ingress.kubernetes.io/tags: key1=value1, key2=value2    alb.ingress.kubernetes.io/certificate-arn: spec:  tls:    - hosts:        -   rules:    - host:       http:        paths:          - path: /            pathType: Prefix            backend:              service:                name: starburst                port:                  number: 8080``````yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: ranger  annotations:    kubernetes.io/ingress.class: alb    alb.ingress.kubernetes.io/group.name: sep    alb.ingress.kubernetes.io/scheme: internet-facing    alb.ingress.kubernetes.io/listen-ports: &#39;[{&quot;HTTP&quot;: 80},{&quot;HTTPS&quot;: 443}]&#39;    alb.ingress.kubernetes.io/ssl-redirect: &#39;443&#39;    alb.ingress.kubernetes.io/success-codes: 200,302    alb.ingress.kubernetes.io/tags: key1=value1, key2=value2    alb.ingress.kubernetes.io/certificate-arn: spec:  tls:    - hosts:        -   rules:    - host:       http:        paths:          - path: /            pathType: Prefix            backend:              service:                name: ranger                port:                  number: 6080```The following are important to remember as you modify the preceding examplewith the particulars of your organization:* **alb.ingress.kubernetes.io/certificate-arn** - If AWS ACM contains multiple  certificates matching the hostname, specify certificate ARN explicitly,  otherwise you can remove this annotation.* **alb.ingress.kubernetes.io/group.name** - {{site.terms.sep}} forces the ALB  controller to combine multiple Kubernetes ingress resources into a single ALB  on AWS, reducing costs.* **alb.ingress.kubernetes.io/success-code** - Contains HTTP response codes of  the root (/) resource that are used by target group health checks. By default  they accept only 200, but SEP responds with 303, and Ranger with 302.* **alb.ingress.kubernetes.io/listen-ports** - Ensures that ALB listens on both  HTTP and HTTPS ports* **alb.ingress.kubernetes.io/ssl-redirect** - Redirects from HTTP to HTTPSOnce all changes to `ingress.yaml` are made, apply the customizations using thefollowing command in the namespace that contains {{site.terms.sep}}:```shell$ kubectl apply -f ingress.yaml```You must wait until the ALB status changes from `Provisioning` to `Ready` beforethe networking behaves as expected with the configuration changes.### Using an AWS load balancer controllerThe [AWSdocumentation](https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html)provides complete instructions for installing the ALB ingress controller on yourEKS cluster. The following example commands provide an overview of the processto install ALB controller that automatically discovers ingress resources andpropagates them to AWS ALB and target groups.```shell$ helm repo add eks https://aws.github.io/eks-charts$ helm repo update$helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system   --set clusterName=```Once the ALB controller is installed, you must define ingress in your{{site.terms.sep}} chart:```yamlexpose:  type: &quot;ingress&quot;  ingress:    ingressName: &quot;starburst-ingress&quot;    serviceName: &quot;starburst&quot;    servicePort: 8080    tls:      enabled: true    host: &quot;starburst.example.com&quot;    path: &quot;/*&quot;    annotations:      kubernetes.io/ingress.class: alb      alb.ingress.kubernetes.io/scheme: interal      alb.ingress.kubernetes.io/target-type: ip```With the defined host, the ingress controller automatically associatescertificates in ACM with a matching Subject Alternative Name or with a wildcardsuch as &quot;*.example.com&quot; with this load balancer.By default, the controller creates an ALB, configures it with TLS certificatestored in the AWS Certificate Manager (private key is not exposed toKubernetes), and terminate TLS at ALB.Target groups are configured to send traffic to ports on Kubernetes workernodes. A NodePort service type must be configured in Kubernetes to exposespecific ports across all nodes.The following traffic flows apply when using &quot;ingress&quot; as the `expose:` type:* Kubernetes pod: Runs the workload and listens on a port* kube-proxy: Listens on NodePort, proxies traffic to the pods in cluster* Target Groups: Discover EKS worker nodes, run health checks* ALB: Terminates TLS, performs HTTP routing* Route 53 A record: Points to ALB endpoints in public subnets{% include caution.html content=&quot;When using ingress, you must specify the TLScertificate as a Kubernetes secret if TLS is enabled.&quot; %}### Using NginxThe [Nginxcontroller](https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/)is deployed as an additional pod in the EKS cluster. Certificate management forHTTPS connections is implemented using one of the following Helm-managedcertificates:* A self-signed certificate* A internal CA-signed certificate* A Let&#39;s Encrypt-issued certificate using [cert-manager.io](https://cert-manager.io/docs/tutorials/acme/ingress/)The following is an example of Nginx chart to control access to an{{site.terms.sep}} cluster:```yamlapiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata:  name: nginx-ingress  annotations:    kubernetes.io/ingress.class: nginx    service.beta.kubernetes.io/aws-load-balancer-internal: &quot;true&quot;    service.beta.kubernetes.io/aws-load-balancer-type: &quot;nlb&quot;spec: tls:  - hosts:    - starburst.example.com    secretName: tls-secret  rules:  - host: starburst.example.comhttp: paths:      - backend:          serviceName: trino          servicePort: 8080 controller:  extraArgs:    ##   default-ssl-certificate: &quot;/&quot;    default-ssl-certificate: default/cert-example.com```In this case, the default certificate is a Kubernetes secret specified byapplying the following YAML:```yamlapiVersion: v1kind: Secretmetadata:  name: cert-example.com  namespace: defaultdata:  ## Both tls.crt and tls.key are base64-encoded PEM files on a single line  tls.crt:   tls.key: ```In the {{site.terms.sep}} `values.yaml` file, ingress is exposed via thefollowing YAML block:```yamlexpose:  type: &quot;ingress&quot;  ingress:    ingressName: &quot;nginx&quot;    serviceName: &quot;trino&quot;    servicePort: 8080    ingressClassName:    tls:      enabled: true    host: &quot;starburst.example.com&quot;    path: &quot;/&quot;    pathType: Prefix    annotations:      kubernetes.io/ingress.class: &quot;nginx&quot;```All inbound HTTPS traffic to `starburst.example.com` is now directed tothe {{site.terms.sep}} coordinator. A similar configuration is applied in the{{site.terms.sb}} Ranger Helm with `ranger.example.com` as the host."
 },
 {
  "title": "Install SEP in EKS",
  "url": "/ecosystems/amazon/eks/eks-sep.html",
  "content": "# {{page.title}}Before you begin installing {{site.terms.sep}} on EKS, ensure that you have donethe following:* Created a [suitable EKS cluster](./eks-cluster-creation.html)* Ensured that your [networking is properly configured](./eks-networking.html)* Set up [autoscaling for your  cluster](../../../platform-administrator/autoscaling.html), if desiredWe suggest that you also first consider whether you require the followingadditional services along with {{site.terms.sep}}:* A [metastore  service](../../../platform-administrator/metastores.html) if you plan to  querying data in distributed object stores* The [Apache Ranger service to provide  RBAC](../../../platform-administrator/deploy-ranger.html) in  {{site.terms.sep_first}}* The {{site.terms.sb}} [cache  service](../../../data-engineer/cache-service.html) to configure and automate  the management of table scan redirections and materialized views in supported connectorsHaving these service requirements in mind provides context for some sections ofthe {{site.terms.sep}} Helm chart. Each of these services are also availablefrom {{site.terms.sb}} as Helm charts.After you have created your cluster and ensured the networking works as expected,[complete installation instructions](../../../latest/k8s/installation.html#sep-installation) for the latest version are available in ourreference documentation.A [complete list of configurationproperties](../../../latest/k8s/sep-configuration.html) and [exampleusage](../../../latest/k8s/sep-config-examples.html) is also available for yourreference.Once your {{site.terms.sep}} installation is complete, you can install one ormore of the additional services."
 },
 {
  "title": "Entities",
  "url": "/starburst-galaxy/security/entities.html",
  "content": "# {{page.title}}The [security system of {{site.terms.sg}}](./index.html) controls access to allentities including clusters, catalogs, users, and roles.The following table lists all available types of entities and the associated[privileges](./privileges.html), if any.            Entity      Description      Privileges                  CLUSTER      A single cluster specified by          name.      USE_CLUSTERSTART_STOP_CLUSTER              CATALOG      A single catalog specified by          name.      None              ROLE      A single role specified by name.      None              USER      A single user specified by name.      None      "
 },

 {
  "title": "Starburst Enterprise in GCP Marketplace",
  "url": "/ecosystems/google/gcp-marketplace.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} is available directly through the [Google CloudPlatform (GCP)Marketplace](https://console.cloud.google.com/marketplace/product/starburst-public/starburst-enterprise)to run on a variety of instance types. Our GCP marketplaceoffering allows you to easily set up a monthly contract, after which you candeploy {{site.terms.sep}} using the command line or Google&#39;s Click to Deployon Google Kubernetes Engine (GKE).## Using command line deployment vs. Click to DeployWe strongly recommend that you deploy using the command line. Google&#39;s Click toDeploy option is best suited for small proofs-of-concept. Customization optionsmay be limited.Once deployed via the command line, {{site.terms.sep}} is customizable,including connecting to services such as:* Hive Metastore (HMS)* Apache Ranger* Starburst Cache Service## Support for marketplace offerings{{site.terms.sb}} offers the following support for our marketplace subscribers:- Email-only support- Five [email issues to gcpsupport@starburstdata.com](mailto:gcpsupport@starburstdata.com) per month- First response SLA of one business day- Support hours between 9 AM - 6 PM US Eastern Time## Set up your subscriptionBefore you begin, you must have a GCP login with the ability to subscribe toservices.{% include note.html content=&quot;In the following steps, if the blue button says**Purchase** instead of **Configure**, the GCP account you are using is notsetup for subscription billing.&quot; %}To subscribe to {{site.terms.sep}} through the GCP Marketplace:1. Log in with your billable subscriber account and   [access the Starburst Enterprise offering directly](https://console.cloud.google.com/marketplace/product/starburst-public/starburst-enterprise),   or enter “{{site.terms.sep_full}}” in the marketplace search field and   select **{{site.terms.sep_full}} - Distributed SQL Query Engine**.1. Click **Configure**.1. On the resulting screen, select either **Deploy via command line**   (recommended), or **Click to Deploy on GKE**.## Deploy via command line1. Reach out to [{{site.terms.support}}](../../support/index.html) to have your   service account added with to the {{site.terms.sb}} Google Container Registry   (GCR) with the `Storage Object Viewer` role.1. Create a GKE Standard cluster with two nodepools. One nodepool is used for   {{site.terms.sep}} while the other is used for Ranger and HMS. The following   are the recommended minimal specifications to use for a proof-of-concept   deployment:   * Cluster name: my-sep-cluster   * Location type: zonal (lower latency)   * K8s version: 1.20.9-gke.1001   * Primary nodepool name: default-nodepool     * Number of nodes: 3     * Machine configuration: e2-standard-16 (16 CPU and 64 GB RAM)   * Supplementary nodepool name: nonsep     * Number of nodes: 1     * Machine configuration: e2-standard-8 (8 CPU and 32 GB RAM)1. Set the following environment variables to be used with `kubectl` commands as   follows:   ``` shell   $ export TAG=2.2.0   $ export APP_INSTANCE_NAME=   ```1. Select the service account that you added to the {{site.terms.sb}} GCR in   Step 1, and click **Generate license key**. Download the resulting file.1. Apply the downloaded license file with the following command:   ``` shell   $ kubectl apply -f license.yaml   ```1. Confirm that the `-license` secret has been created with   the following command:   ``` shell   $ kubectl describe secret $APP_INSTANCE_NAME-license   ```1. Use the following commands to retrieve and unzip the GCP umbrella chart Helm   chart:   ``` shell   $ wget https://storage.googleapis.com/starburst-enterprise/helmCharts/sep-gcp/starburst-enterprise-platform-charts-$TAG.tgz   $ tar -zxvf starburst-enterprise-platform-charts-$TAG.tgz   ```1. Use `kubectl` to apply Application CRD to avoid harmless errors:   ``` shell   $ kubectl apply -f &quot;https://raw.githubusercontent.com/GoogleCloudPlatform/marketplace-k8s-a   ```1. Generate the complete Kubernetes deployment manifest as follows. The command   as shown assumes the nodepool names as described in previous steps:   ```shell   $ helm template &quot;$APP_INSTANCE_NAME&quot; .         --set deployerHelm.image=&quot;gcr.io/starburst-public/starburstdata/deployer:$TAG&quot;         --set reportingSecret=&quot;$APP_INSTANCE_NAME-license&quot;         --set starburst-enterprise.image.repository=&quot;gcr.io/starburst-public/starburstdata&quot;         --set starburst-enterprise.image.tag=&quot;$TAG&quot;         --set starburst-enterprise.initImage.repository=&quot;gcr.io/starburst-public/starburstdata/starburst-enterprise-init&quot;         --set starburst-enterprise.initImage.tag=&quot;$TAG&quot;         --set metricsReporter.image=&quot;gcr.io/starburst-public/starburstdata/metrics_reporter:$TAG&quot;         --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;         --set starburst-enterprise.coordinator.resources.limits.cpu=15         --set starburst-enterprise.coordinator.resources.requests.cpu=15         --set starburst-enterprise.coordinator.resources.memory=&quot;56Gi&quot;         --set starburst-enterprise.worker.replicas=2         --set starburst-enterprise.worker.resources.limits.cpu=15         --set starburst-enterprise.worker.resources.requests.cpu=15         --set starburst-enterprise.worker.resources.memory=&quot;56Gi&quot;         --set starburst-ranger.admin.image.repository=&quot;gcr.io/starburst-public/starburstdata/starburst-ranger-admin&quot;         --set starburst-ranger.admin.image.tag=&quot;$TAG&quot;         --set starburst-ranger.usersync.image.repository=&quot;gcr.io/starburst-public/starburstdata/ranger-usersync&quot;         --set starburst-ranger.usersync.image.tag=&quot;$TAG&quot;         --set starburst-ranger.gcpExtraNodePool=&quot;nonsep&quot;         --set starburst-ranger.enabled=true         --set starburst-hive.image.repository=&quot;gcr.io/starburst-public/starburstdata/hive&quot;         --set starburst-hive.image.tag=&quot;$TAG&quot;         --set starburst-hive.gcpExtraNodePool=nonsep         --set starburst-hive.enabled=true  &gt; sep_manifest.yaml   ```1. Apply the newly-generated K8s manifest with the following command:   ```shell   $ kubectl apply -f sep_manifest.yaml   ```1. Check to see when the cluster is created and available:   ```shell   $ kubectl get pods -o wide    NAME                                                     READY   STATUS      RESTARTS   AGE    IP           NODE                                             NOMINATED NODE   READINESS GATES    coordinator-64cfdb94fd-v6bxv                             2/2     Running     0          4h8m   10.28.3.8    gke-test-mp-cluster-default-pool-5b89684f-6g3b                  hive-7c8b5b5495-v9gwz                                    2/2     Running     0          4h8m   10.28.0.27   gke-test-mp-cluster-nonsep-0be06378-b718                        ranger-7c6b59bdd5-b9v8s                                  2/2     Running     0          4h8m   10.28.0.28   gke-test-mp-cluster-nonsep-0be06378-b718                        starburst-enterprise-1-lic-secret-job-nfmqg              0/1     Completed   0          4h8m   10.28.2.31   gke-test-mp-cluster-default-pool-5b89684f-qfvp                  starburst-enterprise-1-metrics-reporter-9f9f5f77-7nvd2   2/2     Running     0          4h8m   10.28.2.30   gke-test-mp-cluster-default-pool-5b89684f-qfvp                  worker-76ff548b96-c2rwz                                  1/1     Running     0          4h8m   10.28.2.32   gke-test-mp-cluster-default-pool-5b89684f-qfvp                  worker-76ff548b96-wj996                                  1/1     Running     0          4h8m   10.28.1.12   gke-test-mp-cluster-default-pool-5b89684f-kfz2                 ```1. Once you have confirmed that the cluster is running, ensure that the metrics   reporter is able to submit metrics to GCP metering service. The following   command should return a `Report submission status: 200 : OK` message:   ```shell   $ kubectl logs deployment/$APP_INSTANCE_NAME-metrics-reporter -c metrics-reporter   ```1. Verify that there are no errors reported by ubbagent:   ```shell   $ kubectl logs deployment/$APP_INSTANCE_NAME-metrics-reporter -c ubbagent   ```### Next stepsReview our guides to [Catalogs](../../data-engineer/catalogs.html) and[Connectors](../../latest/connector/starburst-connectors.html), and takeadvantage of our free comprehensive training videos that cover{{site.terms.sep}} security and performance:* [Security](../../platform-administrator/security.html)* [Performance](../../platform-administrator/performance-tuning.html)## Click to DeployGoogle&#39;s Click to Deploy option is best suited for small proof-of-conceptdeployments. Customization options may be limited. We strongly recommend thatyou begin with the *Deploy via command line** option.1. Select an existing cluster or create a GKE Standard cluster.   {{site.terms.sep}} requires two nodepools. One nodepool is used for   {{site.terms.sep}} while the other is used for Ranger and HMS. The following   are the recommended minimal specifications to use for proofs-of-concept:   * Cluster name: my-sep-cluster   * Location type: zonal (lower latency)   * K8s version: 1.20.9-gke.1001   * Primary nodepool name: default-nodepool     * Number of nodes: 3     * Machine configuration: e2-standard-16 (16 CPU and 64 GB RAM)   * Supplementary nodepool name: nonsep     * Number of nodes: 1     * Machine configuration: e2-standard-8 (8 CPU and 32 GB RAM)1. The default values displayed in the configuration screen are absolute   minimums, and are not considered performant. You must change them to   represent the resources available in the selected cluster.1. Select a namespace. Do not use &quot;default&quot; if there is another   {{site.terms.sep}} already running in it.1. To enable global security with Apache Ranger:   1. Check the **Enable Global Security** box. This   creates a new Ranger instance that is configured to communicate with your   {{site.terms.sep}} cluster. Note that you cannot connect to an existing   Ranger instance with Click to Deploy.   1. Provide an admin and service user password for Ranger. These must be at   least 8 alpha-numeric characters in length.1. You must click to **Enable Hive Metastore** if you are connecting any Hive,   Iceberg or Delta Lake data sources.1. Provide a reporting service account.1. Click **Deploy**. Once the application is deployed, it is available in the   GCP Console.### Next stepsThe following pages introduce key concepts and features in {{site.terms.sep}}:* [Catalogs](../../data-engineer/catalogs.html) and  [Connectors](../../latest/connector/starburst-connectors.html)* [Security](../../platform-administrator/security.html)* [Performance](../../platform-administrator/performance-tuning.html)"
 },
 {
  "title": "Google Cloud Storage catalogs",
  "url": "/starburst-galaxy/catalogs/gcs.html",
  "content": "# {{page.title}}You can use a Google Cloud Storage (GCS) catalog to configure access to a [CloudStorage](https://cloud.google.com/storage/) object storage hosted on GoogleCloud.The metadata about the objects and related type mapping needs to be stored in ametastore. You can use a Hive Metastore, or the built-in metastore.## Configure a catalogTo create a GCS catalog, select **Catalogs** in the main navigation and click**Configure a catalog**. Click on the **Google Cloud Storage** button in the**Select a data source** screen.{% include_relative name-description.md %}{% include_relative read-only.md %}### Authentication to GCSProvide the **GCS JSON key** to grant access to the object storage.### Metastore configuration{% include_relative hms.md %}{% include_relative sgm.md %}{% include_relative save.md %}"
 },
 {
  "title": "Get started with Starburst Galaxy",
  "url": "/starburst-galaxy/get-started.html",
  "content": "# {{ page.title }}If your organization is new to {{site.terms.sg}}, you need to request access.Visit the URL you receive provide your email address as username.You receive an activation code in your email shortly after. Use the code tocreate your account and supply a domain for your account.Now you can login at the domain. Create a password for your account, which alsoacts as administrator.With this account you can proceed to create [catalogs](./catalogs/index.html),[clusters](./clusters/index.html), or perform [administrativetasks](./account/index.html) such as creating further users.{% include video.html content=&quot;See it all in action with thedemo videoof Starburst Galaxy.&quot; %}"
 },
 {
  "title": "Google Kubernetes Engine deployments",
  "url": "/ecosystems/google/gke.html",
  "content": "# {{page.title}}The [Google KubernetesEngine](https://cloud.google.com/kubernetes-engine) (GKE), is certified to workwith {{site.terms.sep_first}}.## IntroductionThis user guide provides information on using {{site.terms.sb}} Helm charts todeploy {{site.terms.sep}}, and eventually a cluster in Google Cloud - KubernetesApplications. More information is available in our [Kubernetes referencedocumentation](../../latest/k8s/overview.html), which includes acustomization guide, examples and tips.Before you get started installing {{site.terms.sep}}, we suggest that you readour [customization guide](../../latest/k8s/sep-configuration.html) to learnhow to customize {{site.terms.sep}} clusters.We also have a helpful [installationchecklist](../../latest/k8s/installation.html) for an overview of the generalHelm-based installation and upgrade process.We recommend that you use Google Cloud&#39;s marketplace UI with **Click to Deployon GKE** to deploy. The command line **Deploy via command line** installation isnot supported.## Getting startedSetting up and deploying {{site.terms.sep}} to a Google Kubernetes Enginecluster using Google Cloud is pretty straightforward.You can get started by simply following the UI instructions in the[marketplace](https://console.cloud.google.com/marketplace/details/starburst-public/starburst-presto).### Cluster requirements{{site.terms.sep}} can be deployed to an existing GKE cluster, or to a newlycreated one. If using an existing GKE cluster, make sure it is configured withthe appropriate number and type of instances needed to deploy an{{site.terms.sep}} cluster. Review the {{site.terms.sep}}[requirements](../../latest/k8s/requirements.html) to ensure that you havesufficient resources. With insufficient resources available in the cluster, thedeployed pods may fail to be scheduled.You can also choose to deploy a new cluster with minimal resources and defaultconfiguration from the marketplace offering configuration page. After thedefault {{site.terms.sep}} chart deploys successfully in this cluster, you mustreview the cluster configuration before updating it and making any adjustmentsto ensure that the cluster is sufficiently large.### NetworkingWhen using GKE with a GCP External Load Balancer, `expose.ingress.path:` youmust have a wildcarded path ending with &#39;/*&#39; to ensure that routing is notlimited to a single URL. The &#39;*&#39; character is the only supported wildcardcharacter. It must follow a forward slash &#39;/&#39; and must be the last character inthe pattern. See the [Google URL Mapsdocumentation](https://cloud.google.com/load-balancing/docs/url-map) for moreinformation about path limitations and pattern matching in GKE.### DeployingThe default values of the {{site.terms.sep}} configuration [in theUI](https://console.cloud.google.com/marketplace/details/starburst-public/starburst-presto)work for most of the cases.{% include warning.html content=&quot;If you are enabling global access control using Ranger, make sure you note downthe values of the admin and service user passwords you provide in the UI forlater use.&quot; %}Once done, you can click on **Deploy** to install {{site.terms.sep}} in thecluster. You can track the progress of the deployment in the [applicationspage](https://console.cloud.google.com/kubernetes/application).Once the application is successfully deployed, {{site.terms.sep}} is deployedusing the Helm charts. If you enable either or both of[Ranger](../../latest/k8s/ranger-configuration) and [Hive MetastoreService](../../latest/k8s/hms-configuration) these are also deployed. Thedeployment uses the values you provide in the UI and some default values whendeployed the first time. If you want to upgrade or change the deploymentconfiguration, you can either use the **Application info** from the [applicationpage](https://console.cloud.google.com/kubernetes/application) or the following{{site.terms.sep}} upgrade instructions.## Upgrading {{site.terms.sep}}You can upgrade {{site.terms.sep}} using the [cloudshell](https://cloud.google.com/kubernetes-engine/docs/quickstart#choosing_a_shell).You need to download the Helm charts and extract them to modify thedeployment, as shown in the following:```shellexport TAG=2.0.1export APP_INSTANCE_NAME=wget https://console.cloud.google.com/storage/browser/starburst-enterprise-presto/helmCharts/presto-gcp/starburst-enterprise-presto-charts-$TAG.tgz;tar -zxvf starburst-enterprise-presto-charts-$TAG.tgz;```Now update the ``values.yaml`` in the umbrella chart folder(``starburst-enterprise-presto-charts-$TAG``) before you apply the manifestfile. Alternatively, you can use ``--set`` parameter in ``helm template``command to update the chart configuration. The values set in ``--set`` takesprecedence over the ones set in ``values.yaml``.Once you have the manifest created, apply (``kubectl apply``) the file to updatethe deployment.More details about the {{site.terms.sep}} configuration are found in the [reference documentation](../../latest/k8s/sep-configuration).Make sure you have [usagemetrics](../../latest/k8s/sep-configuration.html?highlight=usage%20metrics#usage-metrics)enabled in your {{site.terms.sep}} configuration. Our Google Cloud offeringrelies on the metrics and if disabled, the deployment is terminated after 36hours.The minimum set of parameters required for the upgrade are shown below:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set [PUT OTHER CHART PARAMETERS YOU WANT TO UPDATE] &gt; presto_manifest.yaml```The following example shows how to update the number of {{site.terms.sep}} workers:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set worker.count &gt; presto_manifest.yaml```Configure ``kubectl`` to point at your cluster and then apply the configuration:```shellkubectl apply -f presto_manifest.yaml```## Enabling global access control with Apache RangerYou can deploy or upgrade Ranger for [global accesscontrol](../../latest/security/global-ranger) by creating a manifest file withRanger parameters set.You can either update the ``values.yaml`` in the umbrella chart folder``starburst-enterprise-presto-charts-$TAG`` for the configuration(``starburst-ranger``) prior to generating the manifest file or or use ``--set``parameter while creating the manifest file. The ``--set`` parameters must beprefixed by ``starburst-ranger``.The complete reference of the chart configuration can be found in the[Ranger chart documentation](../../latest/k8s/ranger-configuration.html).Example with Ranger enabled:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set starburst-ranger.enabled=true   --set [PUT OTHER CHART PARAMETERS YOU WANT TO UPDATE] &gt; presto_manifest.yaml```## Enabling the Hive Metastore ServiceSimilar to Ranger, to deploy or upgrade the Hive Metastore Service (HMS)provided by {{site.terms.sb}}, generate a manifest file with Hive chartparameters set. You can either use ``values.yaml`` (for ``starburst-hive``) oruse the ``--set`` parameters. For Hive charts, the ``--set`` parameters must beprefixed by ``starburst-hive``.The complete reference of the chart configuration for the metastore can be foundin the [HMS documentation](../../latest/k8s/hms-configuration.html).Example with HMS enabled:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set starburst-hive.image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/hive&quot;   --set starburst-hive.image.tag=&quot;$TAG&quot;   --set starburst-hive.enabled=true &gt; presto_manifest.yaml```Configure ``kubectl`` to point at your cluster and then apply the configuration:```shellkubectl apply -f presto_manifest.yaml```## Deleting a deploymentTo delete a GKE deployment, run the following command:```shellkubectl delete -f presto_manifest.yaml```"
 },
 {
  "title": "Glossary",
  "url": "/glossary.html",
  "content": "# {{ page.title }}##### Terms A-E###### Amazon AWS marketplaceA provider for all aspects of the required infrastructure. This includes usingAWS CloudFormation for provisioning, Amazon Simple Storage Service (S3) forstorage, Amazon Machine Images (AMI), and Amazon Elastic Compute Cloud (EC2) forcomputes, Amazon Glue as metadata catalog, and others. For more information, see[Amazon AWS Marketplace](./ecosystems/amazon/index.html).###### Bare-metal serverA physical computer server dedicated to a single tenant. See[bare-metal server](https://en.wikipedia.org/wiki/Bare-metal_server)###### CatalogCatalogs define and name the configuration to connect to, and query a datasource. For more information, see[Catalogs](./data-engineer/catalogs.html).###### ClusterA cluster provides the resources to run queries against numerous data sources.Clusters define the number of workers, the configuration for the JVM runtime,configured data sources, and others aspects. For more information, see [clusterbasics](./platform-administrator/cluster.html).###### ConnectorTransforms the underlying data into the {{site.terms.sep}} concepts of schemas,tables, columns, rows, and data types. A[connector](./data-engineer/connectors.html) is specific to the data source itsupports, and are named as properties in [catalogs](#catalog).###### ContainerA lightweight virtual package of software that contains libraries, binaries,code, configuration files, and other dependencies needed to deploy anapplication. A running container does not include an operating system. It usesthe operating system of the host machine, typically Linux. Learn more at[Container concept](https://kubernetes.io/docs/concepts/containers/) in theKubernetes documentation.###### COTSCommon off-the-shelf. Refers to commodity hardware components.###### Data consumer personaOwns data products such as reports, dashboards, models, and the quality ofanalysis. For more information, see [Starburstpersonas](./get-started/starburst-personas.html#data-consumer).###### Data engineer personaOwns schemas and is responsible for the source data quality and ETL SLA. Formore information, see [Starburst personas](./get-started/starburst-personas.html#data-engineer)###### Data sourceA data source is a system from which data is retrieved. In {{site.terms.sep}},you must connect to a data source so you can query that source by using a[catalog](#catalog). See [Configure and definecatalogs](./data-engineer/catalogs.html)###### External ID (AWS)An external ID is an identifier in AWS that is required for using{{site.terms.sg}}. It is used to ensure that only trusted AWS accountsare given permission to operate the {{site.terms.sg}} clusters based ontheir assigned role and trust policy. For more information on AWS Identity andAccess Management, see [How to use an external ID when granting access to yourAWS resources to a thirdparty](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html).##### Terms F-J###### Google Cloud marketplaceDeploy in the Google Cloud Marketplace or using the Starburst Kubernetessolution on the Google Kubernetes Engine (GKE). GKE is a secure, productionready, managed Kubernetes service in Google Cloud managing for containerizedapplications. For more information, see [Google Cloudmarketplace](./ecosystems/google/index.html).##### Terms K-O###### MarketplacePurchase a preconfigured set of machine images, containers, and other neededresources to run {{site.terms.sep}} on their cloud hosts under your control. See[Marketplace deployments](./ecosystems/index.html).###### Microsoft Azure marketplaceDeploy using in the Azure Marketplace or using the Starburst Kubernetes solutiononto the Azure Kubernetes Services (AKS). AKS is a secure, production-ready,managed Kubernetes service on Azure for managing for containerized applications.For more information, see [Microsoft AzureMarketplace](./ecosystems/microsoft/index.html).##### Terms P-T###### Platform administrator personaOwns platforms and services (ITIL-style). Has service SLA responsibility for theinfrastructure supporting the cluster. For more information, see [Starburstpersonas](./get-started/starburst-personas.html#platform-administrator).###### Presto and {{site.terms.fka}}Old name for Trino.###### Red Hat OpenShift marketplaceA container platform using Kubernetes operators that automates the provisioning,management and scaling of applications to any cloud platform or even on-prem.Starburst Enterprise is available on Red Hat marketplace as of OpenShift version 4.For more information, see [Red HatMarketplace](./ecosystems/redhat/index.html).###### Starburst Enterprise platformHelps companies harness the value of open source {{site.terms.oss}}, the fastestdistributed query engine available today. Starburst adds connectors, security,and support that meets the needs for fast data access at scale. For moreinformation, see [Starburst Enterprise](./starburst-enterprise/index.html).###### SEPAbbreviation of Starburst Enterprise platform. For more information, see[Starburst Enterprise](./starburst-enterprise/index.html).###### SQLStructured Query Language. The standard language used with relational databases.For more information, see [SQL](./data-consumer/starburst-sql.html).###### {{site.terms.oss}}Fast distributed SQL query engine for big data analytics, formerly{{site.terms.fka}}.###### Virtual machine (VM)An emulation of the hardware of a computer system on a physical host machine, soany operating system suitable for that hardware can run in the emulator. Atypical example is a Linux virtual machine running on a Windows-based hostmachine. See [virtual machine](https://en.wikipedia.org/wiki/Virtual_machine)."
 },
 {
  "title": "AWS Glue",
  "url": "/ecosystems/amazon/data-sources/glue.html",
  "content": "# {{page.title}}[AWS Glue](https://aws.amazon.com/glue/) data catalogs are a supported metadatacatalog for {{site.terms.sep_first}}, and can be used as an alternative to theHive Metastore to query your [S3 data](./s3.html) with thefollowing connectors:* [Hive connector](../../../latest/connector/starburst-hive.html)* [Delta Lake](../../../latest/connector/starburst-delta-lake.html)* [Iceberg](../../../latest/connector/starburst-iceberg.html)Ensure the requirements for the connector are fulfilled.## RequirementsBefore you configure the Glue metastore, verify the following prerequisites:* Your {{site.terms.sep}} instance must have permissions to access both S3 and  Glue AWS services.* For CFT deployments, review the  [IAM role permissions requirements](../../../latest/aws/requirements.html#iam-requirements)  if you are providing your own IAM Instance.* When using the AMI and launching it manually, make sure you choose an IAM Role  that satisfies the requirements.## Configuration1. Configure to use Glue as metastore in the catalog properties file  ```text  connector.name=hive  hive.metastore=glue  ```2. Add [other desired Glue properties](../../../latest/connector/hive.html#aws-glue-catalog-configuration-properties)   such as the AWS region or credentials to use.3. Restart the cluster to apply the changes.## AWS Glue with {{site.terms.sep}} AMIYou can use the {{site.terms.sep}} AMI from the AWS Marketplace, with the Hiveconnector to use Glue.After the configuration as described in the preceding section, you can restartthe AMI:```textsudo service starburst restart```## AWS Glue with CloudFormation templateWhen using the CloudFormation template in AWS, you can leverage Glue bynavigating to the **Stack Creation** form and choosing **AWS Glue Data Catalog**in the **MetastoreType** field in the **Starburst Enterprise Configuration**section.## {{site.terms.sep}} with AWS Glue usageWhen configured, the Glue data catalog is available via the catalog from withinthe CLI or any other {{site.terms.sep}} connection. You must specify thelocation of the data on S3 for either the entire schema or at the table level.For example, to create a schema ``myschema`` in the Glue data catalog, with theS3 base directory (root folder for per-table subdirectories) pointing to theroot of ``my-bucket`` S3 bucket, run the following SQL command:```sql    CREATE SCHEMA mycatalog.myschema    WITH (location = &#39;s3://my-bucket/&#39;)```You can also create and edit the schema and tables directly from Glue. InGlue terminology, a schema is referred to as a “database”.## Table and column statistics support{{site.terms.sep}} supports standard AWS Glue table and column statistics viathe [AWS GlueAPI](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-tables.html#aws-glue-api-catalog-tables-GetColumnStatisticsForTable).You can create and manage the statistics with the[``ANALYZE``](../../../latest/sql/analyze.html) statement.### Legacy statistics{{site.terms.sep}} releases prior to 354-e use custom statistics. These{{site.terms.sep}}-based statistics are no longer supported in 354-e and laterreleases, and are replaced by the standard Glue statistics.{% include note.html content=&quot;The``hive.metastore.glue.column-statistics.enabled`` configuration property isdeprecated.&quot; %}You can enable read-only access to the {{site.terms.sep}}-based table statisticsfor transition purposes. Set the property``hive.metastore.glue.read-properties-based-column-statistics`` to ``true``during the migration time, until the standard statistics are available. Thelegacy statistics are only used if standard statistics are not present for atable or a partition.{{site.terms.sep}}-based statistics are still stored in JSON format as Gluetable and partition parameters. After the migration these can be deleted.## Known limitations of AWS Glue supportThe following {{site.terms.sep}} features are not supported with the Glue datacatalog:* Statistics are not preserved when a column is renamed. Tables with renamed  columns must be re-analyzed.* Renaming tables from within AWS Glue is not supported.* Partition values containing quotes and apostrophes are not supported (for  example, ``PARTITION (owner=&quot;Doe&#39;s&quot;``).* Using [Hive authorization](../../../latest/connector/hive-security.html) is not  supported."
 },
 {
  "title": "Image usage",
  "url": "/internal/images.html",
  "content": "## ImagesDisplay images with:* Our own [include image syntax](#include-image-syntax)* [HTML syntax](#html-syntax)* [Markdown syntax](#markdown-syntax)### include image syntaxThis is our in-house and preferred image syntax.This syntax provides a `screenshot` attribute that places a single pixel borderaround the image, which is best used for screenshots that have white space up totheir edges, so that the image is clearly delineated from the page&#39;s text.This syntax also allows you to specify valid CSS to tailor a particular image.If no `pxwidth` value is passed, the image defaults to the file&#39;s inherent widthup to a maximum of 90% of the current container for very large images.Another option of the `include image` syntax is to turn very large images intotwo-size modal images. The specified image is reduced for normal display on thepage, limited to `pxwidth` size, but the image is also now a button. Whenclicked, the image expands and is shown in a larger modal window. The`screenshot` and `modal` options are mutually exclusive; use one or the other.Use `include image.html` in a separate tag block with the parameters shown inthe following table:* Parameters may be used in any order.| Key         | Value | Required | Notes || ----------- | ----------- | ----------- | ----------- || url         | Relative path to the asset.  | **Required** || alt‑text    | String for alt tag content. | **Required** || descr       | String for description tag content for accessibility support. | *Optional* | Use if `alt‑text` does not convey enough information || pxwidth     | Width value in pixels, integer only. See the notes below | *Optional* || screenshot  | Set to &#39;true&#39; adds 1px solid border. No effect if modal. | *Optional* || modal       | Set to &#39;true&#39; adds modal ability. | *Optional* | Must assign `img‑id` if true || img‑id      | Adds ID string. | *Optional* | **Required for modal.** Must be unique per page. || style       | Pass any valid CSS to the image. | *Optional* |For the `pxwidth` key:* Don&#39;t include a `pxwidth` line, or leave its value empty to specify using the  native image resolution by default. ``pxwidth=&#39;&#39;``* Do not specify ``pxwidth=&#39;0&#39;``, which disables the image.* For modal, `pxwidth` specifies the non-modal display size on the page before  expansion.#### Using within listsWhen using the `include image` within a list, you **must** prefix the `includeimage` tag block with a `&amp;nbsp;` and the proper indentation in order to ensurecorrect alignment. If no `&amp;nbsp;` is used, the list items after the `includeimage` will start over at `1`.```{%raw%}1. The first item in the list  &amp;nbsp; {% include image.html ... %}2. The second item in the list  1. The first item in a nested list    &amp;nbsp; {% include image.html ... %}  2. The second item in a nested list3. The third item in a list{%endraw%}```#### Copy these imagesFeel free to copy the include tag blocks that display the following images.Paste these as templates into your working documents, then edit as appropriate:```{%raw%}{% include image.html  url=&#39;../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}{%endraw%}```{% include image.html  url=&#39;../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}```{%raw%}{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;Starburst cluster overview&#39;  descr-text=&#39;Image depicting Starburst architecture&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}{%endraw%}```{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;Starburst cluster overview&#39;  descr-text=&#39;Image depicting Starburst architecture&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}### HTML syntaxThis syntax allows you to add further tweaks such as sizing. If you copy thisexample, remember to surround `site.baseurl` with double open and close brace{ } characters.``````### Markdown syntaxOnly for very simple image cases, use Markdown syntax, which is:`![title](../assets/img/image.png)`. Use a relative path to the image from thecurrent location, or use the `site.baseurl` substitution token. (If you copythe following example, remember to surround `site.baseurl` with double open and closebrace { } characters.)```![Starburst](site.baseurl/assets/img/logo/starburst.png)```![Starburst]({{ site.baseurl}}/assets/img/logo/starburst.png)## LogosThe following conditions should be met when optimizing and saving brand logos touse on the site:| Property      | Value || ----------- | ----------- || Bounding | Trim/Cropped || Width      | 400px || Height | Auto (Constrained proportions) || Background   | Transparent        || Format | PNG || Example |  ![dbeaver](../assets/img/logo/dbeaver.png) |When adding a brand logo to the beginning of a page, for example the[Red Hat](../marketplace/redhat/index.html) marketplace page, the initialcontent section should contain the logo using the following HTML structure:```{% raw %}                {% endraw %}```Similarly, the `image.html` `include` may also be used by changing the HTMLsnippet above to:```{% raw %}              {% include image.html      url=&#39;../assets/img/logo/dbeaver.png&#39;      alt-text=&#39;DBeaver Logo&#39;    %}  {% endraw %}```This HTML structure ensures that the logo will be placed to the right of thecontent, creating a two column layout that allows the text to not produce astaggered wrapping effect. Once a certain breakpoint has been reached the logowill then move to above the text content.All content after the above HTML snippet will take up the full width of thecontainer.## IconsYou can and should use fontawsome icons. The site uses fontawesome 5.There are [thousands available](https://fontawesome.com/icons).Syntax is ``.You can change size, color and more.This is a user icon  in a paragraph.Also be careful, we use fontawesome 5 and therefore some icons are in adifferent name space.* `` for * `` for * `` for * `` for Coloring can be done with CSS or an embedded style.* `` for Other examples, in case you need to use them:* `` for * `` for ## Persona based icons          Platform administrator            Data consumer            Data engineer  ## Topic based icons          Admin            Catalog            Clients            Clusters            Connectors            Data sources            Deploying            Migration            Performance tuning            Query performance  "
 },
 {
  "title": "Azure data sources overview",
  "url": "/ecosystems/microsoft/data-sources/index.html",
  "content": "# {{page.title}}Microsoft offers numerous services for data storage with Azure. You can use{{site.terms.sep_first}} and {{site.terms.sg}} to access many of these systems:* [Azure Storage](./azure-storage.html)* [Azure Database](./azure-database.html)"
 },
 {
  "title": "Video library",
  "url": "/videos/index.html",
  "content": "                    {{ page.title }}                                      Explore the growing list of videos to learn about {{site.terms.sb}}        products, installation, tuning, management, SQL, and more.        Use timestamps to drill down and learn about a specific topic.      {% include list-videos.html dir=&quot;/videos/&quot; %}"
 },
 {
  "title": "Starburst Galaxy",
  "url": "/starburst-galaxy/index.html",
  "content": "                              Fully managed query engine for distributed data          {{site.terms.sg}} is the cloud native and fully managed service of          Starburst’s massively parallel processing (MPP), highly performant          query engine. It allows you to query a variety of data sources, or          join data across multiple data sources through a single query.                                Request access                                Watch a demo                              Connect numerous data sources, such as distributed object storage and relationaldatabase management systems, and query them all at the same time.Use SQL or the business intelligence or reporting tool of your choice to analyzeyour data and gain insights quickly.Get started with the following steps:* [Sign up](./get-started.html) or request access from your {{site.terms.sg}}  administrator.* [Configure catalogs](./catalogs/index.html) for your data sources.* Use the catalog in a [cluster](./clusters/index.html).* Query with SQL in [Query editor](./query-editor/index.html) or another [client  application](./clients.html).* Learn how roles and privileges work to protect your data  in the [security overview](./security/index.html)."
 },
 {
  "title": "Client overview",
  "url": "/data-consumer/clients/index.html",
  "content": "# {{ page.title }}{{site.terms.sb}} lets you query all the connected data sources with the mostwidely used and best supported query language — standard SQL. You can even querymultiple data sources with the same query. Simple queries, and these morecomplex federated queries all access the data right in the source. There is noneed for ETL processes.As a data consumer, you can use any supported client tool to connect to your{{site.terms.sb}} cluster and execute your queries. Many tools allow you muchmore powerful usage than simply running custom written queries. You can createcomplex reports, charts, dashboards, and many other useful results.## Basic connection informationThe details you need to connect to {{site.terms.sb}} are independent of yourpreferred client tool. For a minimum connection without TLS, you need:* URL of the {{site.terms.sb}} cluster, including the port used.* Credentials, typically username and password.Ask your {{site.terms.sb}} platform administrator for this information.Let&#39;s look at some examples:A basic test installation on your local computer, using the default port and noTLS configuration:* http://localhost:8080* username can be a random string like your first name since no authorization is  configured* no passwordThe same basic test application running on a different server:* http://starburst.example.com:8080* random username string* no password### Default portsThe following table shows the default ports for a {{site.terms.sep}} cluster:| Port | Connection type ||------|-----------------|| 8080 | Direct to coordinator, no security enabled || 8443 | Direct to TLS-enabled coordinator ||   80 | Connect through load balancer or proxy server ||  443 | TLS-enabled load balancer or proxy server |## TLS connection informationIf you enable TLS for your coordinator, you typically use a load balancer orproxy. In this case, the default port is generally used, and the protocolchanges to ``https``:* https://starburst.example.com* random username string* passwordTLS is a requirement for authorization against a provider&#39;s data, such as yoursite&#39;s LDAP directory. In this case, you must use real credentials:* https://starburst.example.com* LDAP username* LDAP passwordIf your client tool uses JDBC to connect, you must enable TLS support with the``SSL=true`` parameter in the JDBC configuration for your client, as describedin [Enable JDBC TLS support](./jdbc.html#enable-jdbc-tls-support).Other authorization providers may require additional credentials. Support forother providers varies among client tools.## Determine cluster versionThe version of {{site.terms.sep}} running on the cluster determinescompatibility of suitable clients. Specifically important is whether the clusteris using {{site.terms.sep}} 354-e or newer, or an older release prior to therename of the open source project. Newer versions use the name Trino, whileolder releases up to 350-e use the Presto or PrestoSQL name.You can determine the version of the cluster by asking your platformadministrator, or with one of the following methods, depending on your access:* Connect a client or browser to the ``v1/info`` REST endpoint. For example,  connect to ``http://starburst.example.com:8080/v1/info``.* Connect a modern web browser to the cluster and log in to view the  {{site.terms.sep}} [Web UI](../../starburst-enterprise/try/verify.html). For  example, connect to ``http://starburst.example.com:8080/ui``. The version  number is shown on the right side of the top row.* Connect the {{site.terms.sep}} [CLI](./cli.html) and run the following query:  ``select * from system.runtime.nodes;``## General adviceGeneral advice for using clients is the following:* For clusters running {{site.terms.sep}} version 350 or earlier, use version  350 of the client.* For clusters running {{site.terms.sep}} version 350, you can use client  version 350, 354 or newer to help with migration.* For clusters running {{site.terms.sep}} versions 354 and later, use the same  version of the client as the cluster or a newer client.This applies for the CLI and JDBC driver. Some clients, such as ODBC driver,have separate versioning, and details are documented with the client. Otherclients, including open source tools like [DBeaver](./dbeaver.html), use theTrino name for versions 354 or newer and PrestoSQL or Presto for older versions.Examples:{{site.terms.sep_full}} version 360-e LTS recommended clients:* CLI 360* JDBC driver 360* DBeaver with **Trino** connection{{site.terms.sep_full}} version 345-e LTS recommended clients:* CLI 350* JDBC driver 350* DBeaver with **PrestoSQL** connection## {{site.terms.sb}} client tools and drivers{{site.terms.sb}} offers a number of supported clients and tools* [Command line interface (CLI)](./cli.html) for shell scripts and manual  execution using a terminal* [Java Database Connectivity (JDBC) driver](./jdbc.html), typically for  JVM-based applications and others with JDBC support* [Open Database Connectivity (ODBC) driver](./odbc.html), typically for for  Windows-based applications and others with OBDC supportThe JDBC and ODBC driver can be used for many other clients, and you can findinstructions and specifics tips for the following tools:* [Tableau](./tableau.html)* [Microsoft Power BI](./powerbi.html)* [DBeaver](./dbeaver.html)* [SQuirrel SQL](./squirrel-sql.html)* [Looker](./looker.html){% include note.html  content=&quot;If you encounter an error message on connection stating &quot;Session  properties cannot be overridden once a transaction is active&quot;, enable **Auto  commit** or the equivalent setting in your client. Refer to your client&#39;s  documentation for information on where to configure this setting.&quot;%}## Other client tools and driversIn addition, the {{site.terms.oss}} community provides the following clients:* [trino-python-client](https://github.com/trinodb/trino-python-client)* [trino-go-client](https://github.com/trinodb/trino-go-client)The wider open source community maintains [numerous other clients andtools](https://trino.io/resources.html) that can be used.## Other resourcesThe O&#39;Reilly book, [Trino, the DefinitiveGuide](https://www.starburst.io/info/oreilly-trino-guide/), was writtenhere at {{site.terms.sb}}, and contains some great information on gettingstarted with using different types of clients with {{site.terms.sb}}. It&#39;savailable for free!"
 },
 {
  "title": "Starburst Admin - get started",
  "url": "/starburst-enterprise/starburst-admin/index.html",
  "content": "# {{ page.title }}{{site.terms.sbadmin}} is a[collection](https://docs.ansible.com/ansible/latest/user_guide/collections_using.html#collections)of [Ansible](https://www.ansible.com/) playbooks for installing and managing{{site.terms.sep_first}} or {{site.terms.oss}} clusters.It includes the following features:* Installation and upgrade of {{site.terms.sep_first}} or {{site.terms.oss}}  using the RPM or `tar.gz` archives* Update the coordinator and worker nodes configuration files,  including catalog properties files for data source configuration* Service management of the cluster and on all nodes (start/stop/restart/status)* Collection of logs* Support for adding custom binary files, such as custom connectors or UDF* Support for adding custom configuration filesAll target machines must meet the [requirements](#requirements)outlined below prior to installing {{site.terms.sbadmin}}.{{site.terms.sbadmin}} does not manage the creation of the servers, theoperating system installation and configuration, or the Python and Javainstallation. It is also not designed to manage other related tools such asApache Ranger, a Hive Metastore Service, or any data source.It is most suitable for managing clusters installed on[bare-metal](../../glossary.html#bare-metal) servers or [virtualmachines](../../glossary.html#virtual-machine). Use the [Kubernetes with Helmsupport](../../latest/k8s.html) instead of {{site.terms.sbadmin}} if you usecontainers and Kubernetes.{% include note.html content=&quot;The legacy Presto Admin is deprecated, and nolonger supported for Starburst Enterprise version 354-e and higher.&quot; %}## RequirementsDeep knowledge of Ansible is not expected for usage, but familiarity withAnsible and Ansible playbooks is required.The following sections detail the requirements for the machine where you runAnsible and {{site.terms.sbadmin}}, called the *control node*, and the requirements forthe machines where you install and manage {{site.terms.sep}}, called the*cluster nodes*.### Requirements for the control nodeThe [controlnode](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html)is used to run {{site.terms.sbadmin}}, and therefore Ansible playbooks.[Standard Ansiblerequirements](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#prerequisites)apply:* Ansible 2.10 or higher* Linux/Unix operating system* Python 2.7 and higher or Python 3.5 and higherIn addition, the following resources are needed:* [SSH connectivity to the cluster nodes](https://docs.ansible.com/ansible/latest/user_guide/connection_details.html)* Downloaded {{site.terms.sep_first}} or {{site.terms.oss}} `tar.gz` or  [RPM](../../latest/installation/rpm.html) archive files on the control node,  or alternatively URL to the files that is accessible on all cluster nodesThe controller node can be any machine that is configured to fulfill theserequirements. For initial testing you can use your workstation or even a node inthe cluster directly. Production usage should follow Ansible best practices, anduse dedicated workflow or Ansible orchestration and automation tools such as[Ansible Tower](https://www.ansible.com/products/tower) or[Concord](https://concord.walmartlabs.com/).### Requirements for managed cluster nodes{{site.terms.sbadmin}} does not manage the cluster hardware, operating systemor package installation. It relies on the existence of all the nodes inthe cluster and the fact they fulfill the requirements detailed in this section.Typically provisioning systems such as Puppet, Chef, Terraform and others areused to prepare the cluster nodes.All cluster nodes need to fulfill the normal {{site.terms.sep_first}} or{{site.terms.oss}} requirements:* Linux operating system* Java runtime environment* PythonMemory and hardware resource requirements depend on the planned capacity of thecluster. Following are a few high level guidelines:* Use identical hardware configurations for all workers.* Start with at least two workers, scale up as needed.* Prefer fewer, more powerful worker nodes over many smaller ones.* For performance reasons, nodes are ideally located on the same subnet and  within the same data center. All nodes communicate using TCP/IP.Review the [requirements for the specific version ofSEP](../../latest/installation/deployment.html#requirements) for additionaldetails.Specific testing is performed with the distributions CentOS versions 7 andCentOS 8 and Red Hat Enterprise Linux (RHEL) versions 7 and 8.Additional requirements:* [Enabled SSH access and connectivity from the control  node](https://docs.ansible.com/ansible/latest/user_guide/connection_details.html),  the configured user must have root or sudo access. If the sudo user requires a  password, use `ask-become-pass` when running playbooks.* `rsync`, often an optional package that needs to be installed.* `bash`, typically installed by default.When using {{site.terms.sbadmin}} with an RPM archive:* RPM-based Linux distribution* `rpm` command, `yum`, `dnf`, or others are not requiredWhen {{site.terms.sbadmin}} with an `tar.gz` archive:* GNU `tar` command* `unzip` command## Install {{site.terms.sbadmin}} on the control node{{site.terms.sbadmin}} is a collection of Ansible playbooks that you install onthe control node:* Contact {{site.terms.support}} for the {{site.terms.sbadmin}} `tar.gz` binary  package.* Download it onto the control node into any directory, such as `~/tmp`.* Access the directory in a command line interface.* Install the collection with the following command:  ```shell  ansible-galaxy collection install starburst-admin-*.tar.gz  ```* Confirm the command finishes successfully:  ```shell  Starting galaxy collection install process  Process install dependency map  Starting collection install process  Installing &#39;starburst.admin:1.0.0&#39; to &#39;....&#39;  starburst.admin:1.0.0 was installed successfully  ```The collection is installed into `/home//.ansible/collections` bydefault. The installation path `/home//.ansible/collections/ansible_collections/starburst/admin/files`is used for the binaries and all the configuration files for a cluster. Makesure you manage the files in this directory with a version control system.You can override the installation path with the option `-p `.### Install on multiple control nodesIf you need to install the collection into numerous control nodes, you can makethe binary available on a remote URL:* Make the binary available on a server via HTTP, for example,  `https://repo.example.com/files/starburst-admin-1.0.0.tar.gz`.* Create a file `requirements.yml` that includes a link to the binary.  ```yaml  ---  collections:      # Example link to tar.gz package      - https://repo.example.com/files/starburst-admin-1.0.0.tar.gz  ```* Use the YAML file for the installation  ```sh  ansible-galaxy collection install -r requirements.yml  ```## Next stepsNow that you have set up the control nodes and the managed cluster nodes, youcan proceed with the [initial installation on thecluster](./installation-guide.html)."
 },
 {
  "title": "User guide",
  "url": "/starburst-enterprise/index.html",
  "content": "                    {{site.terms.sep_full}}                              Try {{site.terms.sep_full}}                      Check the reference documentation                                        Welcome to the {{site.terms.sep_first}}.                                                                              Try {{site.terms.sep_full}}                                                                                                    Reference docs                                                                                                    {{site.terms.sb}} Admin                                                Still not sure where to start? Head over to our user personas page to learn which fits you best.              Browse topics      We&#39;ve also gathered some resources on popular topics for you.                                                Glossary                                                                    Videos                                                                    Clusters                                                                    SQL                              "
 },
 {
  "title": "Amazon data sources overview",
  "url": "/ecosystems/amazon/data-sources/index.html",
  "content": "# {{page.title}}Amazon offers numerous services for data storage with AWS. You can use{{site.terms.sep_first}} and {{site.terms.sg}} to access many of these systems:* [Amazon S3](./S3.html)* [AWS Glue](./glue.html)* [Amazon RDS](./rds.html)"
 },
 {
  "title": "Catalogs overview",
  "url": "/starburst-galaxy/catalogs/index.html",
  "content": "# {{page.title}}Each catalog contains configuration for {{site.terms.sg}} to access a datasource. Configure catalogs and use them in clusters to query data sources in{{site.terms.sg}}.Data sources and clusters need to be located in the same cloud provider andregion to enable optimal performance and avoid unnecessary data transfer costs.Once the catalog is defined and used in a cluster, you can query the datasource by accessing the catalog and the nested schemas and tables.## Data sources{{site.terms.sg}} facilitates access to numerous different data sources.Configuration for these objects storage systems, relation databases, and othersystems varies by cloud and hosting provider. The following sections provideinformation on how to configure these data sources so they can be used incatalogs.* [Amazon S3](./s3.html), object storage on Amazon Web Services, combined with  Amazon Glue, your own Hive Metastore Service, or {{site.terms.sg}} metastore* [Azure Data Lake Storage](./adls.html), object storage on Microsoft Azure,  combined with your own Hive Metastore Service, or {{site.terms.sg}} metastore* [Google Cloud Storage](./gcs.html), object storage on Google Cloud,  combined with your own Hive Metastore Service, or {{site.terms.sg}} metastore* [MySQL](./mysql.html), relational database in numerous variants on Amazon Web  Services, Google Cloud, or Microsoft Azure* [PostgreSQL](./postgresql.html), relational database in numerous variants on  Amazon Web Services, Google Cloud, or Microsoft Azure{% include video.html content=&quot;See it all in action with the Starburst Galaxyintroduction and the demo video for S3 catalogs.&quot; %}## Datasets{{site.terms.sg}} also provides access to a number of full datasets. You cancreate a catalog using the datasets, and use them for a number of purposes:* Easy demonstration of {{site.terms.sg}} features without the need to configure  an external data source.* Availability of a full dataset to query, learn SQL, and experiment with  [different clients](../clients.html).* Performance and other benchmark tests with well known data and standardized  queries.The following dataset catalogs are available:* [TPC-H](./tpch.html)* [TPC-DS](./tpcds.html)* [Sample dataset](./sample.html)"
 },
 {
  "title": "Security overview",
  "url": "/starburst-galaxy/security/index.html",
  "content": "# {{page.title}}{{site.terms.sg}} includes a built-in, role-based access control (RBAC) systemto implement your security needs. It makes it easy to configure the correctaccess rights to {{site.terms.sg}}, the clusters, and the configured catalogswith the data from the data sources for every [user](../account/users.html).Roles and privileges provide fine-grained control that protects your data,allowing you to define just the right mix of allowed actions and access for eachfunction in your organization.A [role](./roles.html) has a name and an optional description, and can beassigned [privileges](./privileges.html) on [entities](./entities.html) likecatalogs and clusters. Users are assigned one or more roles.You can manage users, roles, and privileges in the [Accountsection](../account/index.html) of {{site.terms.sg}}.Access to the {{site.terms.sg}} user interface, and directly to clusters with[clients](../clients.html), is secured with Transport Layer Security andglobally trusted certificates."
 },
 {
  "title": "Starburst for platform administrators",
  "url": "/platform-administrator/index.html",
  "content": "                                  Anytime you need help with an {{site.terms.sep_first}} administration           task or concept, you should start here. This guide takes           you to the most up-to-date information on running your {{site.terms.sep}} cluster. Learn more about platform administrators.        And if you have not seen it already, read our          guide to choosing the right          deployment.                                                                                         Introduction                Learn how {{site.terms.sep}} works, and access resources for security and deployment options.                                                                            What you need to know                                                                Cluster basics            Everything you need to know to get started                                                                                Tune your cluster            Advanced configuration for even better performance                                                                                Deploy and update            From initial install to keeping {{site.terms.sep}} up-to-date                                                                                Secure your cluster            Client access, data sources, and more                                     Looking for a different guide? Go to {{site.terms.sb}} for    data consumers    or    platform administrators.      "
 },
 {
  "title": "Clusters",
  "url": "/starburst-galaxy/clusters/index.html",
  "content": "# {{ page.title }}A cluster in {{site.terms.sg}} provides the resources to run queriesagainst numerous [catalogs](../catalogs/index.html). You can access the dataexposed by the catalogs with [Query editor](../query-editor/index.html) or [otherclients](../clients.html).Access your clusters at any time by clicking **Clusters** on the left handmenu. A `demo` cluster is included by default.{{site.terms.sg}} allows you to [create](#cluster-create),[edit](#cluster-create), [delete](#cluster-create),[start](#cluster-start), and [stop](#cluster-stop) clusters.## ConceptsCreating and managing clusters is an essential task for a platformadministrator in {{site.terms.sg}}. A cluster with the desired catalogs isrequired for a data consumer to use [SQL statements in clienttools](../clients.html) to analyze the available data. The following conceptsare important to perform this work efficiently.### Cloud provider region and catalogs[Catalogs](../catalogs/index.html) define the details to access a data source.Any data source is located in a specific cloud region of a specific cloudprovider. For example, your Cloud SQL for MySQL database is hosted in the`us-east1` region of Google Cloud.A cluster can include one or more catalogs. If multiple catalogs areconfigured, you can query them with SQL using the same client connection. Youcan also query the data in [multiple catalogs within one SQLstatement](../../data-consumer/data-mesh.html).A cluster and all its configured catalogs must be located in the same cloudprovider and region. This allows for maximum performance and avoids datatransfer costs for access across regions or even cloud providers.### SizeThe size of a cluster is determined by the number of server nodes used toprocess queries, as well as the size of the individual nodes. A largercluster, consisting of more and larger nodes, is capable of processing morecomplex queries, handle more concurrent users, and provide higher performance,by using more resources.The available sizes include  ``free``, ``x-small``, `small`, `medium`, `large`,``x-large``, and ``2x-large`. You can create a cluster with any size, and changesize based on the current needs.### Status and transitionsAny cluster can be in one of the following three states:  Stopped  A stopped cluster consists of a small configuration set only. No      significant resources are used, and no costs are incurred.  Running  A running cluster consists of a number of server nodes. It continues to      be in the running state, while users are submitting queries for      processing.  Suspended  A suspended cluster consists of a small configuration set, and a      mechanism to listen to incoming user request. It does not include any      actively running server nodes, and no costs are incurred.A newly created cluster begins in the stopped state, and can be [started in thelist of cluster](#cluster-start).A running cluster can be manually [stopped in the list ofcluster](#cluster-stop).A running cluster transitions to the suspended status when no user queries areprocessed, and the configured time for **Auto suspend** elapsed. When a usersubmits a query to a suspended cluster, the cluster is started, and thequery is processed. The user has to wait for the cluster to start, whichtypically takes between one and five minutes.## List of clustersThe list of clusters is accessible by clicking **Clusters** on the left handmenu. It displays the following information about each cluster:*  to access a drop down of more actions:  * **Query** to navigate to the [Query editor](../query-editor/index.html)    using the current cluster as context.  * **Get connection** shows details to use to connect [clients](../clients.html)  to the cluster.  * **Stop** to stop the cluster.  * **Edit cluster** to [edit the cluster](#cluster-edit).  * **Change owner** to change the owner of the cluster to [a different    role](../security/roles.html).  * **Open** to navigate to the Trino Web UI to analyze the cluster.  * **Delete cluster** to delete the cluster.* **Name** of the cluster, used to identify a cluster in the user interface  as well as in the connection string for clients.* **Status** includes the current [status](#cluster-status) as well as buttons  to change the status including **Stop** and **Start**.* **Quick actions** show buttons for varying operations, depending on the  cluster status.* **Catalogs** lists the configured catalogs used in the cluster.* **Size** displays the configured [cluster size](#cluster-size).* **Auto suspend** displays the configured time for an inactive cluster to  transition to [suspended status](#cluster-status).*  to [edit the cluster](#cluster-edit).## Create a clusterMake sure that you have configured the desired catalogs to avoid restarts beforecreating a cluster. Use the **Create cluster** button above the [list ofclusters](#cluster-list) and proceed with the following steps:1. Set the **Cluster name** to a meaningful name for users.1. Configure the **Cluster size** from the available [sizes](#cluster-size).1. Add one or more **Catalogs** to provide access to the configured data   sources to users in this cluster. The catalogs all needs to be using the   same [cloud provider and region](#cluster-region) as the cluster itself.1. Choose the **Cloud provider region** to use for deploying and running the   cluster.1. Decide on the roles to grant access to the cluster and configure access   with **Allowed roles**.1. Click **Create cluster** to save the configuration to the list of   clusters as a stopped cluster.If desired, proceed to [start the cluster](#cluster-start).## Start a clusterUse the **Start** button in the [list of clusters](#cluster-list).## Stop a clusterUse the **Stop** button in the [list of clusters](#cluster-list). You needto stop a cluster to be able to [delete it](#cluster-delete).## Edit a clusterUse the **Edit cluster** option available with the  button, or the  button in the [listof clusters](#cluster-list) to access the edit view.You can change the configuration of a cluster without affecting your users andany running queries. Many changes, such as adding catalogs, changing size, orchanging cloud region, require a new cluster.Use the **Update** button in the [list of clusters](#cluster-list) to applythe configuration changes. The new cluster is created transparently and newqueries are transferred to it as soon as it is ready. At the same time runningqueries continue to operate and complete on the existing cluster.## Delete a clusterYou need to stop the cluster in the [list of clusters](#cluster-list) tobe able to delete a cluster. Once stopped, you can delete the cluster withthe **Delete cluster** option in the  dropdown from the list. Alternatively, you can access the [edit view for thecluster](#cluster-edit) and use the **Delete cluster button** at thebottom of the view."
 },
 {
  "title": "Starburst",
  "url": "/index.html",
  "content": "                    Everything about {{site.terms.sb}} products                                      What can you learn here?      No matter which {{site.terms.sb}} product you use, and what you are trying to achieve,        you can find help on these pages:              Installing and operating a {{site.terms.sb}} cluster on your own infrastructure        Using Starburst on public cloud provider infrastructure        Connecting data sources to query your relational database or your object storage        Using a BI tool to query data with {{site.terms.sb}}        Writing SQL statements to get more insights from your data            Explore the site to find all this and more information in articles,      the reference documentation, videos and other material.                          Which crew member best describes you?                                                                              Data consumer              You use data from {{site.terms.sb}} with your BI and data science tools to create important business insights                                                                                                    Data engineer              You provide the source data to data consumers using {{site.terms.sep}}, and ensure its quality, availability, and performance                                                                                                    Platform administrator              You install and manage the clusters that serve the data, and ensure everything is performing well                                                Are you a data leader? You can learn more    about how {{site.terms.sb}} products can impact your data insights, ops processes, budget, and efficiency.        Still not sure where to start? Head over to our user personas page to learn which fits you best!              Other resources                                                Video library                                                                    Reference docs                                                                    SQL                              "
 },
 {
  "title": "Account overview",
  "url": "/starburst-galaxy/account/index.html",
  "content": "# {{ page.title }}The **Account** section provides features to manage [users](./users.html) andaccess to {{site.terms.sg}}, configured as [roles andprivileges](./roles-privileges.html).The [Usage and billing](./usage-billing.html) section allows to manage paymentsand monitor your use of {{site.terms.sg}}.You can also inspect an [audit log](./audit-log.html) view of the performedconfiguration changes."
 },
 {
  "title": "Amazon EKS overview",
  "url": "/ecosystems/amazon/eks/index.html",
  "content": "# {{page.title}}Amazon Elastic Kubernetes Service (EKS) is certified to work with{{site.terms.sep_first}}:* [Cluster creation](./eks-cluster-creation.html)* [Networking](./eks-networking.html)* [Configure IAM in EKS](./eks-iam.html)* [Install SEP in EKS](./eks-sep.html)More information is available in our [Kubernetes referencedocumentation](../../../latest/k8s.html), including an installation checklist, acustomization guide, examples, and more tips."
 },
 {
  "title": "Amazon AWS",
  "url": "/ecosystems/amazon/index.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} can be used with many components of Amazon AWS. You candeploy there, access data sources in Amazon AWS and use some of the otherfeatures of Amazon AWS.Find all the relevant information for using {{site.terms.sb}} and Amazon AWS inthe following sections and guides.## DeploymentsYou can install {{site.terms.sb}} on Amazon Elastic Kubernetes Service (EKS),Amazon Elastic Compute Cloud (EC2) or through the AWS Marketplace, once you haveestablished your enterprise account.To get started {{site.terms.sb}} offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### Amazon EKSAmazon Elastic Kubernetes Service (EKS) is certified to work with{{site.terms.sep}}.More information is available in our [dedicated EKS section](./eks/index.html).### Amazon EC2As a user of Amazon EC2 instances you can manage your {{site.terms.sep}} clusterwith the same [AMI and CloudFormation Template](../../latest/aws.html) as AWSMarketplace users.Alternatively, you can manage a RPM or tarball-based installation with[Starburst Admin](../../starburst-enterprise/starburst-admin/index.html).### AWS Marketplace{{site.terms.sep}} is [available on AWSmarketplace](./aws-marketplace) via [CloudFormationTemplate](../../latest/aws.html)        ## DocumentationYou are currently viewing the AWS section of our user guides. In the left-handnavigation are guides for AWS-specific topics. We also have user guides tailoredtoward the different types of {{site.terms.sb}} users: [data consumers, dataengineers and platform administrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### AWS marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:* Email-only support* Five [email issues to  awssupport@starburstdata.com](mailto:awssupport@starburstdata.com) per month* First response SLA of one business day* Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Hitchhiker&#39;s Guide to editing Starburst content",
  "url": "/internal/index.html",
  "content": "# {{ page.title }}* [Markdown guide](./markdown.html) with all sorts of examples and usage* Our documentation [iconography](./icons.html) guide* [Personas we write for](./personas.html)* [Metadata tags for page front matter](./tag-list.html)## BreadcrumbsThe site currently uses a simple path based breadcrumb system implemented in`breadcrumbs-simple.html`.The [breadcrumbs system](https://www.seanh.cc/2020/01/01/jekyll-breadcrumbs/)from Sean Hammond is also included as inspiration for future improvements ormigration or a merge of the two systems. It does not work for plain pages andpaths but does work nicely for posts and collections."
 },
 {
  "title": "Starburst for data leaders",
  "url": "/data-leader/index.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a fast, interactive distributed SQL query enginethat decouples compute from data storage. {{site.terms.sep}} lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single {{site.terms.sep}} query can combine data from all these datasources and more. {{site.terms.sep}} can run on-prem as well as in many cloudenvironments, and comes with [30+ supported enterpriseconnectors](https://www.starburst.io/learn-presto/connectors), providinghigh performance SQL-based access to most of the data platforms in yourorganization such as Snowflake, Postgres, and Hive.Maybe your organization relies on a single variant of SQL, or maybe they use afew. With {{site.terms.sep}}, you only need to know {{site.terms.sb}} SQL, whichis ANSI-compliant and should feel comfortable and familiar. {{site.terms.sep}}takes care of translating your queries to the correct SQL syntax for your datasource.{{site.terms.sep}} can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast, and requires a minimal disk investment. It can pullyour landing times forward, and help you meet or beat your SLAs. And, Starbursthas robust access control options for your organization, from integrating withLDAP to using your existing Ranger-managed policies.Your analysts and data scientists can connect to most of their favorite toolsusing only the {{site.terms.sep}}-specific JDBC or ODBC driver, much like theones they already use, making the transition as frictionless as possible.Want to know more? No problem! Here are some resources to answer commonquestions about {{site.terms.sep}}&#39;s capabilities and value:* [What problems does Starburst solve?](https://www.starburst.io/presto-videos/video-series-intro-to-starburst-and-presto/)* [Will this help me future-proof my data ops?](https://www.starburst.io/presto-videos/modern-data-architecture/)* [How does Starburst deliver value?](https://www.starburst.io/download-starburst-presto-technical-solution-brief/)* [Is there a comprehensive product review available?](https://www.starburst.io/starburst-overview-bloor/)"
 },
 {
  "title": "Query editor overview",
  "url": "/starburst-galaxy/query-editor/index.html",
  "content": "# {{ page.title }}Query editor offers a full environment to write and execute SQL statements. Youcan access all running clusters and the configured catalogs.Access the query editor at any time by clicking **Query editor** on the lefthand menu. Alternatively, you can use the **Query** item in the  options in the [list ofclusters](../clusters/index.html#cluster-list).## Separate query editor tabsYou can rename a tab by double-clicking on the tab name. The + button canbe used to create another tab.## Cluster explorerEach tab contains the cluster explorer panel on the left. Use it to explore theavailable clusters, catalogs and schemas.## Editor panelThe editor panel allows you to type one or more SQL statements. Use the **Run**button to execute a single statement in the editor. If the editor containsmultiple statements, highlight a specific statement to be able to execute it.The right hand corner above the editor contains a drop down selector for thecluster. You can optionally select a default catalog and schema for yourqueries in the editor. The `USE catalog.schema` statement can alternatively setthese defaults.Additional available commands include the following:* **Copy** to transfer all the text from the editor to the clipboard* **Prettify** to improve the formatting of all statements in the editor* **Command palette** to access a list of further available command## Results panelThe results panel below the editor displays the query processing progress andresults. Typically one or more rows and columns are returned by a query.The **Web UI** button can be used to access the query processing details.The **Download** button  can be used to request a CSV file of the data."
 },
 {
  "title": "Deploying Starburst in the cloud",
  "url": "/ecosystems/index.html",
  "content": "                    {{page.title}}                                       If you need information on using {{site.terms.sep_first}} with a      specific ecosystem, you&#39;ve come to the right place! If you already have      Starburst, click on your ecosystem below for the guides and how-tos you      need.            Getting ready to deploy {{site.terms.sep}}? Along with our      install-anywhere Kubernetes      deployment, {{site.terms.sep}} is available directly      in many of your favorite ecosystems as a part of their marketplace      offerings. You can purchase preconfigured machine images, containers and      other needed resources to run {{site.terms.sep}} on their cloud hosts,      under your control. A free trial configuration is available from most      providers. For all cloud provider offerings, you need the following:              Access credentials from your cloud provider        The URLs and access credentials for the data sources you want your            cloud-hosted SEP cluster to access        All appropriate ACLs must be in place            Click on your preferred ecosystem below to learn more.                                {{site.terms.sb}} is available in the following ecosystems:                                                              Amazon AWS                                                                        Google Cloud                                                                        Microsoft Azure                                                                        Red Hat OpenShift                              "
 },
 {
  "title": "Starburst tech blog",
  "url": "/blog/index.html",
  "content": "      {% for post in site.posts limit:3 %}                        {{post.title}}          {{ post.date | date_to_string }} |            {%- if post.author -%}            {% include people.html people=post.author %}            {%- endif -%}                    {{ post.excerpt }}          Read more &amp;rarr;                {%- if post.image -%}                                  {%- endif -%}          {% endfor %}    Recent          {% for post in site.posts offset:3 limit:12 %}                                                    {{post.title}}            {{ post.date | date_to_string }} |            {%- if post.author -%}            {% include people.html people=post.author %}            {%- endif -%}                                    {{ post.excerpt }}            Read more &amp;rarr;                        {% endfor %}                  Want more? Visit the archives,      or subscribe via RSS.      "
 },
 {
  "title": "Azure Kubernetes Service overview",
  "url": "/ecosystems/microsoft/aks/index.html",
  "content": "# {{page.title}}[Azure Kubernetes Service (AKS)](https://azure.microsoft.com/services/kubernetes-service/)is certified to work with {{site.terms.sep_first}}:* [Create an AKS cluster](./aks-cluster-creation.html)More information is available in our [Kubernetes referencedocumentation](../../../latest/k8s.html), including an installation checklist, acustomization guide, examples, and best practices."
 },
 {
  "title": "Try Starburst Enterprise",
  "url": "/starburst-enterprise/try/index.html",
  "content": "# {{ page.title }}{{site.terms.sb}} makes it easy to set up trial configurations of{{site.terms.sep_first}}.For evaluation purposes, {{site.terms.sep}} can be installed without a license.To use more than the basic functionality you must [request a triallicense](https://www.starburst.io/contact/).Features that require a license include:* Starburst Insights Worksheets SQL editor and query history* Advanced authentication methods and other security features* Starburst Cached Views* Enhanced and exclusive [connectors](../../latest/connector.html)## Deployment optionsYour preferred platform for running applications influence how you deploy{{site.terms.sep}} to a production environment. {{site.terms.sb}} supports thefollowing deployments:* [Container](../../glossary.html#container)* [Bare metal](../../glossary.html#bare-metal)* [Virtual machine](../../glossary.html#virtual-machine)We recommend deploying {{site.terms.sep}} using containers and Kubernetes.Linux is the required operating system.## Container-based deployment on KubernetesYou can manage {{site.terms.sep_full}} and a number of other components, such asthe Hive metastore service or Apache Ranger, with the[available Kubernetes support](./k8s.html).You can use the Kubernetes support for trial, development, and productionclusters.For a very limited trial, you can use[Docker on a server or your local workstation](./docker.html).## Bare metal and virtual machine deploymentTo use bare metal hardware or virtual machines, deploy and manage{{site.terms.sep}} clusters with [StarburstAdmin](../starburst-admin/index.html). Use your own data center or cloudprovider to provide and manage these clusters.You can use {{site.terms.sbadmin}} for trial, development, andproduction clusters.For a very limited trial, you can manage the server manually or use your localworkstation. In this case, use the most recent {{site.terms.sep}}[tarball](./tarball.html) or [RPM](../../latest/installation/rpm.html) packagedirectly."
 },
 {
  "title": "Starburst for data engineers",
  "url": "/data-engineer/index.html",
  "content": "                                  Anytime you need help connecting to data sources, configuring or securing        catalogs, or understanding how the data sources are used as catalogs, start here.        Learn more about data engineers.                                                                                  Introduction              Create your data mesh              to access all your data with {{site.terms.sb}}                                                              Helpful resources                                                                              Catalogs              Define, configure, and secure data sources in Starburst                                                                                                    Connectors              Enable access to any data source in your data mesh                                                                                                    Query performance              Get the best out of your cluster resources                                                                                                    Starburst Insights              Easy access to cluster and query metrics                                                       Looking for a different guide? Go to {{site.terms.sb}} for      data consumers      or      platform administrators.            "
 },
 {
  "title": "Microsoft Azure",
  "url": "/ecosystems/microsoft/index.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} can be used with many components of Microsoft Azure.You can deploy there, access data sources in Microsoft Azure and use some of theother features of Microsoft Azure.Find all the relevant information for using {{site.terms.sb}} and MicrosoftAzure in the following sections and guides.## DeploymentsYou can install {{site.terms.sep}} on Microsoft Azure Kubernetes Service (AKS),with Azure Virtual Machines, or through the Azure Marketplace, once you haveestablished your enterprise account.To get started {{site.terms.sb}} offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### Azure AKSYou can use the Microsoft Azure Kubernetes Service (AKS) directly. AKS iscertified to work with {{site.terms.sep}}.More information is available in our [Kubernetes referencedocumentation](../../latest/k8s/overview.html), including a customizationguide, examples and tips.We also have a helpful [installationchecklist](../../latest/k8s/installation.html) for an overview of the generalHelm-based installation and upgrade process.### Azure VMs and App ServicesAs an alternative, you can use Azure App Services and linux-based Azure VMs foryour {{site.terms.sep}} cluster, managing an RPM or tarball-based installationwith [Starburst Admin](../../starburst-enterprise/starburst-admin/index.html).### Azure Marketplace{{site.terms.sep}} is [available in the Microsoft AzureMarketplace](./azure-marketplace.html).          ## DocumentationYou are currently viewing the Microsoft Azure section of our user guides. In theleft-hand navigation are guides for Azure-specific topics. We also have userguides tailored toward the different types of {{site.terms.sb}} users: [dataconsumers, data engineers, and platformadministrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### Azure Marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:- Email-only support- Five [email issues to azuresupport@starburstdata.com](mailto:azuresupport@starburstdata.com) per month- First response SLA of one business day- Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Starburst for data consumers",
  "url": "/data-consumer/index.html",
  "content": "                                  Start here anytime you need help with using {{site.terms.sb}}          products to drive your day-to-day analytics, update your SQL skills,          or improve your data science or business intelligence insights.          Learn more about data consumers.                                                                                                  Introduction                Learn about working with                {{site.terms.sb}} as data consumer.                                                                                                                        Clients                Use {{site.terms.sb}}                products with your favorite tools.                                                                                Helpful resources                                                                                SQL              SQL syntax, data types, and functions and operators                                                                                                    Your data mesh              Combine data sources in one query with {{site.terms.sb}} to access your data anywhere                                                                                                    {{site.terms.sb}} {{site.terms.insights}}              Simple Worksheet SQL query editor included in {{site.terms.sep_full}}                                                                                                    Materialized views              Get your data even faster with              auto-refreshing materialized views                                                       Looking for a different guide? Go to {{site.terms.sb}} for      data engineers      or      platform administrators.            "
 },
 {
  "title": "Slides",
  "url": "/slides/index.html",
  "content": "                    {{page.title}}        Presentations are a great resources to share information with        others concisely.              Use the following available slide decks to teach your teams:* [Introduction to Starburst](./starburst-intro.html)These presentations are also used for some of our [training and demovideos](../videos/index.html)."
 },
 {
  "title": "Google Cloud",
  "url": "/ecosystems/google/index.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} can be used with many components of Google Cloud. Youcan deploy there, access data sources in Google Cloud and use some of the otherfeatures of Google Cloud.Find all the relevant information for using {{site.terms.sep}} and Google Cloudin the following sections and guides.## DeploymentsYou can install {{site.terms.sep}} on Google Kubernetes Engine (GKE), GoogleCloud Compute Engine, or through the Google Cloud Marketplace, once you haveestablished your enterprise account.To get started {{site.terms.sb}} offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### Google Kubernetes Engine (GKE)GKE is a fully supported deployment service for {{site.terms.sep}} with[more information available in the dedicated documentation](./gke.html).### Google Cloud Compute EngineAs an alternative to using GKE, you can use virtual machines in Google CloudCompute Engine for your {{site.terms.sb}} cluster, managing an RPM ortarball-based installation with [StarburstAdmin](../../starburst-enterprise/starburst-admin/index.html).### Google Cloud Marketplace{{site.terms.sep}} is certified on [GoogleCloud](https://console.cloud.google.com/marketplace/product/starburst-public/starburst-enterprise) and is available straight from the Cloud Console, lettingyou deploy {{site.terms.sep}} now, and scale later as your data needs grow.        ## DocumentationYou are currently viewing the Google Cloud section of our user guides. In theleft-hand navigation are guides for Google Cloud-specific topics. We also haveuser guides tailored toward the different types of {{site.terms.sb}} users:[data consumers, data engineers, and platformadministrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### Google Cloud Marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:- Email-only support- Five [email issues to gcpsupport@starburstdata.com](mailto:gcpsupport@starburstdata.com) per month- First response SLA of one business day- Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Red Hat OpenShift",
  "url": "/ecosystems/redhat/index.html",
  "content": "# {{page.title}}OpenShift from Red Hat Marketplace (RHM) is a container platform usingKubernetes operators that automate the provisioning, management, and scaling ofapplications to any cloud platform or even on-prem. Whether you need informationabout deploying {{site.terms.sb}} on OpenShift, or working with data sourcesthere, we have the information you need right here.{{site.terms.sb}} offers a stable, secure, efficient, and cost-effective way toquery all your enterprise data, wherever it resides with{{site.terms.sep_first}}. Our Kubernetes deployments ease the burden andcomplexity of configuring, deploying, and managing {{site.terms.sep}}.{% include video.html content=&quot;Check out this video demonstration by Karl Eklund, Principal Architect atRed Hat, showing OpenShift as a central location to explore data and buildmodels.&quot; %}## Deployments{{site.terms.sep}} is available to install through RHM, or[directly](https://harbor.starburstdata.net/) from {{site.terms.sb}} once youhave established your enterprise account.{{site.terms.sb}} also offers Starburst also offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### OpenShift deploymentsYou can use {{site.terms.sb}}&#39;s Kubernetes deployment directly on OpenShift.More information is available in our [Kubernetes referencedocumentation](../../latest/k8s.html), including our [customizationguide](../../latest/k8s/sep-configuration.html) for {{site.terms.sep}}.We also have a helpful [installationchecklist](../../latest/k8s/installation.html) for an overview of the generalHelm-based installation and upgrade process.### Red Hat Marketplace{{site.terms.sep}} is available as an operator on[RHM](https://marketplace.redhat.com/en-us/products/starburst-enterprise)as of OpenShift version 4. Our [Red Hat OpenShift deployment guide](./openshift-deployment.html) provides instructions for using {{site.terms.sb}}&#39;s Helm-based installation and upgrade process.        ## DocumentationYou are currently viewing the Red Hat OpenShift section of our user guides. Inthe left-hand navigation are guides for OpenShift-specific topics. We also haveuser guides tailored toward the different types of {{site.terms.sb}} users:[data consumers, data engineers, and platformadministrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### AWS marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:- Support through the OpenShift Support portal- Five issues per month- First response SLA of one business day- Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Get started",
  "url": "/get-started/index.html",
  "content": "                    {{page.title}}                                              {{site.terms.sb}} makes it easy to set up trial configurations for the {{site.terms.sep_first}}.        Try {{site.terms.sep_full}}.                                                                                          Choose the right deployment              This guide helps you select the best solution for your organization.                                                              Welcome to the user guides. Select your role below to start your journey.                                                                              Data consumer              I use data from {{site.terms.sb}} with my BI and data science tools to create important business insights.                                                                                                    Data engineer              I provide the source data to data consumers and ensure its quality, availability, and performance.                                                                                                    Platform administrator              I install and manage the clusters that serve our data,                 and ensure everything is performing well.                                                Still not sure where to start? Head over to our user personas page to learn which fits you best.              Browse topics                                              Security                                                                    Video library                                                                    Glossary                                                                    Connectors                              "
 },
 {
  "title": "Starburst Insights and Worksheet query editor",
  "url": "/data-consumer/clients/insights.html",
  "content": "# {{ page.title }}{{site.terms.insights}} is a web-based interface providing an intuitive queryeditor and data browser, along with a visual overview of important querystatistics. It is available as part of {{site.terms.sep_full}}, and needs to be[activated and configured](../../latest/insights.html) by your platformadministrator.## AccessTo access {{site.terms.insights}}, get the {{site.terms.sep_full}} cluster URLfrom your platform administrator. For example:```shellhttps://my.internal.url/```The {{site.terms.insights}} URL is that URL with `/ui/insights` appended:```shellhttps://my.internal.url/ui/insights```Login with the same credentials you use to access {{site.terms.sep_full}} withthe CLI or any other client.## Easy data exploration with Worksheet{{site.terms.insights}} includes an ad hoc querying tool, Worksheet. Worksheetis a powerful, web-based tabbed query editor and data source explorer. Itallows you to run multiple ad hoc queries and, with a click, drill down into theinformation you need to optimize it.{% include image.html  url=&#39;../../assets/img/general/insights-worksheet.png&#39;  img-id=&#39;insight-worksheet&#39;  alt-text=&#39;Insights Worksheet&#39;  descr-text=&#39;Image depicting Insights Worksheet&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;  screenshot=&#39;true&#39;%}Worksheet is meant for quick exploration and query validation. If your resultset is very wide, or is greater than 1000 rows, it is truncated. Queries andresults are only available until a new query is run in a tab, or your page ortab is closed or refreshed, whichever comes first.## Detailed views of query statisticsYou can also drill down into query details, including step-by-step executionstatistics to help you optimize query performance.{% include image.html  url=&#39;../../assets/img/general/insights-query-details.png&#39;  img-id=&#39;insights-query-details&#39;  alt-text=&#39;Insights query details&#39;  descr-text=&#39;Image depicting Insights query details&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;  screenshot=&#39;true&#39;%}## Other features and details{{site.terms.insights}} includes [numerous otherfeatures](../../latest/insights.html) that allow your platform administrator tounderstand the performance and usage of your {{site.terms.sep_full}} cluster."
 },
 {
  "title": "Starburst Admin installation guide",
  "url": "/starburst-enterprise/starburst-admin/installation-guide.html",
  "content": "# {{ page.title }}You can proceed with the installation using the following steps, after[preparing the control node and the managed clusternodes](./index.html#requirements).## Define the inventoryBefore using the playbooks, you need to edit the [Ansible inventoryfile](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html),`hosts`, to define where to install the software:1. Copy the `playbooks/hosts.example` inventory file to `playbooks`, name it   `hosts` and set the hostname for your coordinator and your worker(s).2. Set the environment variable `ANSIBLE_INVENTORY` to the absolute path   of the hosts file, for example:   ```   export ANSIBLE_INVENTORY=~/.ansible/collections/ansible_collections/starburst/admin/playbooks/hosts   ```3. Specify the IP address of the single coordinator host and one or more worker   hosts.4. Set the username for Ansible to connect to the host with SSH with   `ansible_user` for each host.5. Set the password  value with `ansible_password` or use a path to a private   key using `ansible_ssh_private_key_file`.The following snippet shows a simple example with one coordinator and twoworkers:```[coordinator]172.28.0.2 ansible_user=root ansible_password=changeme[worker]172.28.0.3 ansible_user=root ansible_password=changeme172.28.0.4 ansible_user=root ansible_password=changeme```Run the Ansible `ping` command to validate connectivity to the hosts:```$ ansible all -m ping -i playbooks/hosts172.28.0.4 | SUCCESS =&gt; {    &quot;ansible_facts&quot;: {        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    },    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}172.28.0.2 | SUCCESS =&gt; {    &quot;ansible_facts&quot;: {        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    },    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}172.28.0.3 | SUCCESS =&gt; {    &quot;ansible_facts&quot;: {        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    },    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}```Alternatively to using `ANSIBLE_INVENTORY` for the location of the hosts file,you can create an `ansible.cfg` file in the root directory of `starburst-admin`(typically `~/.ansible/collections/ansible_collections/starburst/admin/`) withthe setting `inventory=playbooks/hosts` in the `[defaults]` section and run thecommands from that folder.```[defaults]inventory=playbooks/hosts```You can also specify the hosts file when running the `ansible-playbook` commandwith the `-i ` parameter.Maintaining multiple host files and potentially multiple set of configurationfiles enables you to manage multiple clusters.## Provide {{site.terms.sep_full}} packageDecide if you want to use the RPM or tarball installation. RPM requires `sudo`access and installs files in specific folders. When using the tarball, customfolders can be specified, and they don&#39;t have to be owned by root.Download the binary archive and place it in the `files` directory.Alternatively, configure the `installer_url` in `files/vars.yml` to point to aURL that is available from all your cluster nodes to download the packagedirectly.Reference the [documentation about the installplaybook](./playbook-reference.html#install) to learn more about all therelevant configuration steps and options.## Create minimal configurationThe default configuration is managed in separate configuration files in `files`as well as variables defined in in `playbooks/vars.yml`. It is suitable to getstarted after the following adjustments:* Update the `version` value in `vars.yml` to the version of downloaded binary  file.* Set the `environement` value in `vars.yml` to the desired cluster name.* Update the Java heap space memory configuration `-Xmx16G` in  `files/coordinator/jvm.config.j2` and `files/worker/jvm.config.j2` to  about 80% of the total available memory on each host.* Remove the `query.*` properties in  `files/coordinator/config.properties.j2` and  `files/worker/config.properties.j2` if you are using {{site.terms.sbadmin}}  1.0.0. Newer version do not include these properties. {{site.terms.sbadmin}}  calculates optimal values automatically.* Add the {{site.terms.sep_full}} license file to ``files/extra/etc`` as  ``etc/starburstdata.license``. For more information, see [Push  configurations](./playbook-reference.html#push-configs).The default setup includes a `tpch` catalog, that you can use to test queriesand ensure the cluster is operational.If you are using the tarball, you can edit the target installation paths.Proceed with this simple configuration until you have verified that the clustercan be started and used, and then follow up with more configuration and changes.Reference the [documentation about the push-configsplaybook](./playbook-reference.html#push-configs) to learn more about all therelevant configuration steps and options.The [operation guides](./operation-guides.html) include numerous tips and tricksfor these next steps for your deployment.## Run installation and start the clusterYou are finally ready to get {{site.terms.sep_full}} installed on all clusternodes. The following steps copy the package to all cluster nodes and install it.Ansible creates the necessary configuration files from `files` and the definedvariables to the nodes, and starts the application on all of them:```shellansible-playbook playbooks/install.ymlansible-playbook playbooks/push-configs.ymlansible-playbook playbooks/start.yml```* [install details](./playbook-reference.html#install)* [push-configs details](./playbook-reference.html#push-configs)* [start details](./playbook-reference.html#start)## Accessing the Web UIA first step to validate the cluster is to log in to the [WebUI](../../latest/admin/web-interface.html):* Navigate to port 8080 on the coordinator host with the IP address or the  configured fully qualified domain name.* Login with any username.* Verify the displayed number of workers equals the number of workers in the  `hosts` file.## Connecting with clientsYou can use the [CLI or any otherclient](../../data-consumer/clients/index.html) to connect to the cluster andverify that the `tpch` catalog is available.## Managing configurationFor production usage you need to manage a separate set of all configurationfiles for each cluster you operate. It is also important to keep track of anychanges. Typically a version control system such as Git is used.You can also expand your processes to automatic management with a GitOpsworkflow and tools such as [AnsibleTower](https://www.ansible.com/products/tower) or[Concord](https://concord.walmartlabs.com/).## Next stepsAfter the successful installation and first initial tests, you can proceed toread about all the [available playbooks](./playbook-reference.html) and [learnabout updating and operating the cluster](./operation-guides.html)."
 },
 {
  "title": "Starburst for data engineers",
  "url": "/data-engineer/introduction.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a fast, interactive distributed SQL query enginethat decouples compute from data storage. {{site.terms.sep}} lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single {{site.terms.sep}} query can combine data from all these datasources and more forming a data mesh.{{site.terms.sb}} can greatly reduce reliance expensive, complex and oftenbrittle ETL frameworks and their pipelines. Because it uses data instead of diskto execute queries across the cluster, it’s also fast. {{site.terms.sep}} canpull your landing times forward, and help you meet or beat your SLAs.## How does this work?{{site.terms.sep}} comes with [supported connectors](../latest/connector.html)providing high performance SQL-based access to most of the data platforms inyour organization - such as Teradata, Oracle, PostgreSQL, and Hive. Each dataplatform is defined as a [catalog](./catalogs.html). Catalogs, in turn, defineschemas and their tables. Catalogs also, at a minimum, define the[connector](./connectors.html) that {{site.terms.sep}} uses to connect to thatdata source:```connector.name=sqlserverconnection-url=jdbc:sqlserver://:;database=connection-user=rootconnection-password=secret```Once you have your connection established, many connectors have configurationproperties that help you tune the connector’s performance, such as for timeouts,retries and connection limits.{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;SEP architecture&#39;  descr-text=&#39;Image depicting SEP components and architecture&#39;  pxwidth=&#39;500&#39;  modal=&#39;true&#39;%}{{site.terms.sep}} uses an ANSI-compliant SQL that should feel comfortable andfamiliar. {{site.terms.sep}} takes care of translating your queries to thecorrect SQL syntax for your data source. If you are migrating from Hive, we havea [migration guide](../../latest/appendix/from-hive.html) in ourdocumentation.## How do I get started?As a first step you should read our [guide to choosing the right deployment](../get-started/choosing-the-right-deployment.html).If your organization does not already have {{site.terms.sep}}, you can request a[trial instance](https://www.starburst.io/#!), following the steps in [Trying{{site.terms.sb}} products](../starburst-enterprise/try/index.html)."
 },
 {
  "title": "Starburst for data consumers",
  "url": "/data-consumer/introduction.html",
  "content": "# {{ page.title }}If you champion data-driven decisions in your org, {{site.terms.sb}} hasthe tools to connect you to the data you need. {{site.terms.sb}} brings allyour data together in a single, federated environment. No more waiting for dataengineering to develop complicated ETL. The data universe is in your hands!{{site.terms.sep_full}} is a distributed SQL query engine. Maybe you know asingle variant of SQL, or maybe you know a few. {{site.terms.sb}}&#39;s SQL isANSI-compliant and should feel comfortable and familiar. It takes care oftranslating your queries to the correct SQL syntax for your data source. All youneed to access all your data from a myriad of sources is a single JDBC or ODBCclient in most cases, depending on your toolkit.Whether you are a data scientist or analyst delivering critical insights to thebusiness, or a developer building data-driven applications, you’ll find you caneasily query across multiple data sources, in a single query. Fast.## How does this work?Data platforms in your organization such as Snowflake, Postgres, and Hive aredefined by data engineers as *catalogs*. Catalogs, in turn, define schemas andtheir tables.  Depending on the data access controls in place, discovering whatdata catalogs are available to you across all of your data platforms can beeasy! Even through a [CLI](../latest/installation/cli.html), it’s asingle, simple query to get you started with your federated data:{% highlight sql %}presto&gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows){% endhighlight %}After that, you can easily explore schemas in a catalog with the familiar `SHOWSCHEMAS` command:{% highlight sql %}presto&gt; SHOW SCHEMAS FROM hive_sales LIKE `%rder%`; Schema--------- order_entries customer_orders(2 rows){% endhighlight %}From there, you can of course see the tables you might want to query:{% highlight sql %}presto&gt; SHOW TABLES FROM order_entries; Table------- orders order_items(2 rows){% endhighlight %}You might notice that even though you know from experience that some of yourdata is in MySQL and others in Hive, they all show up in the unified `SHOWCATALOGS` results. From here, you can simply join the data sources fromdifferent platforms as if they were from different tables. You just need to usetheir fully qualified names:{% highlight sql %}SELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &gt; 300LIMIT 2;{% endhighlight %}## How do I get started?The first order of business is to get the latest {{site.terms.sb}}[JDBC](../latest/installation/jdbc.html) or[ODBC](./clients/odbc.html) driver and get itinstalled. Note that even though you very likely already have a JDBC or ODBCdriver installed for your work, you do need the {{site.terms.sb}}-specificdriver. Be careful not to install either in the same directory with other JDBCor ODBC drivers!If your data ops group has not already given you the required connectioninformation, reach out to them for the following:* the JDBC URL - `jdbc:presto://example.net:8080`* whether your org is using SSL to connect* the type of authentication your org is using - username or LDAPWhen you have that info and your driver is installed, you are ready to connect.## What kind of tools can I use?More than likely, you can use all your current favorite client tools, and evenones on your wishlist with the [help of our tips andinstructions](./clients/index.html).{% include image.html  url=&#39;../assets/img/general/data-consumption-overview.png&#39;  img-id=&#39;dco&#39;  alt-text=&#39;Data Tools and SEP&#39;  descr-text=&#39;Image depicting how SEP connects data sources to analytics tools&#39;  pxwidth=&#39;400&#39;  modal=&#39;true&#39;%}## How do I migrate my data sources to {{site.terms.sb}}?In some cases, this is as easy as changing the sources in your `FROM` clauses.For some queries there could be slight differences between your data sources’native SQL and SQL, so some minor query editing is required. Rather thanchanging these production queries on the fly, we suggest using your favorite SQLclient or our own[CLI](../../latest/installation/cli.html) to test yourexisting queries before making changes to production.If you are migrating from Hive, we have a [migrationguide](../../latest/appendix/from-hive.html) in ourdocumentation. To help you learn how others have made the switch, here is ahandy walk-through of using[Looker](https://www.starburstdata.com/presto-videos/query-federation-using-looker-and-starburst-presto/)and {{site.terms.sep_full}} together.## Where can I learn more about {{site.terms.sb}}?From our documentation, of course! Visit our [data consumer&#39;s user guide]({{site.baseurl }}/data-consumer/index.html)."
 },
 {
  "title": "Starburst for data platform administrators",
  "url": "/platform-administrator/introduction.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a fast, interactive distributed SQL query enginethat decouples compute from data storage. {{site.terms.sep}} lets you query datawhere it lives, including Hive, Snowflake, MySQL and even proprietary datastores. A single {{site.terms.sep}} query can combine data from all these datasources and more. {{site.terms.sep}} can run on-prem as well as in many cloudenvironments.{{site.terms.sep}} can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast. It can pull your landing times forward, and help youmeet or beat your SLAs. And, {{site.terms.sep}} has robust access controloptions for your organization, from integrating with LDAP to using yourRanger-managed policies.## How does this work?[SEP](../latest/overview/concepts.html) is a distributed system that runson COTS hardware. The coordinator parses, analyzes and plans query execution,and then distributes the query plan for processing among worker machines in thecluster. Workers use connectors specific to your data sources, such asSnowflake, Postgres, and Hive to transform queries and return data.{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;Starburst cluster overview&#39;  descr-text=&#39;Image depicting Starburst components and architecture&#39;  pxwidth=&#39;500&#39;  modal=&#39;true&#39;%}{{site.terms.sep}} uses ANSI-compliant SQL, and takes care of translating yourqueries to the correct SQL syntax for your data sources.{{site.terms.sep}}’s ability to federate data sources in a single query reducesyour organization&#39;s reliance on temporary tables and more complex ETL pipelines.Because {{site.terms.sep}} query processing works in memory, your diskinvestment is light.## How do I get started?As a first step you should read our [guide to choosing the right deployment](../get-started/choosing-the-right-deployment.html).You can spin up a [trial instance](https://www.starburst.io/#!) following thesteps in [Trying {{site.terms.sb}}products]({{site.baseurl}}/starburst-enterprise/try).When you are ready to create a production cluster and you&#39;ve chosen one of ourKubernetes-based deployments, read our handy [introduction to SEP withKubernetes](../latest/k8s/overview.html) guide. It explains{{site.terms.sep}} cluster design and sizing, as well as best practices forcustomization and configuration with Helm.We also have some great [training videos](../videos/index.html) to get youstarted, and some articles on topics you are likely to have hard questions on:* Data architecture [philosophy and  approach](https://www.starburst.io/presto-videos/modern-data-architecture/)* [Reference  architectures](https://www.starburst.io/learn/open-source-trino/reference-architectures/)* Security  [guide](https://www.starburst.io/starburst-enterprise-security-guide) and  [deep dive](https://trino.io/blog/2020/08/13/training-security.html)* [SEP and Helm](../latest/k8s.html)* SEP [administration](../latest/admin.html)"
 },
 {
  "title": "JDBC driver",
  "url": "/data-consumer/clients/jdbc.html",
  "content": "# {{ page.title }}The Java Database Connectivity (JDBC) driver enables any application supportinga JDBC driver to connect to {{site.terms.sb}} clusters. The application can thenissue SQL queries and receive results.Typically the applications are JVM-based, or support JDBC driver usage with adifferent mechanism. Using an applications with the JDBC driver generallyfollows the same steps.## Connection informationGet the [connection information](./index.html#connection-information) and the[cluster version](./index.html#connection-information) for the cluster you wantto connect to and use to run queries.## Add the driver to your clientThe way to add the JDBC driver to your [client application](./index.html) variesfor each application.Some applications, such as [DBeaver](./dbeaver.html), automatically download andconfigure the driver, or include it by default and no action is necessary.Many applications, however, require you to download the driver and place it in adirectory that varies for each application. Refer to your client application&#39;sdocumentation for the necessary details.The JDBC driver needs to be compatible with the [version of the cluster you areconnecting to](./index.html#cluster-version).### Download the JDBC driverYou can request the JDBC driver to match a specific {{site.terms.sb}} versionfrom [{{site.terms.support}}](../../support.html) or [directly from thereference documentation](../../latest/installation/download.html#clients).Alternatively, to gain access to {{site.terms.sep}} archives, visit the[{{site.terms.sb}} website](https://www.starburst.io) and click either**Get Started** or **Download Free**.This opens a dialog that prompts for your name, email address, and location.Fill out the form *using a valid email address*, then click **Free Download**.Click the link to the **Downloads** page in the resulting email. The page isorganized into Long-Term Support (LTS) and Short-Term Support (STS) sections.The LTS section is split into **{{site.terms.sep_full}}** and **Clientapplications** steps.If your cluster version is a current release, use the **LTS Step 2** or **STS**sections to download the JDBC driver version that matches your cluster version.You can also download the JDBC driver binary directly from the link in thereference documentation:* For cluster versions older than 350, download JDBC driver [version  350](../../350-e/installation/download.html).* For cluster versions 350, 354, and newer, download the matching JDBC driver  version:  * [Version 354](../../354-e/installation/download.html)  * [Latest version](../../latest/installation/download.html)Use the reference documentation&#39;s version selector in the top right corner toselect the download page for different releases, or contact[{{site.terms.support}}](../../support.html).## Create a connection to the clusterWith an installed driver you can now configure a connection to a cluster.Typically, you only need the connection information. Other details such as theJDBC URL are automatically configured in many applications, or can be seen andupdated in a user interface dialog.The main parameters are the driver classname, which is typically preconfiguredby your client driver integration, the JDBC URL and the credentials from theneeded [connection information](./index.html#connection-information).Driver version 354 and newer:* Classname: `io.trino.jdbc.TrinoDriver`* JDBC URL: `jdbc:trino://host:port`Driver version 350 and older:* Classname: `io.prestosql.jdbc.PrestoDriver`* JDBC URL: `jdbc:presto://host:port`The `host` parameter is the network name or IP address of the coordinator ofyour {{site.terms.sep}} cluster. Enter the `port` provided by your networkadministrator, or enter one of the [default ports](./index.html#default-ports)for {{site.terms.sep}} clusters. See the [reference documentation for the JDBCdriver](../../latest/installation/jdbc.html) for other supported JDBCparameters.## Enable JDBC TLS supportIf your {{site.terms.sb}} cluster is configured to require a TLS connection fromclients, you must configure your client as follows:* Specify a valid username and password for the authentication system in use on  the coordinator, such as LDAP.* Specify the JDBC parameter `SSL=true` (where the older term `SSL` activates  `TLS` support).  * If the client UI provides an interface, such as a grid for specifying    parameters, use that.  * If not, you can append `?SSL=true` to the JDBC connection string. For    example:  ```url  jdbc:trino://host:port?SSL=true  ```## Start queryingThe configured connection can now be opened and you can start running queries.For example, if your application allows you to write and run queries, you cansee what catalogs are available:```sqlSHOW CATALOGS;```Many applications provide a user interface to see the list of catalogs, schema,tables and much more.## Next stepsNow you can take advantage of the features of your application, and potentiallylearn more about the [SQL support](../starburst-sql.html) in{{site.terms.sb}}."
 },
 {
  "title": "Try Starburst Enterprise with Kubernetes",
  "url": "/starburst-enterprise/try/k8s.html",
  "content": "# {{ page.title }}Kubernetes is the preferred platform for {{site.terms.sep_first}} deployments.You can manage the deployment directly with the available Helm charts orthrough your preferred cloud provider&#39;s[marketplace](../../ecosystems/index.html). The following platforms aresupported:* Amazon Elastic Kubernetes Service (EKS)* Google Kubernetes Engine (GKE)* Microsoft Azure Kubernetes Service (AKS)* Red Hat OpenShift 4.x or higherYou can use a K8s deployment as a proof-of-concept (PoC) and scale it up easilyto a fully-fledged deployment of {{site.terms.sep}} in your organization.{% include note.html content=&quot;The [marketplaceofferings](../../ecosystems/index.html) provide even more convenient, yet lessflexible alternatives for running Starburst Enterprise.&quot;%}This page assumes you have the following:* Familiarity with Kubernetes.* ``kubectl`` is installed and running.* You can create and administer a Kubernetes cluster.* You have access to our [Helm chart repository](../../latest/k8s/requirements.html#k8s-helm-repository) and  [Docker registry](../../latest/k8s/requirements.html#k8s-docker-registry),  provided by {{site.terms.support}}.Your {{site.terms.sep}} cluster must be located in a namespace dedicated to itand it alone. Our K8s overview discusses {{site.terms.sep}} [clusterdesign](../../latest/k8s/overview.html#kubernetes-cluster-design-for-sep-full),and our [K8s requirements](../../latest/k8s/requirements.html) covers minimumproduction cluster specs and versions for tools that you need.## {{site.terms.sb}} Helm chartsThe following Helm charts are available as part of the {{site.terms.sep}} K8soffering:* [Starburst Enterprise](../../latest/k8s/sep-configuration) for images, security,  coordinator and worker nodes, as well as catalogs, mounted volumes and other  properties.* [Apache Ranger](../../latest/k8s/ranger-configuration) including the  Starburst Ranger plugin, policy database and everything you need to integrate  Ranger with {{site.terms.sep}}.* Apache [Hive Metastore Service](../../latest/k8s/hms-configuration).* [Starburst cache service](../../latest/k8s/cache-service-configuration) to use  {{site.terms.sb}} {{site.terms.cached_views}}.## Prepare for deploymentA proof-of-concept for {{site.terms.sep}} can have as few as two nodes, oneeach for the coordinator and a worker, in a dedicated namespace.### Set up registry accessOnce you have the {{site.terms.sep}} Helm chart, create the``registry-access.yaml`` file to override the default, empty values as describedin our [referencedocumentation](../../latest/k8s/installation.html#create-this-one-file-before-you-begin).You use this one file for all {{site.terms.sb}} Helm charts.### Prepare cluster and resource configurationsNow it&#39;s time to create your [correctly-sized Kubernetescluster](../../latest/k8s/requirements.html#k8s-cluster-requirements), andensure that your Helm/``kubectl`` configuration points at the correct clusterusing ``kubectl cluster-info``.With that done, create a [minimal YAML configurationfile](../../latest/k8s/sep-config-examples.html#k8s-ex-sep-prod-setup) with thememory and CPU resource configurations for ``coordinator:`` and ``worker:`` thatreflect your cluster&#39;s available resources. We suggest a name such as``sep-PoC-setup.yaml``.{% include warning.html content=&quot;**Do not skip this step.** The default valuesfor memory and CPU resources likely vary significantly from your cluster&#39;savailable resources. If you attempt to run SEP with the defaults,it may not start.&quot; %}## Deploy {{site.terms.sep}}Run the ``helm`` command to install the default chart, as well as any overrideYAML files using the ``--values`` argument, as in the following example:```shell$ helm upgrade my-sep-poc-cluster starburstdata/starburst-enterprise     --install     --version 360.2.0     --values ./registry-access.yaml     --values ./sep-PoC-setup.yaml```We strongly suggest using the ``helm upgrade`` rather than ``helminstall``. It ignores any unchanged configuration files and only applies thosewith changes.### VerifyTo verify that your {{site.terms.sep}} cluster is operating, first determine theIP address or the DNS hostname of the coordinator by running the ``kubectl getpods`` command. Next, use the IP address or hostname to verify the coordinatoris running by using the Web UI as described in [Verify theserver](./verify.html). You can use the same information to connect with the CLIor the JDBC driver.## Configure your clusterFor a proof-of-concept, we suggest that you start with small, focused changesthat configure one concept, such as networking, security, or data sources.Depending on your organization&#39;s security requirements and the location of your{{site.terms.sep}} cluster relative to your data sources, you may find that youneed to configure [internalcommunication](../../latest/k8s/sep-configuration.html#internal-communication) and[networking](../../latest/k8s/sep-configuration.html#exposing-the-cluster) before youcan configure a data source.Once you have verified that your cluster has access to your data sources, it&#39;stime to [configure the datacatalogs](../../latest/k8s/sep-configuration.html#catalogs) that describe your datasources.### Restart to deploy configuration changesRun the ``helm upgrade`` command to apply any changes, adding any new YAMLfiles, for instance, in the following example, an ``sep-catalogs.yaml`` file hasbeen added:```shell$ helm upgrade my-sep-poc-cluster starburstdata/starburst-enterprise     --install     --version 360.2.0     --values ./registry-access.yaml     --values ./sep-PoC-setup.yaml     --values ./sep-catalogs.yaml```## Next stepsAs you build out your PoC and beyond, our reference documentation has anextensive [SEP configurationsection](../../latest/k8s/sep-configuration.html), along with[examples](../../latest/k8s/sep-config-examples.html). You may also wish to setup these optional items:* [Hive Metastore Service](../../latest/k8s/hms-configuration)* [Apache Ranger](../../latest/k8s/ranger-configuration)* [Starurst cache service](../../latest/k8s/cache-service-configuration)"
 },
 {
  "title": "Looker",
  "url": "/data-consumer/clients/looker.html",
  "content": "# {{ page.title }}[Looker](https://looker.com) is a powerful cloud-based business intelligencetool. It is a cloud application integrated with the Google Cloud data analyticsplatform. Looker supports Trino as a dialect with the [JDBC driver](jdbc.html)pre-installed, so you can use Looker out-of-the-box to access {{site.terms.sep}}clusters.  ## RequirementsLooker/Google Cloud must have network connectivity to your {{site.terms.sep}}cluster.## ConnectionUse the following steps to prepare Looker to access your cluster:1. Get the necessary [connection information](./index.html#basic-connection-information)   for your cluster.2. In Looker, select **Admin &gt; Connection &gt; Add Connection**.3. On the **Connection Settings** page, enter the following information:    * **Name**: a name for the {{site.terms.sb}} connection    * **Dialect**: Select *Trino*. If you are using {{site.terms.sep}} version      351-e or earlier, select *PrestoSQL*.    * **Remote Host:Port**: the FQDN or IP address and port of your cluster      or its load balancer/proxy    * **Database**: a catalog name in your {{site.terms.sep}} cluster for Looker      to examine by default    * **Username**: For a cluster without security, enter any name. For a      cluster with authentication configured, enter a valid username for the      authentication type in use on your cluster, such as LDAP.    * **Password**: For a cluster without security, leave this field blank. For      a cluster with authentication configured, enter the password for the      username provided.    * **SSL**: If your cluster is using TLS/HTTPS, enable this setting.      Otherwise, leave disabled.    * **Verify SSL**: Ignore this field. See [TLS/HTTPS](#tlshttps) for      information on how Looker handles TLS and the Java TrustStore.    Configure the remaining optional settings as desired for your cluster. For    more information on these settings, read the    [Looker documentation](https://docs.looker.com/setup-and-management/connecting-to-db#creating_a_new_database_connection).   &amp;nbsp;{% include image.html   url=&#39;../../assets/img/general/looker-connection.png&#39;   img-id=&#39;Connection configuration&#39;   alt-text=&#39;Configuring a connection to Starburst Enterprise &#39;   descr-text=&#39;Image showing an example configuration for Starburst Enterprise   in Looker&#39;   pxwidth=&#39;500&#39;   screenshot=&#39;true&#39;   %}4. Click **Test These Settings** to run a simple connection test to the cluster.5. After a successful test, click **Update Connection** to save the   configuration.## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is also required touse [TLS/HTTPS](../../latest/security/tls). If you are following the bestpractice to use a globally trusted certificate, use the cluster&#39;s HTTPS URL inthe **Remote Host** field as shown in the steps above.If you&#39;re not using a globally trusted certificate, you may have to configurethe TrustStore used by Looker. By default, Looker uses the standard JavaTrustStore. To modify the TrustStore configuration for the {{site.terms.sep}}connection, add the necessary[JDBC driver parameters](../../latest/installation/jdbc#connection-parameters)to the connection configuration, in the **Additional Params** field.## QueryingYou can create Looker models and projects with the {{site.terms.sb}} connectionyou defined. You can also use SQL Runner to query the catalogs configured inyour {{site.terms.sep}} cluster."
 },
 {
  "title": "Markdown usage",
  "url": "/internal/markdown.html",
  "content": "# {{ page.title }}This page shows the output of all the supported markdown syntax usage. The siteuses CommonMark.## Markdown source files- Standard extension is `.md`- 80 character hard wrap width for markdown code- Embedded HTML can be wider- Use 2 space indent for HTML (no tab, not wider)## Section titlesTitles are marked with one or more `#`.Before title..# Level 1after title and before next one, should only be used for page title## Level 2after title and before next one### Level 3after title and before next one#### Level 4after title and before next one##### Level 5after title and before next one, too deep, probably should never be used###### Level 6after title and before next one, too deep, probably should never be used## Text formattingNormal text in a paragraph. You can just write along. Spaces between word orline breaks don&#39;t matter. An empty line starts a new paragraph.Use `_` or `*` to highlight words in _italics text_. Use `__` or `**` tohighlight words in __bolded text__. Don&#39;t mix bolding and italics.* __bold with underscore__* **bold with double asterisk*** _italics with underscore_* *italics with asterisk*Standard usage at Starburst is to use asterisk `*bold*`.## Code blocks and other source codeInline usage of source code uses simple backticks to surround the variable orsimple command:Add `~/bin` to the `PATH` with `EXPORT PATH=~/bin:$PATH`.Separate code block with three backticks:```cd /opt/dev/myprojectmvn clean install```You can declare a language like `shell` after the initial three backticks toget syntax highlighting.Here is a `shell` fenced block:```shellcd /opt/dev/myprojectmvn clean install```A `java` fenced block:```javaString a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an int```A `yaml` fenced block:```yamlimage:  repository: &quot;harbor.starburstdata.net/starburstdata/hive&quot;  tag: &quot;338.2.2-rc.1-SNAPSHOT&quot;  pullPolicy: &quot;IfNotPresent&quot;expose:  type: &quot;clusterIp&quot;  clusterIp:    name: &quot;hive&quot;    ports:      http:        port: 9083  nodePort:    name: &quot;hive&quot;    ports:      http:        port: 9083        nodePort: 30083```A `sql` fenced block:```sqlSELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]```If for some reason you really must have line numbers, then you must use a liquidhighlight block. This `java` highlight block uses the `linenos` keyword todisplay line numbers:{% highlight java linenos %}String a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an int{% endhighlight %}However, if you find yourself needing to refer to line numbers, your codeblockis likely too long for your purpose. Consider instead a one- to two-line excerptto illustrate what you are writing about, even if you keep the longer codeblockintact.## Unordered listsUse `-` or `*` for the items.- Apple- Pear- BananaYou can also indent:- Pets    - Dog    - Cat- Farm animal    - Cow    - Sheep## Ordered listsThe actual numbers in your document&#39;s source code aren&#39;t used as the line numbervalues in static documents. The line numbers you see in the generated documentsare  computed. So you can use e.g. 1. all the time. This example uses 1. for alllines in the source document, but the generated document are numbered correctly:1. Item 11. Item 2    1. Nested 1 (nested items in lists require 4 spaces to indent properly)    1. Nested 21. Item 3## Definition listsUse [dl/dt/dd HTML](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dl):  Term 1  the description for term 1  Term 2  the description for term 2## Links- Markdown syntax with `[text](url)` is preferred- Relative links are preferred to avoid issues with the root context- Ideally use `({{ site.baseurl }}/file.html ` for absolute links## TablesDirect layout markup:| Tables        | Are           | Cool  || ------------- |:-------------:| -----:|| col 3 is      | right-aligned | $1600 || col 2 is      | centered      |   $12 || zebra stripes | are neat      |    $1 |Semantic markup:Markdown | Less | Pretty--- | --- | ---*Still* | `renders` | **nicely**1 | 2 | 3Raw HTML:            First name      Last name                  Alison      Loney              Manfred      Moser      ## VideosEmbedding YouTube videos is supported courtesyhttps://github.com/nathancy/jekyll-embed-video. video-embed.css is included tomake it responsive, but that is untested. Other sources can be supported, butYouTube is all we need for now.You must include the video ID you want to use in the front matter.```---youtubeId1: buqtdpuZxvk---```You can have multiple videos on a page, you just have to give them differentnames in the front matter.If you want to use an entire video, just use youtubePlayer.html in your include,with the :```include youtubePlayer.html```If instead you want to play just a portion, use youtubeSnippet.html in yourinclude, and provide start and end values in seconds after the video id:```id=page.youtubeId1 start=22 end=31```At this point, you might be wondering to yourself, &quot;But what if I want to startat a particular place, then play until the end?&quot; Great question! In that case,just set the value for end to -1:```id=page.youtubeId1 start=22 end=-1```You&#39;ll have to read the .md file for now to see the entire include, as it justrenders the included HTML as html in a comment:{% include youtubeSnippet.html id=page.youtubeId1 start=22 end=31 %}If you are looking to include a Wistia video, use the wistiaPlayer.html in your include.You can customize the start time by using the `start` property with thefollowing format `start=&#39;1m30s&#39;`.*You **must** use either single or double quotes when adding a property. Thevalue of the `id` parameter is the same as the value of the `wvideo=` argument inthe Wistia URL.```{%raw%}{% include wistiaPlayer.html id=&#39;j38ihh83m5&#39; %}{%endraw%}```{% include wistiaPlayer.html id=&#39;j38ihh83m5&#39; %}If you just want to add a admonition with a text to a video, use the``video.html``. Careful about the apostrophes used for the embedded URL.{% include video.html content=&quot;Check out the TrinoCommunity Broadcast interview with the Apache Superset teamto learn more and see a demo.&quot; %}## AdmonitionsThe simplest admonition is just using a blockquote.&gt; This is a simple blockquote.And after this insightful blockquote, you can have a look at a even biggerblockquote.&gt; And here is a longer one.&gt;&gt; Its has two sentences. And also&gt;&gt; * a list item&gt; * another itemBeyond that you can use specific warning, caution and note admonitions.As per [Google developer documentation(GDD)](https://developers.google.com/style/notices) a warning is: &quot;Stronger thana Caution; it means &quot;Don&#39;t do this.&quot;Note that multi-line, paragraph-shaped passages work in the styling:{% include warning.html content=&quot;Like a red morn that ever yet betokened,Wreck to the seaman, tempest to the field,Sorrow to the shepherds, woe unto the birds,Gusts and foul flaws to herdmen and to herds.William Shakespeare&quot; %}A caution, which, as per GDD &quot;Suggests proceeding with caution&quot;:{% include caution.html content=&quot;Do not be tricked into thinking that there areno crocodiles just because the water is still. - Malaysian proverb&quot; %}A note (GDD: &quot;An ordinary note or tip&quot;):{% include note.html content=&quot;Try using a small LIMIT when testing out a newquery.&quot; %}A note with a long link to see if it breaks/wraps:{% include note.html content=&quot;This is an example note to see how the line breakis handled by css.https://starburstdata.atlassian.net/wiki/spaces/PM/pages/1080688861/Starburst+Galaxy+UX+Research-let+saddareallylong+link&quot; %}You can also use [capture to create a large section ofcontent](https://jekyllrb.com/docs/includes/#passing-parameter-variables-to-includes)and then pass that to the admonition.## Side navigationTo include multiple levels of navigation you will need to use the following ymlsyntax:```nav:  - title: SEP Overview    context: /starburst-enterprise/index  - title: Platform administrators    id: admins    collapse: /platform-administrator    subnavigation:      - title: Home        context: /platform-administrator/index      - title: SEP clusters        id: test        collapse: /platform-administrator/cluster        subfolderitems:         - title: Test           context: /platform-administrator/cluster/test```"
 },
 {
  "title": "Materialized views",
  "url": "/data-consumer/materialized-views.html",
  "content": "# {{ page.title }}[Starburst Enterprise platform](../starburst-enterprise/index.html)({{site.terms.sep}}) supports materialized views with the[Hive](../latest/connector/starburst-hive.html#materialized-views) and[Iceberg](../latest/connector/iceberg.html#materialized-views) connectors.Materialized views increase query performance by providing pre-computed resultsfrom complex, analytical queries. With {{site.terms.sep}}, you can run federatedqueries, create materialized views, and access catalogs of results through theHive and Iceberg connectors.Materialized views created in a Hive catalog may be automatically refreshed, asdiscussed in this document.{% include note.html content=&quot;There is no support at this time for automaticrefresh of materialized views in Iceberg.&quot; %}## PrerequisitesYour {{site.terms.sep}} platform administrator or a data engineer withadministrative access to {{site.terms.sep}} has to first enable materializedviews in one or more Hive or Iceberg catalogs. Check with them to learn whichcatalogs are enabled, and which schemas may be used.You also must have the necessary access privileges to create data in the schemadesignated in each enabled catalog.## Create and use a materialized viewAny federated query that runs successfully in {{site.terms.sep}} can be used tocreate a materialized view.  Materialized views in {{site.terms.sep}} arecreated in the same way as in other data platforms, with a [CREATE MATERIALIZEDVIEW](../latest/sql/create-materialized-view.html) statement. In the followingexample, the `mysalescatalog` has been configured to allow materialized views:```sqlCREATE MATERIALIZED VIEW mysalescatalog.mysalesschema.mv_cust_tot_return AS    SELECT      sr_customer_sk ctr_customer_sk,      sr_store_sk ctr_store_sk,      sum(sr_return_amt) ctr_total_return    FROM    tpcds.sf1.store_returns,    tpcds.sf1.date_dim    WHERE ( (sr_returned_date_sk = d_date_sk) AND (d_year &gt; 2000) )    GROUP BY sr_customer_sk, sr_store_sk;```Once a materialized view exists, you can query it like any regular table:```sqlSELECT * FROM mycatalog.mysalesschema.mv_cust_tot_return;```## Automatically refresh materialized views in HiveHive catalogs in {{site.terms.sep}} configured to allow materialized viewsprovide several `WITH` clause properties to configure refresh schedules and hownew data is imported:* `refresh_interval` and `cron`: Choose one method to specify a refresh  frequency. The defined interval must be greater than or equal to five minutes.* `max_import_duration`: Specifies how long to allow a refresh to complete  before failing.* `grace_period`: Specifies the amount of time in-flight queries can run against  an expiring snapshot.* `incremental_column`: Specifies the column to be used to identify new data  since the last refresh. If you do not use this field, {{site.terms.sep}} performs a full refresh.We suggest that you review our reference documentation, which has [moreinformation about theseproperties](../latest/connector/starburst-hive.html#automated-materialized-view-management).In the following example, the `refresh_interval` property is used toautomatically refresh the data every 24 hours from the time the `CREATE`statement initially runs:```sqlCREATE MATERIALIZED VIEW myhive.mysalesschema.mv_cust_tot_returnWITH (  refresh_interval = &#39;24.0h&#39;,  grace_period = &#39;5.00m&#39;,  max_import_duration = &#39;30.00m&#39;) AS    SELECT      sr_customer_sk ctr_customer_sk,      sr_store_sk ctr_store_sk,      sum(sr_return_amt) ctr_total_return    FROM    tpcds.sf1.store_returns,    tpcds.sf1.date_dim    WHERE ( (sr_returned_date_sk = d_date_sk) AND (d_year &gt; 2000) )    GROUP BY sr_customer_sk, sr_store_sk;```In this example, the refresh runs for a maximum of 30 minutes. Unless the`cron` property is specified, the time at which data in a materialized view isrefreshed is based on the moment the `CREATE MATERIALIZED VIEW` statement firstruns, plus the `refresh_interval`.You can run refreshes on a set schedule by using the `cron` property instead.The `cron` property uses normal cron expressions. Here is the same materializedview, created with a `cron` schedule and an incremental column:```sqlCREATE MATERIALIZED VIEW myhive.mysalesschema.mv_cust_tot_returnWITH (  cron = &#39;30 2 * * *&#39;  grace_period = &#39;5.00m&#39;,  max_import_duration = &#39;30.00m&#39;,  incremental_column = &#39;sr_returned_date_sk&#39;) AS    SELECT      sr_customer_sk ctr_customer_sk,      sr_store_sk ctr_store_sk,      sum(sr_return_amt) ctr_total_return    FROM    tpcds.sf1.store_returns,    tpcds.sf1.date_dim    WHERE ( (sr_returned_date_sk = d_date_sk) AND (d_year &gt; 2000) )    GROUP BY sr_customer_sk, sr_store_sk;```This causes the refresh to execute at 2:30 AM daily, and loads only new data asdetermined by the `sr_returned_date_sk` date column."
 },
 {
  "title": "Metabase",
  "url": "/data-consumer/clients/metabase.html",
  "content": "# {{ page.title }}"
 },
 {
  "title": "Metastore options",
  "url": "/platform-administrator/metastores.html",
  "content": "# {{ page.title }}Metastores map files in distributed objects stores such as S3 and HDFS intotables, and provide metadata such as columns names and type mapping. Metastoresalso provide data to the cost-based optimizer. A metastore service is requiredfor {{site.terms.sb}} products when querying object storage systems with theHive connector and others. Your choice of metastore service depends on which{{site.terms.sb}} product you are using:* Amazon Glue - {{site.terms.sep_first}} on AWS* Hive Metastore Service (HMS) - {{site.terms.sep}} on any platform* {{site.terms.sb}} built-in metastore service - {{site.terms.sg}} only## The {{site.terms.sg}} metastore{{site.terms.sg}} provides a built-in metastore that requires no additionalinstallation.## Amazon GlueIf you are deploying {{site.terms.sep}} to AWS, you can use [AmazonGlue](../ecosystems/amazon/data-sources/glue.html) as your metastore for[Hive](../latest/connector/starburst-hive.html), [DeltaLake](../latest/connector/starburst-delta-lake.html) and[Iceberg](../latest/connector/starburst-iceberg.html) data.## Hive Metastore ServiceYou can deploy HMS using the {{site.terms.sb}} Kubernetes (K8s) Helm chart foruse with {{site.terms.sep}} on supported [Kubernetesservices](../../../latest/k8s/overview.html#kubernetes-platforms).This section describes the process for deploying HMS into your environment. Our[referencedocumentation](../../../latest/k8s/hms-configuration.html)contains a complete listing of configuration properties and additionalcustomization options in the Helm chart.### Get the Helm chartGet the latest `starburst-hive` Helm chart from the {{site.terms.sb}}. The linkis available in the [installationchecklist](../../../latest/k8s/installation.html#sep-installation). Review our[customization bestpractices](../../../latest/k8s/overview.html#customization-best-practices) toensure your customizations are easy to apply when you upgrade to later versions.### Provide your registry credentialsWe recommend that you use a separate `registry-access.yaml` file across all Helmcharts as described in our SEP [K8s installationinstructions](../../../latest/k8s/installation.html#create-this-one-file-before-you-begin).As an alternative, you can edit the `registryCredentials:` node of the Ranger Helmchart to include them.### Configure the serverEnsure that the following values in the Helm chart reflect your environment:* `serviceAccountName:` - We strongly recommend using a [service  account](../../latest/k8s/hms-configuration.html#service-account) for the pod.* `expose:` section - Set the value for the correct `type:` for your environment,  and [configure the required key-value pairs for the  type](../../latest/k8s/hms-configuration.html#exposing-the-pod-to-outside-network).* Top-level `resources:` - Ensure that the CPU and memory sizes are appropriate  for your instance type.{% include caution.html content=&quot;We strongly suggest leaving the`heapSizePercentage:` at the default value.&quot; %}### Configure the PostgreSQL backing databaseThe configuration properties for the PostgreSQL database are found in the`database:` top-level node. As a minimal customization, you must ensure that thefollowing are set correctly for your environment:```yamldatabase:  type: &quot;internal&quot;  internal:    port: 5432    databaseName: &quot;hive&quot;    databaseUser: &quot;Hive&quot;    databasePassword: &quot;HivePassw0rd1234&quot;```You must also configure `volume:` persistence and resources, as well as the`resources:` for the backing database itself in the `database:` node. For acomplete list of available backing database properties, see our [referencedocumentation](../../latest/k8s/hms-configuration.html#internal-database-backend-for-hms).{% include note.html content=&quot;The `database.resources:` node is separate fromthe top level `resources:` node. It defines the resources available to thebacking database itself, not the HMS server.&quot; %}### Setup authenticationIn the `hdfs:` section of the Helm chart, you [provide authentication for thestorage account](../../latest/k8s/hms-configuration.html#storage) used to queryand create objects. Secrets are specified directly in the HMS chart.{% include note.html content=&quot;For AWS, you can provide access to S3 usingsecrets, or by using IAM credentials attached to the metastore pod.&quot; %}### Run the Helm commandWhen the HMS is configured as desired for your organization, run the followingcommand to deploy it:```shell$ helm upgrade -i hms starburst/starburst-hive -f hms-values.yaml```Once the pod is deployed, other services can use this HMS if needed."
 },
 {
  "title": "Migrating your analytics to Starburst",
  "url": "/data-consumer/migration.html",
  "content": "# {{ page.title }}In some cases, migrating your analytics to {{site.terms.sb}} is as easy asswapping out your client, such as a JDBC driver, and changing the sources inyour `FROM` clauses. But because not every data source implements the SQLstandard the same, there could be slight differences between your data sources’native SQL and {{site.terms.sb}}&#39;s SQL. The information on this page will getyou started moving your tools and workflows to {{site.terms.sb}}.## Getting and installing {{site.terms.sb}} clients{{site.terms.sb}} has JDBC and ODBC drivers to connect your favorite tools to yourfavorite data. We have an [entire documentsection](./clients/index.html) that covers downloading, installing andconnecting clients.## Migrating queries from ANSI-standard SQL implementationsFor SQL implementations that follow the ANSI standard closely, only minor queryedits are likely. Rather than changing these production queries on the flythough, we suggest using your favorite SQL client or our own[CLI](./clients/cli.html) to test your existing queriesbefore making changes to production.Our full {{site.terms.sb}} [SQL referencemanual](../latest/language.html) is available to help youresolve any small implementation differences as you migrate your queries to{{site.terms.sb}}.## Migrating queries from HiveIn other cases where a SQL implementation deviates significantly from the ANSIstandard, such as with Hive&#39;s HiveQL, there some things you&#39;ll want tokeep in mind. Our documentation covers the syntactic and semantic differencesbetween HiveQL and other non-ANSI standard implementations such as:* Array syntax and handling* Syntax for strings and identifiers* CAST considerations* Differences in datediff* Complex expressions and subqueries* INSERT and OVERWRITE operationsRead the [Hive migration](../../latest/appendix/from-hive.html) page in ourreference documentation for detailed information on these topics.## Reducing ETLWith {{site.terms.sb}}, there are many opportunities to both reduce yourreliance on increasingly complex ETL pipelines and the intermediate storage tocentralize data that you must transform and clean up tech debt while you do so.These are complex problems and it can be hard to know where to begin. Thefollowing sections offer some strategies that make approaching the problemeasier.### Reduce or remove duplicative or similar processingIf you have multiple pipelines that ultimately produce the similar data with thesame grain, start there by combining those queries. For example, if youcalculate budget for ads at an ad_id level, and someone else is calculatinginvoices for the same ad_id grain, combine those queries. You save the doublecompute and storage demands so that you don’t scan the data twice, then move itaround twice, and finally write it twice with just one column being different.### Optimize around end usageAs you are doing your due diligence for this migration, whether you choose to leave some pipelines in your current framework or not, take the opportunity to review your pipeline design against their purpose:* For end usage purely for downstream ETL, optimize for writes* For end usage for dashboards and reports, optimize for readsFor in-depth information on optimizing queries, dive into our [trainingvideo](../data-engineer/query-performance.html) onoptimizing query performance, and reference the [queryoptimizer](../latest/optimizer.html) section of our referencedocumentation.### Reduce or remove the need for temp tables and intermediate disk storageData products that are derived from disparate sources often need to land in anintermediate schema where they can be combined locally. With {{site.terms.sb}},you can simplify this type of processing and remove the need for temp tables andschemas by taking advantage of {{site.terms.sb}}&#39;s powerful query federationabilities. This has the additional, positive side effect of obviating the needfor managing temp table cleanup jobs, too.### Reduce or remove little-used queries and data productsIf you are not already, start measuring the usage for dashboards and reports. Have any been abandoned? Has the usage shrunk enough on any of them so that there is no longer a justifiable ROI on maintaining them? If so, work with your stakeholders on a sunset plan, and remove the surfaces and pipelines.## Next stepsOnce you have cleaned up your pipelines, start looking at where you can delightyour customers with more aggressive data landing times. Upstream data ETL andavailability will continue to set a lower bound on your landing times, but thereis a positive, domino effect from reducing pipeline complexity and the needto wait for slow compute and storage operations."
 },
 {
  "title": "MySQL catalogs",
  "url": "/starburst-galaxy/catalogs/mysql.html",
  "content": "# {{page.title}}You can use a MySQL catalog to configure access to a[MySQL](https://www.mysql.com/) or compatible database in the followingdeployments:Amazon RDS on Amazon Web Services:* [Amazon Aurora](https://aws.amazon.com/rds/aurora/), MySQL-compatible edition* [Amazon RDS for MariaDB](https://aws.amazon.com/rds/mariadb/)* [Amazon RDS for MySQL](https://aws.amazon.com/rds/mysql/)Azure Database on Microsoft Azure:* [Azure Database for MariaDB](https://azure.microsoft.com/en-us/services/mariadb/)* [Azure Database for MySQL](https://azure.microsoft.com/en-us/services/mysql/MySQL)Cloud SQL on Google Cloud:* [Cloud SQL for MySQL](https://cloud.google.com/sql/mysql)## Configure a catalogTo create a MySQL catalog, select **Catalogs** in the main navigation and click**Configure a catalog**. Click on the **MySQL** button in the **Select a datasource** screen.{% include_relative cloud-provider.md %}{% include_relative name-description.md %}{% include_relative read-only.md %}{% include_relative configure-db-connection.md %}{% include_relative amazon-rds.md %}{% include_relative azure-database.md %}{% include_relative cloud-sql.md %}{% include_relative test-connection.md %}{% include_relative save.md %}"
 },
 {
  "title": "ODBC driver",
  "url": "/data-consumer/clients/odbc.html",
  "content": "# {{ page.title }}The Open Database Connectivity (ODBC) driver enables anyapplication supporting an ODBC driver to connect to {{site.terms.sb}} clusters.The client application can then issue SQL queries to a {{site.terms.sep_first}}cluster and receive results.For more detailed information, refer to the [complete ODBC driverdocumentation](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/Starburst+ODBC+Driver+Install+and+Configuration+Guide.pdf)## InstallationTo install the ODBC driver, [download the latest driver](#download-the-driver)for your operating system version. Typically you have to install the driver andadd it to your client application. Refer to your client&#39;s documentation for moredetails.Some clients may require an older version of the ODBC driver than the downloadsavailable on this page. See [Driver versionrequirements](#driver-version-requirements) for more information.### Download the driverThe latest version of the Starburst ODBC driver (v{{site.terms.odbc_latest}}) isavailable for the following operating system versions and driver managers. Olderversions of the driver may be subject to different platform requirements.#### WindowsThe Starburst ODBC driver supports the following operating system versions:* Windows 10, 8.1, or 7 SP1* Windows Server 2016, 2012, or 2008 R2 SP1Download the latest Starburst ODBC driver for Windows:* [Starburst ODBC 32-bit  .msi](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC32bit-{{site.terms.odbc_latest}}.msi)* [Starburst ODBC 64-bit  .msi](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC64bit-{{site.terms.odbc_latest}}.msi)The ODBC driver can be managed on Windows with Windows ODBC Administrator.#### MacOSThe Starburst ODBC driver supports the following operating system versions:* MacOS version 10.15, 10.14, or 10.13Download the latest Starburst ODBC driver for macOS:* [Starburst ODBC  .dmg](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC.{{site.terms.odbc_latest}}.dmg)The ODBC driver can be managed on macOS with the following driver managers:* iODBC 3.52.9 or later* unixODBC 2.2.14 or later#### LinuxThe Starburst ODBC driver supports the following operating system versions:* Red Hat® Enterprise Linux® (RHEL) 7 or 6* CentOS 7 or 6* SUSE Linux Enterprise Server (SLES) 12 or 11* Debian 9 or 8* Ubuntu 18.04, 16.04, or 14.04Download the latest Starburst ODBC driver for Linux:* [Starburst ODBC RHEL i686/32-bit  .rpm](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC-32bit-{{site.terms.odbc_latest}}.el6.i686.rpm)* [Starburst ODBC RHEL x86/64-bit  .rpm](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC-64bit-{{site.terms.odbc_latest}}.el6.x86_64.rpm)* [Starburst ODBC SUSE 12 i686/32-bit  .rpm](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC-32bit-{{site.terms.odbc_latest}}.suse12.i686.rpm)* [Starburst ODBC SUSE 12 x86/64-bit  .rpm](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC-64bit-{{site.terms.odbc_latest}}.suse12.x86_64.rpm)* [Starburst ODBC Debian 32-bit  .deb](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/starburstodbc-32bit-_{{site.terms.odbc_latest}}.deb)* [Starburst ODBC Debian 64-bit  .deb](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/starburstodbc-64bit-_{{site.terms.odbc_latest}}.deb)* [Starburst ODBC 32-bit  .tar.gz](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC-32bit-{{site.terms.odbc_latest}}.tar.gz)* [Starburst ODBC 64-bit  .tar.gz](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/{{site.terms.odbc_latest}}/StarburstODBC-64bit-{{site.terms.odbc_latest}}.tar.gz)The ODBC driver can be managed on Linux with the following driver managers:* iODBC 3.52.9 or later* unixODBC 2.2.14 or later### Driver version requirementsFor most clients, download the newest version of the {{site.terms.sb}} ODBCdriver. Some ODBC-based clients may require earlier versions of the ODBC driver.Contact {{site.terms.support}} to request specific driver versions.Note that version 1.2.16 and earlier of the ODBC driver require a license file,which you can also request from {{site.terms.support}}. Install this licensefile in the ``lib`` directory of the ODBC driver installation.## Connect to a clusterWith the ODBC driver installed, you can now configure a connection to a cluster.Typically you only need the[connection information](./index.html#basic-connection-information).Other details such as the ODBC connection are automatically configured in manyapplications, or can be seen and updated in a user interface dialog.## Start queryingYou can now open a connection and start running queries.For example, if your application allows you to write and run queries you canrun the following command to see what catalogs are available:```sqlSHOW CATALOGS;```Many client applications provide a user interface to see the list of catalogs,schema, tables, and much more.## Next stepsNow that you can connect to {{site.terms.sep}} with a client application,learn more about {{site.terms.sb}}&#39;s [SQL support](../starburst-sql.html)."
 },
 {
  "title": "Red Hat OpenShift deployment guide",
  "url": "/ecosystems/redhat/openshift-deployment.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} is available as an operator on OpenShift directlythrough the Red Hat Marketplace.## PrerequisitesBefore you get started, here are some things you need:* Access to an OpenShift cluster with with [correctly-sized  nodes]({{site.sep_rhm_url}}k8s/requirements.html#k8s-cluster-requirements),  using IAM credentials, and with sufficient Elastic IPs* Previously installed and configured Kubernetes, including access to  `kubectl`* An editor suitable for editing YAML files* Your {{site.terms.sep}} license fileBefore you get started installing {{site.terms.sep}}, we suggest that you readour [reference documentation](../../latest/k8s/overview.html) and our helpful[customization guide](../../latest/k8s/sep-configuration.html).## Quick startAfter you have signed up through RHM, download the latest OpenShift ContainerPlatform (OCP) client for your platform from [the OpenShift mirrorsite](https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/), andcopy the `oc` executable into your path, usually `/usr/local/bin`. Once thisis done, you are ready to install the operator in OCP4.Using your administrator login for Red Hat OCP, log in to the OCP web consoleand click **Operators &gt; OperatorHub** in the left-hand menu.Once there, select &quot;Starburst Enterprise&quot; from the **Project:** drop-down menu, and navigatethrough the projects to **All Items &gt; Big Data &gt; {{site.terms.sb}}** until yousee **{{site.terms.sep_full}}**. Click on that tile, then click the **Install**button.When the **Create Operator Subscription** page appears, select the**{{site.terms.sb}}** project as the specific namespace on the cluster, leaveall other options as default, and click **Subscribe**. When the operation is complete, you are subscribed to the {{site.terms.sep}}operator, and it is installed and accessible to you in OCP4.## Getting up and running### InstallationOnce you have your operator subscription in place, it&#39;s time to install. Thereare several steps to getting {{site.terms.sep}} installed and deployed:* Installing the {{site.terms.sep}} cluster* Installing the Hive Metastore Service (HMS)You must install the HMS to connect and query any objects storage with the Hiveconnector. This is typically a core use case for {{site.terms.sep}}, and then arequired step. The HMS is used by {{site.terms.sep}} to manage the metadata ofany objects storage.Follow these steps to **Install Starburst Enterprise Operator** via Red HatMarketplace:1. On the main menu, click **Workspace** &gt; **My Software** &gt; **Product** &gt;  **Install Operator**2. On the **Update Channel** section, select an option, &#39;stable&#39; or  &#39;alpha&#39;3. On the **ApprovalStrategy** section, select either **Automatic** or  **Manual**, the approval strategy corresponds to how you want to process  operator upgrades)4. On the **TargetClustersection**:  * Click the checkbox next to the clusters where you want to install the    **Starburst Enterprise Operator**  * For each cluster you selected, under** Namespace Scope**, on the **Select    Scope** list, select an option5. Click **Install**,  it may take several minutes for installation to completeOnce installation is complete, the status changes from *Installing* to *Up to date*.### ConfigurationWhen the operator installation is complete, you can proceed to deploy two customresources:* [Starburst Enterprise](./samples/starburst-enterprise-values.yaml)* [Starburst Hive](./samples/starburst-hive.yaml)Just like with installation, there are several steps to configuring{{site.terms.sep_full}}:* [Configuring SEP]({{site.sep_rhm_url}}k8s/sep-configuration.html)* [Configuring the Hive metastore]({{site.sep_rhm_url}}k8s/hms-configuration.html)Each of these steps uses a specific Helm chart `values.yaml` configuration.Click on the links for detailed instructions on configuring each of the customresources.The following setup steps are required:* Configure the resource requirements based on your cluster and node sizes* Update the image repository and tags to use the RedHat registry:  * Starburst Enterprise: `registry.connect.redhat.com/starburst/starburst-enterprise:354-e-ubi`  * Starburst Enterprise Init: `registry.connect.redhat.com/starburst/starburst-enterprise-init:354.0.0-ubi`  * HMS: `registry.connect.redhat.com/starburst/hive:354.0.0-ubi`## Next stepsYour cluster is now operational! You can now connect to it with your [clienttools](../../data-consumer/clients/index.html), and start querying your datasources.Follow these steps to quickly test your deployed cluster:1. Create a route to the default &#39;starburst&#39; service. If you changed the name in  the `expose` section, use the new name.2. Run the following command using the [CLI](../../data-consumer/) with the  configured route:  ```  trino --server  --catalog tpch  ```3. Run `SHOW SCHEMAS;` in the CLI, and you can see a list of schemas  available to query with names such as `tiny`, `sf1`, `sf100`, and others.We&#39;ve created an [operations guide]({{site.sep_rhm_url}}k8s/operation.html) toget you started with common first steps in cluster operations.It includes some great advice about starting with a small, initial configurationthat is built upon in our [cluster sizing and performance videotraining](../../videos/2020-09-09-cluster-sizing-and-performance-video.html).## Troubleshooting{{site.terms.sep}} is powerful, enterprise-grade software with many movingparts. As such, if you find you need help troubleshooting, here are some helpfulresources:* [LDAP authentication]({{site.sep_rhm_url}}security/ldap.html?#troubleshooting)* [Data consumer guide with clients, SQL and other tips](../../data-consumer/index.html)## FAQ**Q: Once it&#39;s deployed, how do I access my cluster?**A: You can use the [CLI on aterminal](../../data-consumer/clients/cli.html) or the [WebUI]({{site.sep_rhm_url}}admin/web-interface.html) to access your cluster. Forexample:* {{site.terms.oss}} CLI command: `./trino --server  example-starburst-enterprise.apps.demo.rht-sbu.io --catalog hive`* Web UI URL: `http://example-starburst-enterprise.apps.demo.rht-sbu.io`* [Many other client applications](../../data-consumer/clients/index.html) can  be connected, and used to run queries, created dashboards and more.**Q: I need to make administrative changes that require a shell prompt. How do Iget a command line shell prompt in a container within my cluster?**A: On OCP, you&#39;ll get a shell prompt for a pod. To get a shellprompt for a pod, you&#39;ll need the name of the pod you want to work from. To doso, log in to your cluster as per your RHM documentation. For example:```shelloc login -u kubeadmin -p XXXXX-XXXXX-XXXXX-XXXX https://api.demo.rht-sbu.io:6443```Get the list of running pods:```shell❯ oc get pod -o wideNAME                                                 READY   STATUS    RESTARTS   AGE   IP            NODE                                         NOMINATED NODE   READINESS GATEShive-XXXXXXXXX-lhj7l        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal               starburst-enterprise-coordinator-example-XXXXXXXXX-4bzrv   1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal                starburst-enterprise-operator-7c4ff6dd8f-2xxrr                     1/1     Running   0          41m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal               starburst-enterprise-worker-example-XXXXXXXXX-522j8        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal               starburst-enterprise-worker-example-XXXXXXXXX-kwxhr        1/1     Running   0          27m   10.130.2.XX   ip-10-0-162-XXX.us-west-2.compute.internal              starburst-enterprise-worker-example-XXXXXXXXX-phlqq        1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal                ```The `pod name` is the first value in a record. Use the `pod name` to open ashell:```shell❯ oc rsh starburst-enterprise-coordinator-example-XXXXXXXXX-4bzrv```A shell prompt will appear. For example, on OCP 4.4:```shellsh-4.4$```**Q: Is there a way to get a shell prompt through the OCP web console?**A: Yes. Log in to your OCP web console and navigate to**Workloads &gt; Pods**. Select the pod you want a terminal for, and click the**Terminal** tab.**Q: I&#39;ve added a new data source. How do I update the configuration torecognize it?**A: Using the [making configurationchanges](#configuration) section to edit your YAML configuration,find `additionalCatalogs`, and add an entry for your new data source. Forexample, to add a PostgreSQL data source called `mydatabase`:```yaml    mydatabase: |      connector.name=postgresql      connection-url=jdbc:postgresql://172.30.XX.64:5432/pgbench      connection-user=pgbench      connection-password=postgres123```Once your changes are complete, click `Save` and then `Reload` to deployyour changes. Note that this restarts the coordinator and all workers on thecluster, and might take a little while."
 },
 {
  "title": "Starburst Admin operation guides",
  "url": "/starburst-enterprise/starburst-admin/operation-guides.html",
  "content": "# {{ page.title }}After satisfying the [initial requirements](./index.html#requirements) for thecontrol node and all the managed nodes in the cluster, you can get a cluster upand running with the help of the[installation guide](./installation-guide.html).A fully configured deployment requires a bit more configuration and work. It istypically performed incrementally and you use {{site.terms.sbadmin}} to help youfor a number of scenarios. They range from simple tasks like changing aconfiguration in a catalog file to more complex ones such as adding a newcatalog file, scaling the cluster up or down, or adding security configuration.The following sections includes these and many other scenarios and refers to the[playbook reference](./playbook-reference.html) and other sections as necessary.## Targeting specific hostsBy default, Ansible runs the tasks defined in the playbooks on all cluster nodesdefined in the inventory `hosts` file.You can target specific host groups or even specific hosts with the `-l` option.The example hosts files uses the groups `coordinator` and `worker` to target thegroups. Individual hosts are targeted with the IP address.**Example: Roll out configuration changes to workers only*** Update the relevant configuration files in `files/worker`, for example changed  JVM configuration.* Push the configuration only to the workers  ```  ansible-playbook -l worker playbooks/push-configs.yml  ```* Restart only the workers  ```  ansible-playbook -l worker playbooks/restart.yml  ```**Example: Update security configuration on the coordinator*** Update the relevant configuration files in `files/coordinator`, for example  updated certificate files.* Push the configuration only to the coordinator  ```  ansible-playbook -l coordinator playbooks/push-configs.yml  ```* Restart only the coordinator  ```  ansible-playbook -l coordinator playbooks/restart.yml  ```**Example: Restart or remove a misbehaving worker only*** Determine the worker that has problem, for example from [collecting the  logs](./playbook-reference.html#collect-logs) or [checking the service  status](./playbook-reference.html#check-status)* Attempt a restart of the one worker that seems to have problems  ```  ansible-playbook -l 172.28.0.3 playbooks/restart.yml  ```* Check status after restart  ```  ansible-playbook -l 172.28.0.3 playbooks/check-status.yml  ```* Check the logs after restart  ```  ansible-playbook -l 172.28.0.3 playbooks/collect-logs.yml  ```## Adding, removing and updating catalogsYou can add a new catalog, update an existing catalog or even remove a catalogand roll that change out across the cluster with the following steps:* Perform the desired changes in the `files/catalog` directory.* [Stop the cluster](./playbook-reference.html#stop).* [Push the configuration](./playbook-reference.html#push-configs).* [Start the cluster](./playbook-reference.html#start).A full cluster restart of the coordinator and all workers is necessary to applythe updated configuration.## Changing other configuration* Perform the desired changes in the `files` subdirectories and files.* [Stop the cluster](./playbook-reference.html#stop) or selectively the  coordinator or all workers.* [Push the configuration](./playbook-reference.html#push-configs).* [Start the cluster](./playbook-reference.html#start) or selectively the  coordinator or all workers..The safest option is to restart the complete cluster. Depending on theconfiguration changes it can be possible to just restart all the workers or thecoordinator only. For example, for authentication configuration to{{site.terms.sep}} a coordinator restart can be sufficient.## Upgrading {{site.terms.sep_full }}Upgrading SEP using Ansible is similar to initial installation. The process hasto be performed without any users running queries.Use the following steps to perform the update:* Download the binary package for the new version and place it in `files`.* Alternatively place the binary package on a server and update the installer URL.* Update the version number in `vars.yml`.* Update all the configuration and add any new configuration files.* [Stop the cluster](./playbook-reference.html#stop).* [Install the new packages to the cluster](./playbook-reference.html#install).* [Push the configuration](./playbook-reference.html#push-configs).* [Start the cluster](./playbook-reference.html#start).```ansible-playbook playbooks/stop.ymlansible-playbook playbooks/install.ymlansible-playbook playbooks/push-configs.ymlansible-playbook playbooks/start.yml```To rollback the upgrade, revert any configuration changes, change the valueof `version` to the previous one, and execute the same playbooks above.Note that because all nodes in the cluster must use the same version,it is not possible to perform a rolling restart and avoid interruptions.To minimize down time, we recommend installing the new version on a setof new hosts, and reconfigure clients to connect to this cluster or updateDNS entries to point to it. Old cluster can be decommissioned after all queriesrunning on it are done.To manage separate installations on more than one sets of hosts, copythe `hosts` file and update values inside it. Then, to use the new file,add the `-i ` parameter when running the `ansible-playbook` command.### Blue/Green deployments and upgradesWith sufficient resources you can significantly improve your upgrade process bymanaging two productions cluster behind a load balancer (LB) with a definedfully qualified domain name (FQDN). The upgrade process can then follow ablue-green deployment process:* Current production cluster in use is *green* and LB uses the FQDN to point  users to it.* Update all configuration and resources for the inactive *blue* cluster to the  new version and configuration.* Install, push configuration and start the *blue* cluster.* Test the *blue* cluster with the direct IP address or an alternative FQDN  configured on the LB.* Switch the LB configuration to point the *blue* cluster.* Shut down the *green* cluster.For the next upgrade the process is the identical, just the role of the twoclusters is reversed.The inactive cluster can be kept running for failover and high availabilityusage or decommissioned and recreated again for future upgrades.## Adding or removing workersYou can use {{site.terms.sbadmin}} to help with scaling your cluster up anddown by adding or removing workers with the following steps.**Scaling up*** Provision the new machine with [necessary requirements for cluster  nodes](./index.html#requirements).* Add the IP address and access details to the `worker` section in the `hosts`  file.* Install on the new node* Push configuration to the new node* Start the new node* Check the log of the new nodeFor example, if you add the new host at `172.28.0.5` as worker you can run theinstallation just to this node:```[coordinator]172.28.0.2 ansible_user=root ansible_password=changeme[worker]172.28.0.3 ansible_user=root ansible_password=changeme172.28.0.4 ansible_user=root ansible_password=changeme172.28.0.5 ansible_user=root ansible_password=changeme```Playbook invocations:```ansible-playbook -l 172.28.0.5 playbooks/install.ymlansible-playbook -l 172.28.0.5 playbooks/push-configs.ymlansible-playbook -l 172.28.0.5 playbooks/start.yml```Specifying the host is optional and can be omitted since the playbooks can runwithout side effects if the desired state is already reached. As a result youcan add a number of workers and then just install, push configuration and startthem all together.**Scaling down*** Stop the worker or perform a graceful shutdown.  ```  # Hard stop  ansible-playbook -l 172.28.0.5 playbooks/stop.yml  # Graceful shutdown  ansible-playbook -l 172.28.0.5 playbooks/graceful-shutdown.yml  ```* Optionally run [uninstall](#uninstall) for the specific worker only  ```  ansible-playbook -l 172.28.0.5 playbooks/uninstall.yml  ```* Remove the worker from `hosts`## Configuring TLSYou can configure [TLS for the coordinator](../../latest/security/tls.html) aswell as [cluster internal authentication andTLS](../../latest/security/internal-communication.html) as usual.For the functionality of the playbooks you need to additionally update the portsconfigured in `vars.yml` so that they can continue to interoperate with thenodes for status checks and other aspects.Using 8443 on the coordinator and workers:```coordinator_port: 8443worker_port: 8443```## Using secretsYou can use [secrets in environmentvariables](../../latest/security/secrets.html) to avoid clear text password andother sensitive data in the configuration files.To inject a secret value into an environment variable, you can modify the`env.sh` shell scripts in `files/worker` and `files/coordinator`. The script isexecuted before {{site.terms.sep}} is started. For example, you can insert codeto retrieve secret values from a secret manager such as [Hashicorp Vault](https://www.hashicorp.com/products/vault),[Keywhiz](https://github.com/square/keywhiz), or a secret manager from yourcloud provider."
 },
 {
  "title": "Tuning your cluster performance",
  "url": "/platform-administrator/performance-tuning.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a more feature-rich version of{{site.terms.oss_first}} providing enhanced query performance, security,connectivity, and ease of use.Learn how to size your cluster and the machines in it to ensure the bestperformance possible for your workload in this training video presented by oneof our founders, Dain Sundstrom. For your convenience, we&#39;ve divided the videotraining course up into topic sections, and provided links to the relevant partsof our documentation below.## General tuning strategy &amp; baseline advice            {% include youtubeSnippet.html id=page.youtubeId1 start=346 end=896 %}              Topics:              Starting big        Stabilizing, then tuning        Options to disable            Running time: ~9 min.      ## Cluster sizing, and how {{site.terms.sep}} uses CPU and memory resources                 {% include youtubeSnippet.html id=page.youtubeId1 start=897 end=2056 %}              Topics:              How memory affects JOIN, GROUP BY, ORDER BY and window functions        Availability        Concurrency            Running time: ~19 min.      ## Machine sizing and its impact             {% include youtubeSnippet.html id=page.youtubeId1 start=2086 end=4393 %}              Topics:              Memory and memory allocation        Shared join hash        Distributed join        Skew        Machine sizes and types        Spilling        Small clusters            Running time: ~38 min.      Additional resources on memory management and spilling in {{site.terms.sep}}:* [Memory management properties](../latest/admin/properties-memory-management.html?highlight=memory%20allocation)* [Spilling properties](../latest/admin/properties-spilling.html)* [Spill to disk](../latest/admin/spill.html)* [JVM Settings](../latest/admin/tuning.html)## Tuning the workload            {% include youtubeSnippet.html id=page.youtubeId1 start=5025 end=5965 %}              Topics:              Query plan        Precomputing        Connectors            Running time: ~16 min.      ## Hive data organization             {% include youtubeSnippet.html id=page.youtubeId1 start=5981 end=6936 %}              Topics:              Organize your data for the Hive connector        Hive partitioning and bucketing        ORC and Parquet        File size        Bad parquet files        Rewrite table with the ORC writer            Running time: ~16 min.      ## Making queries faster             {% include youtubeSnippet.html id=page.youtubeId1 start=6937 end=7713 %}              Topics:              What to look for in a query        Using more hardware        Underutilization        Hive caching            Running time: ~13 min.      For more in-depth information on this topic, watch our [query optimizationtraining video](../../data-engineer/query-performance.html).## Sharing resources, and resource groups             {% include youtubeSnippet.html id=page.youtubeId1 start=7714 end=7899 %}              Topics:              Concurrency        User experience, expectations and satisfaction        Social engineering            Running time: ~3 min.      "
 },
 {
  "title": "Personas used for documentation and design",
  "url": "/internal/personas.html",
  "content": "# {{ page.title }}There are three primary audiences we write and design for:- Starburst platform administrators- Data engineers- Data consumers: analysts &amp; scientistsThere is a single, secondary audience: &quot;Data leaders.&quot; This persona is largely acheck writer for our purposes (VP, CDO, CIO), and does not actually use SEP anydifferently than the primary personas. They are included here for completeness,and should be kept in mind when creating marketing content such as case studies,white papers and ROI-focused materials.This document describes these audiences as personas - fictional amalgamations -that you can empathize with and solve problems on behalf of. In practice,product personas are given names, faces and backgrounds to aid in discussingtheir needs as if they were real people, representative of our customers.## Primary personas### Data consumers - analysts and scientistsData analysts and scientists will approach Starburst in very similar ways.However, their backgrounds and skill sets are different, so we will separatethose out. Their pain points will be treated together.              Chris Consumer    Early career    BSc in physics from University of Toledo, currently working on online MBA    Chris is a Business Analyst. He&#39;s responsible for delivering    visualizations and reports to ensure that his leadership is making    well-informed, data-driven decisions. Chris cares very deeply that not only    are the right questions being asked (and answered), but that the right    data is being used to answer the questions. With the wealth of data    available, it can be easy to overlook and misuse data. The quality of    Chris&#39;s work ultimately rests on the quality and reliability of the data he    uses, so Chris keeps good working relationships with his data engineering    team and often communicates discrepancies and SLAs issues to them. Chris has    some solid SQL chops and is often able to prototype a new data source to be    productionalized by data engineers. Chris feels that he has just the right    combination of technical skills and business acumen.  When you write, design and build for Chris, here are some of the skillsets you can expect him to have:* A reasonable level of skill with SQL, with some knowledge of more advanced  queries* Limited programming skills and methodologies* Tells stories with data* Expert with data visualization tools* Excellent spreadsheet skills, including some modeling* A good ability to detect and articulate issues with data, even if he cannot  remedy them or trace the cause              Cameron Consumer    Early career    PhD in statistics, Stanford    Cameron is a data scientist. She&#39;s responsible for creating data models    and that forecast and describe the business. Cameron worries about the impact    of seasonality on sales, and feels compelled to deliver models that    reflect that impact with a high degree of accuracy. Cameron feels like she    brings the answers to &quot;Why?&quot; and &quot;How?&quot; to the table. Her machine learning    models help her find the levers that the business can pull - the &quot;how,&quot;    and her models account for why the business behaved as it did, or will.    She feels more like an academic than an engineer, and is very proud of her    scientific approach to business. Her digital sales data knowledge is    formidable, and her reputation as an SME ensures that she has a robust    stream of opportunities in her field.  When you write, design and build for Cameron, here are some of the skillsets you can expect her to have:* A reasonable level of SQL skills, with some knowledge of more advanced queries* Reasonable programming skills* Expert in statistical methods and/or machine learning* Some understanding of code repositories* Competence with data visualization tools* A good ability to detect and articulate issues with data, even if they cannot  remedy them or trace the causeCameron&#39;s and Chris&#39;s pain points include, in no particular order:* Having to retrofit tools onto multiple data sources* Long waits for ETL to deliver useable data* Can&#39;t dive into data quality issues* Data engineers sometimes kill their queries because of resource contention* Complex, periodic reports and models are often delayed past due dates### Data engineers                Donna Data Engineer      Mid-career      BSc in computer engineering, University of Illinois at Chicago      Donna Data Engineer is responsible for designing performant data      sources that can answer a broad range of business questions at XYZ, Inc.      Donna found her way to data engineering through internships in college; it      felt like a good blend between the technical chops required for      programming jobs, and the big picture, organizational nature of data that      she is naturally drawn to. As part of her job, she must understand what      data is currently available from what sources, and what new data is needed      to fill in any gaps. Donna has to work with stakeholders to source that      new data, be it from third parties or through new log entries, message      streams or product endpoints. Donna works pretty closely with data      analysts and scientists, and tries to anticipate their needs in order to      keep up with burgeoning data demands.                  Daniel Data Engineer      Mid-career      BSc in computer science, University of New Hampshire      Daniel Data Engineer is responsible for delivering data to data      analysts and data scientists at Acme Corp. Up until a few years ago,      this mostly entailed writing complex ETL in frameworks such as Informatica      and Alteryx. Over the last few years, he&#39;s worked mostly in python-based      frameworks such as Airflow and Bonobo as well as diving into Apache Spark.      Daniel really cares about data landing times, because him and his      coworkers hear from PagerDuty way more than they would like to.   When you write, design and build for Donna and Daniel, here are some of theskill sets you can expect them to have:* Creating and monitoring pipeline health metrics to ensure SLAs are met.* Enabling automated self-service pipelines using Infrastructure as Code (IaC)* Design schemas, data lake and data warehouse solutions in collaboration with  stakeholders.* Building and managing Kafka-based streaming data pipelines* Building and managing Airflow- and Spark-based ETLs* Creating and updating data models &amp; data schemas that reduce system complexity  and cost, and increase efficiency* Preparing and cleaning data for prescriptive and predictive modeling and  descriptive analytics* Identifying, designing, and implementing internal process improvements such as  automating manual processes, optimizing data delivery for greater scalability* Creating data tools for analysts and data scientists* Building data integrations between various 3rd party systems such as Salesforce  and WorkdayDonna&#39;s and Daniel&#39;s pain points, in no particular order:* Keeping up with the changing landscape of data delivery technology* Managing SLAs for data pipelines in environments where the data growth rate  and complexity constantly increases, data pipeline and platform performance* Aligning and negotiating with upstream data sources and infrastructure SLA  owners* Sussing out detailed data requirements from folks with a wide range of data  knowledge* Long, brittle pipelines* Productionalizing non-performant analyst queries* Constantly responding to resource constraint issues* Designing ETL around siloed data* Data cleansing### Platform administrators              Art Administrator    Late career    BSc in computer science, BYU    Art Administrator is responsible for XYZ, Inc&#39;s Starburst cluster. He was    an SRE for the data team for years, and switched roles to platform    engineering after leading the SREs for a bit. Art really cares about    scalability and reliability, especially since XYZ has super aggressive    SLAs both on data landing times and of course availability. Art works    closely with his colleagues in IT to ensure that his systems adhere to    XYZ&#39;s strict access policies and support audit requirements.                   Ada Administrator      Late career      BSc in computer science, University of Washington      Ada Administrator is responsible for both Acme Corp&#39;s Starburst and      Postgres clusters. Ada was a DBA from early to mid-career, and it fell to      her at Acme to figure out the HDFS ecosystem when it came along. Now she      builds and maintains big data clusters for a living. Ada cares a lot      about the using right data platform for the data.  When you write, design and build for Art and Ada, here are some of the skillsets you can expect them to have:* Building and maintaining scalable data platform architectures to support the  ingest, storage and querying of large heterogenous datasets* Creating and monitoring cluster health metrics to ensure optimal performance  and reduce any downtime* Writing clean, production-ready code (in Java, Go etc.) with a strong focus on  quality, scalability and high performance* Using and building scalable asynchronous REST API’s* Working with cloud providers like AWS, Azure and Google Cloud* Implementing and working with persistence technologies like AWS S3, HDFS,  Kafka and ElasticSearch* Designing for data integrity and security through all environments as well as  the data lifecycle* Partnering with data engineers to enable automated self-service pipelines  using Infrastructure as Code (IaC)* Partnering with data engineers to design and improvement schemas, data lake  and data warehouse solutions in collaboration with stakeholdersArt&#39;s &amp; Ada&#39;s pain points, in no particular order:* Sorting through an overload of information to master complex data platforms* Ensuring data platforms can scale to demand and with growth* Architecting solutions that can provide disaster recovery and business  continuity for complex, critical data systems, in conjunction with IT  stakeholders* Assisting in managing budgets and licensing cycles for massive  enterprise-scale software vendors, bandwidth and hardware leases* Constantly tackling inherently complex and highly-visible tasks* Delivering against stringent infrastructure SLAs* Doing more with less, or at least the same team size* Implementing data governance requirements for all data systems## Secondary persona - data leader              Lauren Leader    Mid-to-late career    MBA, Haas School of Business    Lauren is CIO at the newly IPO&#39;ed Clouds &#39;R Us. She&#39;s responsible for    data infrastructure, data governance and delivery, as well as    enabling SOX, GDPR and CCPA compliance. Prior to stepping into her current    role, Lauren was a VP of IT at Acme Corp., where she owned the budget for    all data infrastructure. She calls this her &quot;real-life MBA,&quot; because she    learned the hard way from being caught off-guard by explosive growth in    under-specified legacy systems in multiple budget cycles. Lauren is also    sensitive to scaling, platform lock-in, and staffing around particular    technologies.  When you write for Lauren, here are some of her pain points to keep in mind, inno particular order:* Constantly fighting Shadow IT, up to and including small, narrow-scope  one-off data warehouse solutions which she inevitably must absorb* Architecting around legacy systems, particularly monolithic services* Changing regulatory climate* Staffing for innovation while keeping legacy systems running and trying to  automate* Managing, defending and demanding a budget with rapid growth* Balancing buy-vs-build, including for contracting services* Balancing private cloud vs hosted cloud solutions for cost-effectiveness,  regulatory compliance and security"
 },
 {
  "title": "Starburst Admin playbook reference",
  "url": "/starburst-enterprise/starburst-admin/playbook-reference.html",
  "content": "# {{ page.title }}{{site.terms.sbadmin}} includes numerous playbooks to perform specific actions.You can use them individually or in sequence to satisfy the needs of yourspecific use case for [installing](./installation-guide.html) and[operating](./operation-guides.html).The following playbooks are available:* [Install cluster](#install)* [Push configurations](#push-configs)* [Start cluster](#start)* [Stop cluster](#stop)* [Restart cluster](#restart)* [Check service status](#check-status)* [Graceful worker shutdown](#graceful-shutdown)* [Rolling worker restart](#rolling-restart-workers)* [Collect logs](#collect-logs)* [Uninstall cluster](#uninstall)## Install clusterThe `install` playbook installs the RPM or tarball package on all defined hosts.You are required to place the `tar.gz` or `rpm` file in the `files` directoryand specify the `version` in `vars.yml` to run the playbook:```shellansible-playbook playbooks/install.yml```Errors occurs if RPM and tarball are found or if version values mismatch.Alternatively to the archive in the `files` directory on the control machine,you can set the `installer_url` in `vars.yml` to point to HTTP or HTTPS URL ofthe  `tar.gz` or `rpm`. Comment out the `installer_file` in `vars.yml`. Ansiblethen downloads the binary from the URL directly on the hosts. This approach ismore complex to set up, but scales better since all hosts can download from theURL in parallel. The hosts need to be able to contact the specified URL.The playbook verifies the availability of the required Java runtime on the host,starting first at the value of `JAVA_HOME` and then looking at commoninstallation paths. It uses the script `files/find_java.sh` and fails if noruntime is found.The playbook installs the RPM or unpack the tarball and create the necessarydirectories specified via `vars.yml`.Ansible can operate as a non-root user, but for some operations, it requiresroot privileges. By default, `sudo` commands are used to elevate privileges.Since `sudo` prompts for the user password, run every `ansible-playbook` commandwith the `--ask-become-pass` parameter as detailed in the [become commanddocumentation](https://docs.ansible.com/ansible/latest/user_guide/become.html#become-command-line-options).The RPM installation automatically adds a dedicated system user to run{{site.terms.sep}} as a service. This user owns all configuration files, whichshould not be readable by other users on the hosts. The tarball installationuses the `installation_owner` and `installation_group` defined in `vars.yml`.The install playbook create the user and group.##  Push configurationsThe `push-configs` playbook generates the configuration files for thecoordinator and workers and distributes them to all hosts.The following resources are used to create the set of configuration files:* `files/vars.yml` - variable definitions to use in the configuration files that  are templatized with Jinja2. These files use the extension `.j2`. Ansible uses  the set values and replaces the variable placeholders. For example, the  placeholder `{{ node_environment }}` is replaced with the value `production`  from  `node_environment: production`.* `files/coordinator` - the configuration files for the coordinator.* `files/worker` - the configuration files for all the workers.* `files/catalog` - the [catalog properties files to define the connection to  any data sources](../../data-engineer/catalogs.html), required on the coordinator  and all workers.* `files/extra/etc` - additional files that are placed in the directory  specified by the `etc_directory` variable on all hosts, including the [license  file](../../latest/installation/license-requirements.html) as well as other  configuration files.* `files/extra/lib` - additional files that are placed in the `lib` directory  all hosts, typically binary files or configuration files that are added to the  servers classpath such as custom user-defined function implementations.* `files/extra/plugin` - additional files that are placed in the `plugin`  directory all hosts, typically complete directories with binaries used for a  custom-developed plugin such as a security extension or another connector.Any changes, including deletion of catalog files is synchronized to the hosts.{{site.terms.sbadmin}} automatically uses the folder name `starburst` for{{site.terms.sep}} deployments and uses `trino` for {{site.terms.oss}}deployments.Other file deletions are not synchronized but can be performed with the [Ansiblefilemodule](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/file_module.html).For example, if you made a file `files/extra/etc/foo.sh`, the file is copiedinto `/etc/starburst/foo.sh` on the hosts. You can use the followingcommand to delete it:```ansible all -m file -a &quot;path=/etc/starburst/foo.sh state=absent&quot;```The supported files and configuration methods are [identical to the generalconfiguration files and properties from SEP](../../latest/index.html) for theidentical version you deploy with {{site.terms.sbadmin}}.After you created or updated all the configuration as desired, you can run theplaybook with the following command:```shellansible-playbook playbooks/push-configs.yml```Keep in mind that most of the configuration files need to be distributed to allnodes in the cluster and that they need to be identical for all workers.The best approach to apply any new configuration involves the following steps:* Ensure no users are active and [stop](#stop) the cluster.* Alternatively [shut it down gracefully](#graceful-shutdown).* Update the configuration files.* Push the configuration.* [Start](#start) the cluster.* Verify the changes work.## Start clusterThe `start` playbook start the coordinator and all worker processes on thehosts. It starts {{site.terms.sep}} the `service` command for RPM-basedinstallations or with the `launcher` script for tarball installation.You need to [install the cluster](#install) and [push theconfiguration](#push-configs) before starting the cluster the first time.```ansible-playbook playbooks/start.yml```The playbook relies on the hosts to be up and running and available to Ansible.When restarting the hosts on the operating system or hardware level, theRPM-based installation automatically starts the {{site.terms.sep}} processes.The tarball installation does not start automatically. You can however configureit to perform the start by using the `launcher` script as daemon script. Referto the documentation of your used Linux distribution for details.##  Stop clusterThe `stop` playbook stops the coordinator and all worker processes on the hosts.It does not take into account if {{site.terms.sep}} actively processes anyworkload, and simply terminates.```shellansible-playbook playbooks/stop.yml```You can use the [graceful shutdown](#graceful-shutdown) as an alternative.##  Restart clusterThe `restart` playbook stops and then starts the coordinator and all workerprocesses on the hosts.```shellansible-playbook playbooks/restart.yml```It is equivalent to running the `stop` and the `start` playbook sequentially.##  Check service statusThe `check-status` playbook checks the status of the the coordinator and allworker processes and displays the results.```shellansible-playbook playbooks/check-status.yml```The playbook uses the `init.d` script for the RPM-based installation or the`launcher` script for a tarball installation to get the status of each service.If the service is running, you see a log for each address in your inventory filestating `Running as `:```TASK [Print status] ********ok: [172.28.0.2] =&gt; {    &quot;msg&quot;: &quot;Running as 1965&quot;}ok: [172.28.0.3] =&gt; {    &quot;msg&quot;: &quot;Running as 1901&quot;}ok: [172.28.0.4] =&gt; {    &quot;msg&quot;: &quot;Running as 1976&quot;}```If a service is not active, you see `Not running`:```TASK [Print status] ****ok: [172.28.0.2] =&gt; {    &quot;msg&quot;: &quot;Not running&quot;}ok: [172.28.0.3] =&gt; {    &quot;msg&quot;: &quot;Not running&quot;}ok: [172.28.0.4] =&gt; {    &quot;msg&quot;: &quot;Not running&quot;}```##  Graceful worker shutdownThe `graceful-shutdown` playbook stops the worker processes after all tasks arecompleted.```shellansible-playbook playbooks/graceful-shutdown.yml```Using [graceful shutdown](../../latest/admin/graceful-shutdown.html) takeslonger than using the `stop` playbook, because it allows workers to complete anyassigned work. If all workers are shut down, no further query processing isperformed by the cluster. The coordinator remains running at all times, untilmanually shut down.##  Rolling worker restartThe `rolling-restart-workers` playbook stops and starts all worker processessequentially one after the other using a [graceful shutdown](#graceful-shutdown)and a new start.```shellansible-playbook playbooks/rolling-restart-workers.yml```You can configure the following variables in `files/vars.yml` to managegraceful shutdowns:* `graceful_shutdown_user` - user name to pass to the `X-Presto-User` or  `X-Trino-User` header when issuing the graceful shutdown request via the HTTP  API* `graceful_shutdown_retries` -  number of times to check for successful  shutdown before failing* `graceful_shutdown_delay`- inactive duration between shutdown status checks* `rolling_restart_concurrency` - number of workers to restart at the same timeBy default, the playbook waits up to 10 minutes for any individual worker tostop and start. Each operation has a 10 minute timeout. If this timeout isreached, then the playbook execution fails. If you have a longer shutdowngrace period configured, you may want to extend this timeout.Keep in mind that this playbook does not change any configuration of theworkers. If you push configuration changes to the cluster before a rollingrestart, the cluster can be in an inconsistent state until the restart iscompleted. This can lead to query failures and other issues. A simple additionof a catalog file is possible. The new catalog only becomes usable after allworkers are restarted. Updates to catalog and other configuration filestypically result in problems.## Collect logsThe `collect-logs` playbook downloads the log files from all hosts to thecontrol machine.```shellansible-playbook playbooks/collect-logs.yml```The server, HTTP request, and launcher logs files from each host are copied into`logs/coordinator-` for the coordinator or `logs/worker-`for the workers in the current directory.## Uninstall clusterThe `uninstall` playbook removes all modifications made on the hosts by otherplaybooks. [Stop the cluster](#stop) before running the playbook.```shellansible-playbook playbooks/uninstall.yml```The playbook deletes all data, configuration and log files. It also removes thedeployed binary packages and the created user accounts and groups."
 },
 {
  "title": "PostgreSQL catalogs",
  "url": "/starburst-galaxy/catalogs/postgresql.html",
  "content": "# {{page.title}}You can use a PostgreSQL catalog to configure access to a[PostgresSQL](https://www.postgresql.org/) or compatible database in thefollowing deployments:Amazon RDS on Amazon Web Services:* [Amazon Aurora](https://aws.amazon.com/rds/aurora/), PostgreSQL-compatible  edition* [Amazon RDS for PostgreSQL](https://aws.amazon.com/rds/postgresql/)Azure Database on Microsoft Azure:* [Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/services/postgresql/)Cloud SQL on Google Cloud:* [Cloud SQL for PostgreSQL](https://cloud.google.com/sql/postgresql)## Configure a catalogTo create a PostgreSQL catalog, select **Catalogs** in the main navigation andclick **Configure a catalog**. Click on the **PostgreSQL** button in the**Select a data source** screen.{% include video.html content=&quot;See it all in action with the demo video forStarburst Galaxy.&quot; %}{% include_relative cloud-provider.md %}{% include_relative name-description.md %}{% include_relative read-only.md %}{% include_relative configure-db-connection.md %}{% include_relative amazon-rds.md %}{% include_relative azure-database.md %}{% include_relative cloud-sql.md %}{% include_relative test-connection.md %}{% include_relative save.md %}"
 },
 {
  "title": "Microsoft Power BI",
  "url": "/data-consumer/clients/powerbi.html",
  "content": "# {{ page.title }}You can use the popular analytics platform [Microsoft PowerBI](https://powerbi.microsoft.com/) to access clusters in {{site.terms.sep}} or{{site.terms.sg}} using Power BI DirectQuery.  {{site.terms.sep}} and {{site.terms.sg}} can be accessed via DirectQuery withthe following tools:* Microsoft Power BI Desktop* Microsoft Power BI ServicePower BI lets {{site.terms.sep}} and {{site.terms.sg}} perform all queryprocessing. This combines their scalability and processing power with thereporting features of Power BI.## Requirements* Power BI 2.87.720.0 and higher. Older versions of Power BI only support import  mode.* Starburst Presto OBDC driver version 1.2.16.1016  ([32-bit .msi](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/1.2.16.1016/Starburst+Presto+1.2+32-bit.msi),  [64-bit .msi](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/1.2.16.1016/Starburst+Presto+1.2+64-bit.msi))* License for the ODBC driver, which you can request from  {{site.terms.support}}. Install this license file in the `lib` directory of  the ODBC driver installation.## Data connectivity modesPower BI can access data from {{site.terms.sep}} or {{site.terms.sg}} in thefollowing connection modes:* **DirectQuery**: query data using the query engine without importing that data  to the Power BI client. This mode is recommended for most use cases as it has  the least impact on network or client storage resources, regardless of the  amount of data queried.* **Import**: import queried data directly to the client machine, for further  local analysis with tools like quick insights and calculated tables. Allows  for custom SQL which is not supported by DirectQuery. Note that  import mode is limited to 1GB of data per query.* **Standard ODBC**: use a generic ODBC connection. Allows for custom SQL which  is not supported by DirectQuery. Note that a standard ODBC connection is  limited to 1GB of data per query.## Connect with Power BI DesktopThe following sections describe how to connect to {{site.terms.sep}} or{{site.terms.sg}} from Power BI Desktop.### DirectQuery and import modeTo connect from Power BI Desktop using either DirectQuery or import mode, followthese steps:1. In Power BI Desktop, select **Get Data** &gt; **More**.2. Select **All** &gt; **Starburst Enterprise** and click **Connect**.   &amp;nbsp;{% include image.html   url=&#39;../../assets/img/general/power-bi-sb-source.png&#39;   img-id=&#39;Data source selection&#39;   alt-text=&#39;Selecting Starburst Enterprise as a data source&#39;   descr-text=&#39;Image showing Starburst Enterprise selected as a data source&#39;   pxwidth=&#39;500&#39;   screenshot=&#39;true&#39;   %}3. Configure the necessary [connection and authentication details](./index.html#basic-connection-information)  to access your cluster. You must include a port number, which your network  administrator can provide.4. Select either **DirectQuery** or **Import** as your **Data Connectivity**  mode.5. Select the authentication method.6. Click **OK**.7. Select the same authentication method and add credentials if needed, then  click **Connect** to establish a connection.8. After the connection is established, use the **Data Navigator** to  browse catalogs, query data sources, and more.### Standard ODBCThe standard ODBC connection mode requires you to add the ODBC driver to Windowsas an ODBC data source. To set up the data source, follow these steps:1. Open the **ODBC Data Source Administrator** utility.2. Add the ODBC driver as a **User** or **System Data  Source**.3. Click **Apply**.To connect from Power BI Desktop using a standard ODBC connection, follow thesesteps:1. In Power BI Desktop, select **Get Data** &gt; **More**.2. Select **All** &gt; **ODBC** and click **Connect**.   &amp;nbsp;{% include image.html   url=&#39;../../assets/img/general/power-bi-odbc-source.png&#39;   img-id=&#39;Data source selection&#39;   alt-text=&#39;Selecting ODBC as a data source&#39;   descr-text=&#39;Image showing ODBC selected as a data source&#39;   pxwidth=&#39;500&#39;   screenshot=&#39;true&#39;   %}3. Under **Data source name (DNS)**, select the {{site.terms.sep}} or   {{site.terms.sg}} data source.4. Under **Advanced options** &gt; **Connection string** add ``host=&quot;hostname:port&quot;``,  replacing &quot;hostname:port&quot; with the [connection details](./index.html#basic-connection-information)  for your cluster.5. Configure the necessary [authentication details](./index.html)  for your cluster.7. After the connection is established, use the **Data Navigator** to  browse catalogs, query data sources and more.### Authentication and SecurityPower BI Desktop always tries to connect with an encrypted connection first. Ifyou are connecting without TLS/SSL, the connector offers the option to connectusing an unencrypted connection afterwards.To use a TLS-encrypted connection with your cluster, make sure the server uses aglobally trusted certificate. {{site.terms.sg}} uses a globally trustedcertificate, and the default port 443.If this is not the case, add the server’s certificate to thesystem trust store (**Certificates** &gt; **Trusted Root CertificationAuthorities**) ​before connecting. The certificate can be added for the machine,or for each user running the Power BI connector.  In many organizations thisis handled automatically as part of the operating system and browserconfiguration.### LDAP authentication with Power BI DesktopIf your cluster is configured to use LDAP authentication, select **LDAP** in theauthentication field and provide your username and passwordcredentials.### Kerberos authentication with Power BI DesktopTo use Kerberos authentication, Kerberos must be installed for the user andinitialized using ``kinit``, before using the driver. This establishes your usercredentials on the machine.Select **Kerberos** in the authentication field and providethe Kerberos **Service name**.## Connect with Power BI serviceUsing the web-based Power BI service requires you to have the [on-premises datagateway](https://powerbi.microsoft.com/gateway/) with the appropriatepermissions, and the Presto ODBC driver installed with a license on each clientmachine.To connect to {{site.terms.sep}} or {{site.terms.sg}} as a data source, followthese steps:1. Log in to the Power BI service.2. Navigate to **Setting** &gt; **Manage gateways**.3. Select the gateway, and **Add data source**.4. Set the **Data Source Type** to **Starburst Enterprise**.5. Configure the necessary [connection and authentication details](./index.html#basic-connection-information)  for your cluster. You must include a port number, which your network  administrator can provide.6. Click **Add** to create the data source.## Limitations* Self-signed certificate usage for TLS/SSL connections is not supported.* Writing and using custom SQL statements is not supported with DirectQuery.  The ODBC standard connector can be used for custom SQL but does not support  direct querying.* Authentication type and field name customization does not apply on the Power  BI service. The following is the mapping of the service field names to their  Desktop counterparts:  Service name      | Desktop name  ----------------- | -----------------------  Basic             | LDAP  Key               | Kerberos  Key: Account Key  | Kerberos: Service Name## Release notes### Version 3.0.0December 2021* Support for version 2.0.0+ of the {{site.terms.sb}} [ODBC driver](odbc).* Change query source name to PowerBI-Extension **Breaking changes:*** Remove support for the Presto ODBC driver (prior to ODBC version 2.0.0).### Version 2.0.0April 2021:* Remove beta flag.* Change name to **Starburst Enterprise**."
 },
 {
  "title": "Privileges",
  "url": "/starburst-galaxy/security/privileges.html",
  "content": "# {{page.title}}A privilege applies to an entity and provides the right to perform specificactions. The following sections describe these privileges as they relate to[security](../security/index.html), [catalogs](../catalogs/index.html) and[clusters](../clusters/index.html).There are two categories of privileges:1. Privileges associated with the account and not a specific entity in the   account. These include privileges to create new new global entities, such as   clusters and catalogs, and to manage security for all entities.2. Privileges that grant rights to a single entity. These privileges are called   *entity privileges*.  The only current entity privileges are rights to use or   operate a CLUSTER.## Account privileges            Privilege      Description                  MANAGE_SECURITY      MANAGE_SECURITY is the encompassing privilege for security management.          It allows you to grant or revoke any privilege or role on any entity.          It can grant these to itself, and can also create, update or delete          any user or any role.                    CREATE_CLUSTER      Create a new cluster. Does not convey the right to modify, stop or          start any cluster.              CREATE_CATALOG      Create a new catalog. Does not convey the right to use, modify or          delete any catalog.              CREATE_ROLE      Create a new role. Does not convey the right to grant, modify or          delete any role.              CREATE_USER      Create a new user. Does not convey the right to modify or          delete any user, nor to grant or revoke roles to the user.              VIEW_AUDIT_LOG      View the Audit log page.              MANAGE_BILLING      View usage and billing and update account profile.      ## Cluster privileges            Privilege      Description                  START_STOP_CLUSTER      Start or stop the cluster.              USE_CLUSTER      View and run queries against the cluster. Does not convey the          right to modify, stop or start the cluster.      "
 },
 {
  "title": "Optimizing query performance",
  "url": "/data-engineer/query-performance.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is fast. But did you know that there are stillmany opportunities to make it even faster depending on how you write yourqueries?Learn how to use `EXPLAIN` and `ANALYZE` to improve your query performance inthis training video presented by one of our founders, Martin Traverso. For yourconvenience, we&#39;ve divided the video training course up into topic sections, andprovided links to the relevant parts of our documentation below.## The query lifecycleKnowing what&#39;s happening under the hood in SQL can help you to write queriesthat capitalize on possible optimizations and avoid approaches that will costyou performance. This section provides an overview of what happens as a query isexecuted.             {% include youtubeSnippet.html id=page.youtubeId1 start=491 end=1217 %}              Topics:              Parsing        Analysis        Planning        Optimization        Scheduling and execution            Running time: ~12 min.      ## The EXPLAIN statement in detailIf you want to understand what the {{site.terms.sep}} engine is basing its decisions onas it executes a query, you need to use the `EXPLAIN` statement. This sectionwalks you through this very informative tool in detail.             {% include youtubeSnippet.html id=page.youtubeId1 start=1218 end=2469 %}              Topics:              EXPLAIN        EXPLAIN vs EXPLAIN ANALYZE        Fragment            structure, distribution, row layout, estimates, and performance stats in EXPLAIN ANALYZE        Exchanges            Click the links to read more on that topic in our reference manual.      Running time: ~20 min.      ## General optimizationsThe content in this section is more technique-oriented, and is a complexsubject. We strongly suggest watching it all the way through thoroughly first togain a broad awareness of how you write a query can affect its performancebefore trying these on your own. For further reading, we recommend our[pushdown](../latest/optimizer/pushdown.html)documentation.The SQL engine relies on table statistics to make decisions on optimizations.Enabling dynamic filtering can take optimizations even further. We recommendreading about these powerful features to ensure you are getting the bestperformance possible out of your cluster:* [Dynamic filtering](../latest/admin/dynamic-filtering.html)* [Table statistics](../latest/optimizer/statistics)                 {% include youtubeSnippet.html id=page.youtubeId1 start=2470 end=5940 %}              Topics:              Constant folding        Predicate pushdown        Predicate pushdown into the Hive connector        Hive partition pruning        Hive bucket pruning        Row group skipping for ORC and Parquet        Limit, partial limit, and aggregation pushdown        Skew            Running time: ~58 min.      SEP offers [several properties to control how theoptimizer](../latest/admin/properties-optimizer.html) handles certainoperations.## Cost-based optimizationsThis section presents on overview of how cost-based optimizations work in {{site.terms.sep}},and provides great context for the following recommended reading:* [Cost-based optimizations](../latest/optimizer/cost-based-optimizations.html)* [Cost in EXPLAIN](../latest/optimizer/cost-in-explain.html?highlight=explain)             {% include youtubeSnippet.html id=page.youtubeId1 start=5941 end=6729 %}              Topics:              Partitioned and broadcast joins        Disabling cost-based optimizations        Join reordering        Table statistics        Computing statistics with ANALYZE            Running time: ~13 min.      "
 },
 {
  "title": "Amazon RDS",
  "url": "/ecosystems/amazon/data-sources/rds.html",
  "content": "# {{page.title}}You can query data stored on relational databases hosted on [Amazon RelationalDatabase Service (RDS)](https://aws.amazon.com/rds/) with{{site.terms.sep_first}} and {{site.terms.sg}}.## {{site.terms.sep_full}}Create a catalog with the [MySQLconnector](../../../latest/connector/starburst-mysql.html) with the followingdatabases:* [Amazon Aurora](https://aws.amazon.com/rds/aurora/), MySQL-compatible edition* [Amazon RDS for MariaDB](https://aws.amazon.com/rds/mariadb/)* [Amazon RDS for MySQL](https://aws.amazon.com/rds/mysql/)Create a catalog with the [PostgreSQLconnector](../../../latest/connector/starburst-postgresql.html) with the followingdatabases:* [Amazon Aurora](https://aws.amazon.com/rds/aurora/), PostgreSQL-compatible  edition* [Amazon RDS for PostgreSQL](https://aws.amazon.com/rds/postgresql/)## {{site.terms.sg}}Use a [MySQL catalog](../../../starburst-galaxy/catalogs/mysql.html) with thefollowing databases:* [Amazon Aurora](https://aws.amazon.com/rds/aurora/), MySQL-compatible edition* [Amazon RDS for MariaDB](https://aws.amazon.com/rds/mariadb/)* [Amazon RDS for MySQL](https://aws.amazon.com/rds/mysql/)Use a [PostgreSQL catalog](../../../starburst-galaxy/catalogs/postgresql.html)with the following databases:* [Amazon Aurora](https://aws.amazon.com/rds/aurora/), PostgreSQL-compatible  edition* [Amazon RDS for PostgreSQL](https://aws.amazon.com/rds/postgresql/)"
 },
 {
  "title": "Amazon Redshift",
  "url": "/ecosystems/amazon/data-sources/redshift.html",
  "content": "# {{page.title}}You can analyze structured and semi-structured data across data warehouses,operational databases, and data lakes with [AmazonRedshift](https://aws.amazon.com/redshift/) and {{site.terms.sep_first}}.## {{site.terms.sep_full}}Create a catalog with the [Redshiftconnector](../../../latest/connector/starburst-redshift.html)."
 },
 {
  "title": "Starburst Admin release notes",
  "url": "/starburst-enterprise/starburst-admin/release-notes.html",
  "content": "# {{ page.title }}New releases of {{site.terms.sbadmin}} are created whenever significant newfeatures or bug fixes are available.## 1.2.0 (6 Aug 2021)* Add condition to status printing to detect what preliminary task was skipped* Add `OmitStackTraceInFastThrow` JVM flag* Setup limits in `/etc/security/limits.d` for tarball installation## 1.1.0 (11 Jun 2021)* Fix `installation_group` name issue* Remove `query.*` config property values in files to use automatic defaults* Move user documentation to [doc.starburst.io](https://docs.starburst.io/starburst-enterprise/starburst-admin/index.html)* Remove developer documentation from package## 1.0.0 (24 Mar 2021)* Initial supported release with full feature set to run production systems"
 },
 {
  "title": "Roles and privileges",
  "url": "/starburst-galaxy/account/roles-privileges.html",
  "content": "# {{page.title}}The **Roles and privileges** section displays a list of all configured[roles](../security/roles.html) in your {{site.terms.sg}} account. A role is acollection of [privileges](../security/privileges.html). Users are assigned oneor more roles. Use the edit icon on the left of each row to edit or delete therole.## Create a roleUse the **Create role** button to create additional roles. A role only consistsof a name and a description. Once created it is displayed in the list of roles,and you can proceed to add privileges to the role.## Add privilegesYou can use the **Privileges** button to add or remove privileges to a role. Theassigned privileges are listed in the **View privileges** section. Use thedelete icon beside a privilege to remove the privilege from the role. The **Addprivilege** button displays a dialog that allows you to configure the entity andprivilege to configure the new privilege to add to the role.## Assign a roleThe **Assign** button is used to assign the role to other roles and users."
 },
 {
  "title": "Roles",
  "url": "/starburst-galaxy/security/roles.html",
  "content": "# {{page.title}}The [security system of {{site.terms.sg}}](./index.html) uses roles to bundletogether one or more privileges to perform actions. A role has a name and anoptional description. [Privileges](./privileges.html) can be grantedto roles to configure actions and access to [entities](./entities.html) likecatalogs and clusters.Users are assigned one or more roles, and by selecting a role gain the rightsdefined by the role&#39;s privileges. You can see the role you are currently using,and change to another role, in the top right corner of the user interface.As administrator or with the relevant access, you can manage [roles andprivileges in the Account section](../account/roles-privileges.html) of the userinterface.## Role grantsIf a role is *granted* to another role, all the privileges of the first role arealso conveyed to the second role. The relationship between a role receiving agrant and the role granted is a sort of parent/child relationship, and the onlyrestriction on role grants is that they are not allowed to create grant loops. Arole can have zero or more child roles, and always has one or more parent roles.As a result roles form a `directed acyclic graph`_. The collection of alldescendant roles is called the *active role set* of the starting role.A role has all the privileges of all roles in its active role set. That meansall the privileges of those child roles, plus the privileges of their children,and so on.You can grant one or more roles to a role [in the userinterface](../account/roles-privileges.html).## Roles and entity ownershipAll entities are *owned* by exactly one role. When a new entity is created, theowner is set to the active role, and is granted all privileges on the entity.If a role or any role in the role&#39;s active role set owns an entity, that rolehas the rights to delete or update the entity, and the right to grant privilegeson the owned entity to another role.## User rolesUsers may be granted roles, and all users are implicitly granted the pre-definedrole `public`. At any moment a user assumes a single role, chosen using the roleselection dropdown always shown in the upper right corner of the screen. Byassuming a role, the user has all the privileges of that role, plus all theprivileges of all roles that are descendants of that role, and so on. The set ofall roles transitively granted to the currently set role is called the *activerole set*. The user session has the privileges of all roles in the active roleset, and has ownership on any entity owned by a role in the active role set.## Predefined rolesSeveral roles are predefined and may not be deleted, nor may their predefinedprivileges be revoked:* `accountadmin` is granted the MANAGE_SECURITY privileges and it can therefore  assign any access to itself. It can literally do anything. Only a limited list  of users should be granted the `accountadmin` role. Best practice is to create  new roles appropriate to the needs of your organization, and grant these new  roles only the necessary privileges needed to perform their tasks.* `public` is the name of the predefined default role that is implicitly  granted to every user and cannot be revoked. Administrators can assign  additional privileges to the public role.* `_system` is defined by the SQL specification to be grantor of privileges to  all newly-created entities. It has no other function in the RBAC system, and  it can not be granted to other roles.## Roles should fit user communitiesPicking the new roles and their corresponding privileges starts with identifyingthe different communities of users. In a typical organization most users do notneed to create or manage catalogs or clusters, they just want to run queries.They need nothing beyond USE_CLUSTER on one or more clusters.For these users, it makes sense to create a `data_consumer` role with only theUSE_CLUSTER privilege on one or more clusters.  Users granted the role`data_consumer` can run SQL queries on clusters, but can not do any damageto any {{site.terms.sg}} setup.If users need only one specific cluster at a time, it can make sense to createa role for each cluster with USE_CLUSTER on that cluster, and perhapsadditional roles that group these roles.## Administrative rolesFor administrative operations, it makes sense to divide the privileges intodistinct roles. There are many ways to divide administrative privileges, butorganizations can start with this traditional division of administrativeresponsibility among roles:* `data_admin`, the owner of all catalogs and clusters, with privileges,  CREATE_CLUSTER, CREATE_CATALOG. `sysadmin` has no rights to manage users or  roles.* `security_admin`, the owner of all custom roles, with the privilege  CREATE_ROLE. `securityadmin` can create and delete roles, and grant or revoke  any privilege on any role. `securityadmin` has no privileges to manage or even  access catalogs, clusters or users.* `user_admin`, the owner of all users, with privilege CREATE_USER. `useradmin`  can only manage users, and has no privileges to control catalogs or  clusters."
 },
 {
  "title": "Try Starburst Enterprise with RPM",
  "url": "/starburst-enterprise/try/rpm.html",
  "content": "# {{page.title}}Users of RedHat Enterprise Linux (RHEL) and CentOS can use the RPM package toinstall {{site.terms.sep_first}}.The RPM archive includes the application, all plugins, the necessary defaultconfiguration files, default setups, and integration with the operating systemto start as a service.Use [{{site.terms.sbadmin}}](../starburst-admin/index.html) to install andmanage a cluster of [bare-metal servers](../../glossary.html#bare-metal-server)or [virtual machines](../../glossary.html#virtual-machine-vm). Only use the RPMpackage to deploy on a single node or a few nodes manually, or if you use analternative provisioning system. Find more information in our [deploymentoptions guide](./index.html#deployment-options).You can install and test the RPM with the following steps:1. Download the most recent {{site.terms.sep}} `RPM` package.2. [Install the package](#install-the-package).3. [Add configuration files](#rpm-specific-configuration-settings).4. Start the {{site.terms.sep}} server with the [service   script](#service-script).5. Download any supported [client   tool](../../data-consumer/clients/index.html) and query the cluster.## Requirements* 64-bit versions of RHEL 7, RHEL 8, CentOS 7, or CentOS 8.* x86-64 hardware.* Python 2.7 or later, needed to run the `service` script.* Java 11.0.11 or a later Java 11 LTS release from Azul, OpenJDK, or Oracle  Java distributions. Newer Java releases may work but are not tested  or supported.## Download an {{site.terms.sep}} archiveTo gain access to {{site.terms.sep}} archives, existing customers contact{{site.terms.support}}. If you are not a customer yet, visit the[{{site.terms.sb}} website](https://www.starburst.io) and click either the **GetStarted** or **Download Free** buttons.Fill out the form using a valid email address, then click **Free Download**.Open your email from {{site.terms.sb}}, and click the link to the downloads page.{% include note.html content=&quot;To unlock the performance and securityenhancements of Starburst Enterprise, you must request a license.&quot; %}The Downloads page is organized into a Long-Term Support section at the top withSteps 1 and 2, and a Short-Term Support section at the bottom. Use the LTSoption.From the **Step 1: Starburst Enterprise** section, click the **RPM** button.This starts the download of a file named with the pattern`starburst-enterprise-*.rpm`. If prompted to open or save the file, save it toyour `/home//Downloads` directory.## Install the packageCopy the RPM to the server, if you have downloaded it on a different machine.You need root or sudo access to run the installation commands. Use the`rpm`command to install the package:```shellrpm -i starburst-enterprise-*.rpm --nodeps```## Service scriptThe RPM installation deploys a service script configured with `chkconfig`, so{{site.terms.sep}} starts automatically on OS boot. After installation, you canmanage the {{site.terms.sep}} server with the `service` command for the`starburst` service:```shellservice starburst [start|stop|restart|status]```* `start` Starts the server as a daemon and returns the process ID.* `stop` Shuts down a server started with either ``start`` or ``run``. Sends  the SIGTERM signal.* `restart` Stops and then starts a running server, or starts a stopped server,  assigning a new process ID.* `status` Prints a status line, either *Stopped pid* or *Running as pid*.Once configuration files are in place, you can run start {{site.terms.sep}} withfollowing commands:```shellservice starburst start```Check the server&#39;s status to make sure the server finished the startup process:```shellservice starburst status```As an alternative, look for the exact phrase `SERVER STARTED` in the`/var/log/starburst/server.log` file.```shellgrep &quot;SERVER STARTED&quot; /var/log/starburst/server.log```You can also follow the log with `tail`:```shelltail -f  /var/log/starburst/server.log```## Installation directory structureThe RPM package places the various files used by {{site.terms.sep}} inaccordance with the Linux Filesystem Hierarchy Standard. This differs from thedefault [tarball](./tarball.html) installation of {{site.terms.sep}}, whereall folders are in the installation directory. For example, with a `tar.gz`file, configuration files are located by default in the `etc` folder of theinstallation directory. By contrast, the RPM package installation uses`/etc/starburst` for the same purpose.The RPM installation places {{site.terms.sep}} files using the followingdirectory structure:* `/usr/lib/starburst/lib/`: Various libraries needed to run the product;  plugins go in a `plugin` subdirectory* `/etc/starburst`: General {{site.terms.sep_full}} configuration files such as  `config.properties`, `jvm.config`, and `node.properties`* `/etc/starburst/catalog`: Connector configuration files* `/etc/starburst/env.sh`: Contains the Java installation path used by  {{site.terms.sep_full}}, allows configuring process environment variables,  including [secrets](../../..//latest/security/secrets.html)* `/var/log/starburst`: `server.log` files* `/var/lib/starburst/data`: Data directory* `/usr/shared/doc/starburst`: Documentation* `/etc/rc.d/init.d/starburst`: Service script## RPM-specific configuration settingsThe configuration files need to be placed in the installation directorystructure used by the RPM. In addition, the `node.properties` fileneeds two properties defined to adjust for the directory structure:```properitesnode.data-dir=/var/lib/starburst/datacatalog.config-dir=/etc/starburst/catalog```## Verify the serverTo verify that your locally-run server is operating as expected, invoke the{{site.terms.oss}} UI as described in [Verify theserver](./verify.html).## Run queriesTo run queries against your server, use any supported[client](../../data-consumer/clients/index.html).## UninstallingUninstalling the {{site.terms.sep}} RPM installation is like uninstalling anyother RPM:```shellrpm -e starburst-enterprise-```After uninstalling, all deployed {{site.terms.sep_full}} files are deletedexcept for the logs directory `/var/log/starburst`."
 },
 {
  "title": "Amazon S3",
  "url": "/ecosystems/amazon/data-sources/s3.html",
  "content": "# {{page.title}}You can query data stored on the Amazon Simple Storage Service (Amazon S3) with{{site.terms.sep_first}}. The most common usage is with the [Hiveconnector](../../../latest/connector/starburst-hive.html) and the dedicated[configuration options forS3-access](../../../latest/connector/hive-s3.html).In addition, you need to store the meta data about the object storage. Typicallythis is done with [AWS Glue](./glue.html), but you can also use your own HiveMetastore Service (HMS).Amazon S3 can also be used as storage backend with other systems, and thenqueried with the dedicated connector:* [Delta Lake](../../../latest/connector/starburst-delta-lake.html)* [Iceberg](../../../latest/connector/starburst-iceberg.html)* [Snowflake](../../../latest/connector/starburst-snowflake.html)Ensure the requirements for the connector are fulfilled.## RequirementsTo enable access to S3 with {{site.terms.sep_first}}, the following conditionsmust be met:* Your {{site.terms.sep}} cluster must have the necessary permissions needed to  access your S3 data.* Requirements of the connector used for the catalog you use to access S3 need  to be fulfilled. Typically this includes a meta store, either [AWS  Glue](./glue.html), or a HMS.## Ensuring {{site.terms.sep}} access to S3If your S3 data is publicly available, you do not need to do anything. However,S3 data is not typically publicly available, and you must grant{{site.terms.sep}} access to it.With the CFT support, you must [select an appropriate instanceprofile](../../../latest/aws/requirements.html#aws-instance-profiles) whencreating a cluster.Validate that the selected instance profile is sufficient for {{site.terms.sep}}to read S3 data by opening an SSH connection to the coordinator and issuing thefollowing commands:```shell$ aws s3 ls s3://your-bucket/path/to/dataset/$ aws s3 cp s3://your-bucket/path/to/dataset/data-file - &gt; /dev/null```### Configuring a catalog and meta storeA [catalog must be created](../../../data-engineer/catalogs.html) for your S3bucket.Users of the CFT can  [create a new HMS](../../../latest/aws/metastore.html)via a CloudFormation template. It must have an instance profilegranting access to S3. Alternatively, you can use an existing HMS:1. Set the **MetastoreType** parameter to ``External Hive Metastore  Service`` and **ExternalMetastoreHost** to IP address of your HMS.Users of the [Kubernetes support(../../../latest/k8s.html) can perform similarconfiguration.If the HMS uses authentication, review the Hive connector security[documentation](../../../latest/connector/hive-security.html) to configure theconnector for your environment.## Reading data from S3If you chose an existing HMS instance when configuring the Hive catalog, chancesare that your S3 data is already mapped to SQL tables in the HMS. In that case,you should be able to query it immediately.If you created a new HMS instance, it is likely that your S3 data is not yetmapped. In the HMS you must provide the schema of the data, the file format, andthe data location. For example, if you have ORC or Parquet files in an S3bucket, ``my_bucket``, create a queryable table in the desired schema of yourcatalog.```shell  USE hive.default;  CREATE TABLE orders (       orderkey bigint,       custkey bigint,       orderstatus varchar(1),       totalprice double,       orderdate date,       orderpriority varchar(15),       clerk varchar(15),       shippriority integer,       comment varchar(79)    ) WITH (      external_location = &#39;s3://my_bucket/path/to/folder&#39;,      format = &#39;ORC&#39; -- or &#39;PARQUET&#39;    );```Query the newly mapped table as usual:```shell  SELECT * FROM orders;```## StatisticsIf your queries are complex and include joining large datasets, you may runinto performance issues. This is because {{site.terms.sep}} does not know thestatistical properties of the data necessary for the cost-based optimizer&#39;sdecisions.To gather table statistics, execute the following command:```shell  ANALYZE orders;```## Writing data to S3After configuring  {{site.terms.sep}} to read data from S3, you can write to it.If your HMS contains schema(s) mapped to S3 locations, you can usethem to export data to S3. If you don&#39;t want to use existing schemas (or thereare no appropriate schemas in the HMS), create a new one.```shell  CREATE SCHEMA hive.s3_export WITH (location = &#39;s3://my_bucket/some/path&#39;);```Once you have a schema pointing to a location where you want to export the data,export data using a ``CREATE TABLE AS`` statement that specifies yourdesired file format:```shell  CREATE TABLE hive.s3_export.my_table  WITH (format = &#39;ORC&#39;)  AS ;```Data is written to one or more files within the``s3://my_bucket/some/path/my_table`` namespace. The number of files depends onthe size of the data being exported and possible parallelization of the sourceof the data."
 },
 {
  "title": "Amazon S3 catalogs",
  "url": "/starburst-galaxy/catalogs/s3.html",
  "content": "# {{page.title}}You can use an Amazon S3 catalog to configure access to an [AmazonS3](https://aws.amazon.com/s3/) object storage hosted on Amazon Web Services.The metadata about the objects and related type mapping needs to be stored in ametastore. You can use Amazon Glue, a Hive Metastore Service, or the built-inmetastore.## Configure a catalogTo create an Amazon S3 catalog, select **Catalogs** in the main navigation andclick **Configure a catalog**. Click on the **Amazon S3** button in the**Select a data source** screen.{% include video.html content=&quot;See it all in action with the demo videofor S3 catalogs.&quot; %}{% include_relative name-description.md %}{% include_relative read-only.md %}### Authentication to S3Provide the **AWS access key for S3** and the **AWS secret key for S3** to grantaccess to the object storage.The connection to the object storage requires access key and secret key.### Metastore configuration{% include_relative aws-glue.md %}{% include_relative hms.md %}{% include_relative sgm.md %}{% include_relative save.md %}"
 },
 {
  "title": "Sample dataset",
  "url": "/starburst-galaxy/catalogs/sample.html",
  "content": "# {{page.title}}The sample dataset provides data in a number of smaller tables that representan organization, employer, and related data. The following tables are availablein the `demo` schema of the catalog:* current_dept_emp* departments* dept_emp* dept_emp_latest_date* dept_manager* employees* salaries* titles## Configure a catalogTo create a catalog with the sample dataset, select **Catalogs** in the mainnavigation and click **Configure a catalog**. Click on the **Sample dataset**button in the **Select a dataset** section.{% include_relative cloud-provider.md %}{% include_relative name-description.md %}{% include_relative region.md %}{% include_relative save.md %}"
 },

 {
  "title": "Search",
  "url": "/searchresults.html",
  "content": "  {{page.title}}                                    Filter:                                                  "
 },
 {
  "title": "Securing Starburst Enterprise",
  "url": "/platform-administrator/security.html",
  "content": "# {{ page.title }}Learn how to safeguard your data with {{site.terms.sep_first}}&#39;s securitytoolkit in this training video presented by one of our founders, Dain Sundstrom.For your convenience, we&#39;ve divided the video training course up into topicsections, and provided links to the relevant parts of our documentation below.## Introduction             {% include youtubeSnippet.html id=page.youtubeId1 start=306 end=943 %}              Topics:              SEP security process        What to secure        Preparing: Verifying HTTP            Running time: ~11 min.      ## Client to server encryption with TLSEnabling TLS to the coordinator can be handled from the load balancer in twoways:* Terminate TLS on the load balancer and use HTTP to the coordinator* Pass TLS through the load balancer and use HTTPS to the coordinatorWe highly recommend terminating HTTPS on the load balancer. In this case, the onlyproperty required in the Helm chart `coordinator.etcFiles.properties` sectionunder `config.properties:` is as follows:```propertiescoordinator:  etcFiles:    properties:      config.properties: |        http-server.process-forwarded=true```For non-Kubernetes installs, the same property must be defined instead in the`config.properties` file.To pass TLS through the load balancer, you must configure TLS on thecoordinator. Place a certificate or private key in the coordinator pod withHTTPS enabled. To do so:1. Create a secret from the signed certificate or private key. This file can be   a PEM, JKS, or PK12 keystore.2. Follow the steps outlined in the reference documentation to [configure the   coordinator](../../latest/security/tls.html#configure-the-coordinator). Refer to   your SSL secret as follows in the following snippet, which uses a PEM file   as an example:```propertiescoordinator:  etcFiles:    properties:      config.properties: |        http-server.https.keystore.path=secretRef:ssl-cert:yoursslcert.pem```The following video provides an overview of client-to-server encryption in{{site.terms.sep}}:             {% include youtubeSnippet.html id=page.youtubeId1 start=944 end=2096 %}              Topics:              Approaches for HTTPS, including proxies and load balancers        Adding SSL/TLS certificates        Handling PEM and JKS files        Verifying HTTPS for SEP            Running time: ~19 min.      ## Authentication and authorization in {{site.terms.sep}}### Get up and runningThe {{site.terms.sep}} Helm chart has built-in support for file-based passwordauthentication. In the following example, two users are created with very simplepasswords:```YAMLuserDatabase:  enabled: true  users:    - username: admin      password: adM1nPassWord    - username: user1      password: Us4r1PassW0rd```File-based authentication is recommended only for your initial build-out. Moredetail is available in our [referencedocumentation](../../latest/security/password-file.html),including how to handle non-Kubernetes installs.### Implement LDAPAuthentication via LDAP or Active Directory is configured in the`coordinator.etcFiles.properties` section of the Helm chart under`password-authenticator.properties:`.The following example shows the required Helm chart configuration:```YAMLcoordinator:  etcFiles:    properties:      password-authenticator.properties: |        password-authenticator.name=ldap ldap.url=ldaps://ldap-server:636        ldap.user-bind-pattern=uid=${USER},DC=example,DC=com        ldap.ssl-trust-certificate=secretRef:ldap-ca:ca.crt```In non-Kubernetes installs, the same properties must be defined instead in the`etc/password-authenticator.properties` file.More detail is available in our [referencedocumentation](../../latest/security/ldap.html#password-authenticator-configuration),including how to handle non-Kubernetes installs.### Learn moreThe following video provides an overview of authentication and authorization in{{site.terms.sep}}:             {% include youtubeSnippet.html id=page.youtubeId1 start=2097 end=4141 %}              Topics:              Password file authentication        LDAP authentication (See also: group providers)        Kerberos authentication (See also: passthrough)        Client certificate authentication        JSON Web Token authentication        Using multiple authenticators        Authentication with user mapping        Overview of authorization        File-based system access control            Running time: ~34 min.      ## Securing {{site.terms.sep}}&#39;s internal communications and management endpointsDocumentation for the material covered in this section is found[here](../latest/security/internal-communication.html).             {% include youtubeSnippet.html id=page.youtubeId1 start=4694 end=5608 %}              Topics:              Securing the Starburst cluster itself        Shared secret        Internal HTTPS        Secrets management        Management endpoints            Running time: ~16 min.      ## Hive catalog securityWe recommend the following additional reading, which covers enabling{{site.terms.sep}}&#39;s powerful role-based global access control:* [Access control overview](../latest/security/access-control.html)* [Global access control with Apache Ranger](../latest/security/global-ranger.html)* [Global access control with Privacera](../latest/security/global-privacera.html)* [Built-in system access control](../latest/security/built-in-system-access-control.html)While we strongly recommend implementing global access control, you can stillsecure Hive at the catalog level if your particular situation makes thatnecessary. Documentation covering the various options for securing Hive at thecatalog level can be found as follows:* [Configuring Hive security](../latest/connector/hive-security.html)* [Hive-level security with Apache Ranger](../latest/security/hive-ranger.html)* [Hive-level security with Privacera](../latest/security/hive-privacera.html)* [Hive-level security with Apache Sentry](../latest/security/hive-sentry.html)             {% include youtubeSnippet.html id=page.youtubeId1 start=5609 end=6650 %}              Topics:              Authorization        Metastore authentication        HDFS authentication        Kerberos debugging        S3 authentication        Google Cloud authentication            Running time: ~18 min.      "
 },
 {
  "title": "Sitemap",
  "url": "/sitemap.xml",
  "content": "            https://docs.starburst.io/universe-sitemap.xml              https://docs.starburst.io/latest/sitemap.xml              https://docs.starburst.io/364-e/sitemap.xml              https://docs.starburst.io/360-e/sitemap.xml              https://docs.starburst.io/356-e/sitemap.xml              https://docs.starburst.io/350-e/sitemap.xml              https://docs.starburst.io/363-e/sitemap.xml              https://docs.starburst.io/361-e/sitemap.xml      "
 },
 {
  "title": "SQuirrel SQL Client",
  "url": "/data-consumer/clients/squirrel-sql.html",
  "content": "# {{ page.title }}[SQuirrel SQL](http://www.squirrelsql.org) is a Java-based graphical databaseclient that allows you to view the structure of your database, browse the datain tables, and issue SQL commands. The client is installed as alocal application on your workstation. You can use the client to access datasources from {{site.terms.sb}} clusters, since it supports the[JDBC driver](./jdbc.html).  ## ConfigurationFollow these steps to access your {{site.terms.sep}} cluster with SQuirrel SQL:1. Get the necessary [connection   information](./index.html#connection-information) for your cluster.2. Download the [JDBC driver](../../data-consumer/clients/jdbc.html#download-the-jdbc-driver).1. Copy the JDBC driver `.jar` into the desired directory.2. Start the SQuirrel SQL client.3. Add and configure the Trino JDBC driver:   a. Select **New Driver**.   b. Enter the following information in each field:    * **Name**: Trino    * **Example URL**: `jdbc:trino://host:port/catalog/schema`    * **Website URL**: `https://trino.io`   c. Select the **Extra Class Path** tab.   d. Click **Add** and navigate to the JDBC driver `.jar` you downloaded.   e. Click **List Drivers**.   f. Set the **Class Name** to `io.trino.jdbc.TrinoDriver`.   g. Click **OK**. Look for a success message in the client logs if the      driver setup was successful.      &amp;nbsp; {% include image.html      url=&#39;../../assets/img/general/squirrel-sql-driver.png&#39;      img-id=&#39;SQuirrel SQL driver setup&#39;      alt-text=&#39;SQuirrel SQL driver setup&#39;      descr-text=&#39;Image depicting SQuirrel SQL driver setup&#39;      pxwidth=&#39;500&#39;      screenshot=&#39;true&#39;      %}4. Create an alias for {{site.terms.sb}}:   a. Select **Add Alias**.   b. Enter the following information in each field:      * **Name**: A name for the connection to {{site.terms.sep}}      * **Driver**: Trino (as created during the driver configuration steps)      * **URL**: `jdbc:trino://host:port`. You can optionally specify a        catalog and schema in this URL so you do not have to fully qualify        table names in queries. Replace `host:port` with the connection        details for your cluster.        You must fill in a port number, which your network        administrator can provide, or enter one of the [default        ports](./index.html#default-ports) for {{site.terms.sep}} clusters.      * **User Name/Password**: Credentials to access your {{site.terms.sep}}        coordinator. If no authentication is configured on the coordinator,        leave the password field blank. You must still specify a username so        {{site.terms.sep}} can report the initiator for any queries.   c. Click **OK**.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/squirrel-sql-alias.png&#39;   img-id=&#39;SQuirrel SQL alias setup&#39;   alt-text=&#39;SQuirrel SQL alias setup&#39;   descr-text=&#39;Image depicting SQuirrel SQL alias setup&#39;   pxwidth=&#39;500&#39;   screenshot=&#39;true&#39;   %}5. (Optional) For a TLS-enabled cluster, add the `SSL=true` property and value.   a. Select **Modify** for your {{site.terms.sep}} alias.   b. Click **Properties**.   c. Select the **Driver properties** tab.   d. Right-click a header in the properties table, and select      **Add property**.    &amp;nbsp; {% include image.html    url=&#39;../../assets/img/general/squirrel-sql-connect.png&#39;    img-id=&#39;SQuirrel SQL connection prompt&#39;    alt-text=&#39;SQuirrel SQL connection prompt&#39;    descr-text=&#39;Image showing where to add a property to a SQuirrel SQL alias&#39;    pxwidth=&#39;300&#39;    screenshot=&#39;true&#39;    %}    e. Enter the following information in each field:      * **Property Name**: `SSL`      * **Property Value**: `true`      * **Property Description**: &quot;Set to true in order to enable TLS&quot;    f. Click **Add**.    g. Select the **Use driver properties** and **Specify** checkboxes.    h. Click **OK**, and **OK** again to save your changes to the alias.6. Test your connection:    a. Select the {{site.terms.sep}} alias.    b. Click the **Connect** button.    c. Enter your credentials in the **User** and **Password** fields.    d. Click **Connect**.{% include image.htmlurl=&#39;../../assets/img/general/squirrel-sql-properties.png&#39;img-id=&#39;SQuirrel SQL connection prompt&#39;alt-text=&#39;SQuirrel SQL connection prompt&#39;descr-text=&#39;Image depicting the SQuirrel SQL connection prompt&#39;pxwidth=&#39;500&#39;screenshot=&#39;true&#39;%}{% include note.html  content=&quot;If the connection appears to be successful but the returned  object doesn&#39;t contain any tables, this likely indicates that your client has  reached the cluster but is unable to establish a connection. Check the error  logs, your credentials, port information, and TLS configuration to  troubleshoot.&quot;%}## HTTPS/TLSAny {{site.terms.sep}} cluster that requires authentication is also required touse [TLS/HTTPS](../../latest/security/tls.html). If you&#39;re using globallytrusted certificate best practices, use the cluster&#39;s HTTPS URL in thedriver and alias URLs as shown in the steps above.If you&#39;re not using a globally trusted certificate, you may have to configurethe trust store on your client machine. Consult your site&#39;s networkadministrators for guidance.To use TLS, you must specify the JDBC parameter setting `SSL=true` as shown inthe steps above.## QueryingClick on the {{site.terms.sep}} alias in the **Aliases** tab to connect to your{{site.terms.sep}} coordinator. The initial connection downloads all metadataabout catalogs, schema, tables, columns and more.{% include warning.html  content=&quot;This download can take a significant amount of  time for large catalogs (thousands of tables or more).&quot;%}Under the **Objects** tab, youcan browse your data sources and their metadata.{% include image.html  url=&#39;../../assets/img/general/squirrel-sql-objects.png&#39;  alt-text=&#39;SQuirrel SQL database navigator&#39;  descr-text=&#39;SQuirrel SQL database navigator&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}Under the **SQL** tab, write and execute queries to inspect your tables&#39;contents.{% include image.html  url=&#39;../../assets/img/general/squirrel-sql-query.png&#39;  alt-text=&#39;SQuirrel SQL editor&#39;  descr-text=&#39;SQuirrel SQL editor&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}"
 },
 {
  "title": "User personas",
  "url": "/get-started/starburst-personas.html",
  "content": "# {{ page.title }}No matter what their job title is, most {{site.terms.sb}} users fit one of ouruser personas:* *Data consumers* query data through existing catalogs.* *Data engineers* create catalogs that connect data sources to Starburst.* *Platform administrators* run and maintain the Starburst cluster.These personas embody a set of focused workflows and assume a nominal skillset. Your role may comprise some or all of one or more of these personas&#39;workflows, and that is ok! In fact, we can easily describe this overlap:* All users in the course of their job are data consumers at some level.* Data engineers might do a bit of platform administration.* Platform administrators might do a bit of data engineering.Rather than repeat information or throw every possible workflow at every user,{{site.terms.sb}} user guides are organized to answer a very important question:&quot;Where do I start?!&quot; We use personas to accomplish this.Let&#39;s meet the {{site.terms.sb}} personas so that you can start with the oneclosest to your day-to-day workflows.## Data consumerData analysts, data scientists, and casual report wranglers all use{{site.terms.sb}} in very similar ways. In a nutshell, a data consumer focuseson one or more of the following:* Delivering visualizations and reports.* Making well-informed, data-driven decisions.* Creating forecast and machine learning models that describe the business.* Performing ad hoc analyses.In our guides, we assume a reasonable level of skill with[SQL](../glossary.html#sql), including some knowledge of more advanced queries,and some combination of the following:* Limited to reasonable programming skills* Excellent spreadsheet skills, including some modeling* Knowledge of statistical methods and/or machine learning* Competence with data visualization and/or reporting tools* Ability to detect and articulate issues with data, even if unable to  remedy them or trace the causeIt&#39;s worth noting that downstream users who only consume data through generatedreports and visualizations and don&#39;t actively query data themselves directly aredirect customers of data consumers. Our documentation does not teach basic SQLskills.Our [data consumer user guide](../data-consumer/index.html) provides muchinformation and in-depth training that covers:* Starburst clients* Query federation* Query optimization* Migrating queries to Starburst SQL              Learn more from the data consumer guides                          ## Data engineerData engineers deliver data to data consumers in a performant and suitableformat, and in a timely manner with an expectation of a given level of dataquality. Often they source new data from a variety of relational databases,object stores, log entries, message streams or product endpoints. In a nutshell,a data engineer focuses on:* Creating and updating data models &amp; data schemas.* Building and managing ETLs.* Identifying, designing, and implementing internal process improvements such as  automating manual processes, optimizing data delivery for greater scalability.* Building and managing streaming data pipelines.* Building data integrations between various 3rd party systems such as Salesforce  and Workday.Data engineers are customers to platform administrators and the servicesthey provide. Data consumers are the direct customers of data engineers.{{site.terms.sb}} lets data engineers decouple compute from and simplifiesdelivering the data that your users need. Our [data engineer userguides](../data-engineer/index.html) gets you started with detailed information andtraining on topics such as:* Creating catalogs to connect data sources.* Diagnosing and fixing query performance issues.* Developing custom connectors.              Learn more from the data engineer guides                          ## Platform administrator{{site.terms.sb}} platform administrators care about the scalability,performance and reliability of the {{site.terms.sb}} cluster. They balance SLAsfor both data landing times and availability; implement access and datagovernance policies; and support audit requirements. In a nutshell, platformadministrators focus on:* Building and maintaining scalable data platform architectures to support the  ingest, storage and querying of large heterogenous datasets.* Creating and monitoring cluster health metrics to ensure optimal performance  and reduce any downtime.* Implementing and working with cloud providers like AWS, Azure, and Google  Cloud.{{site.terms.sb}} runs on [COTS](../glossary.html#cots) hardware, and uses memoryinstead of disk, making it fast and more cost-effective. We&#39;ve put together muchin-depth information and training in our [platform administratorguide](../platform-administrator/index.html) for you,covering:* Security* Performance tuning* Cluster setup and configuration              Learn more from the platform administrator guide                          "
 },
 {
  "title": "Using SQL in Starburst",
  "url": "/data-consumer/starburst-sql.html",
  "content": "# {{ page.title }}{{site.terms.oss}}&#39;s open source distributed SQL engine runs fast analyticqueries against various data sources ranging in size from gigabytes topetabytes. {{site.terms.sb}} brings that SQL engine to even more data sources,with more robust features. Because {{site.terms.sb}}&#39;s SQL is ANSI-compliant andsupports most of the SQL language features you depend on, you can hit the groundrunning.Business intelligence users and data scientists can continue to use theirfavorite [client tools](./clients/index.html) such as Tableau, Qlik and ApacheSuperset to access and analyze virtually any data source, or multiple datasources in a single query.## The basicsWe know you want to jump right in, and we know you already have awesomeanalytics skills. It&#39;s just a matter of harnessing the power of{{site.terms.sb}} products to take your analytics even further. With that inmind, you can browse our latest reference materials to learn just how familiar{{site.terms.sb}} SQL is:* [SQL language](../latest/language.html)* [SQL statement syntax](../latest/sql.html)* [Functions and operators](../latest/functions.html)### CatalogsOne key difference worth highlighting is the concept of *catalogs*. Each of yourdata sources is defined as a catalog in {{site.terms.sb}}, and that catalog inturn contains schemas. Using a SQL client such as our CLI, you can discover whatcatalogs are available:```sqltrino&gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows)```From there, you can use the familiar `SHOW SCHEMAS` command to drill furtherdown.### Fully-qualified table namesTable names are fully qualified when they include the catalog name:```..```This becomes critical when creating [federated queries](./data-mesh.html).### General SQL featuresJust in case you&#39;d like a review, here&#39;s a walkthrough of some basic SQLfeatures in {{site.terms.sb}} from one of our founders, David Phillips:             {% include youtubeSnippet.html id=page.youtubeId1 start=585 end=1060 %}              Topics:              Formatting        CASE and searched CASE expressions        IF expressions        TRY expressions        Lambda expressions            Click on the links to read more on that topic in our reference manual.      Running time: ~8 min.      ## Advanced SQLReady to move past the basics? For your convenience, we&#39;ve divided the AdvancedSQL for {{site.terms.sb}} video training course up into topic sections, andprovided links to the relevant parts of our documentation below.### Advanced aggregation techniques             {% include youtubeSnippet.html id=page.youtubeId1 start=1970 end=3654 %}              Topics:              count() with DISTINCT        Approximations, including counting and percentiles        max_by() values        Pivoting with count_if() and FILTER        Complex aggregations        Checksums        ROLLUP        CUBE        GROUPING SETS            Running time: ~28 min.      ### Window functions             {% include youtubeSnippet.html id=page.youtubeId1 start=5280 end=6768 %}              Topics:              Row numbering        Ranking        Ranking and numbering without ordering        Bucketing and percentage ranking        Partitioning        Accessing leading and training rows with lead() and lag()         Window frames        Accessing first, last and Nth values        ROWS vs RANGE using array_agg()        Using aggregations in window functions            Running time: ~25 min.      ### Array and map functionsMany data stores allow to to create arrays, but it isn&#39;t always easy.{{site.terms.sb}} allows you to easily create arrays and maps with your data.Creating arrays with your data is easy:```sqlSELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]```SQL array indexes are 1-based. Learn more about how to use and manipulate themin this in-depth video.             {% include youtubeSnippet.html id=page.youtubeId1 start=4067 end=5214 %}              Topics:              Accessing array and map elements with element_at()        Sorting arrays with array_sort()        matching elements with any_match(), all_match() and none_match()        Filtering elements        Transforming elements        Converting arrays to strings        Computing array products        Unnesting arrays and maps        Creating maps from keys and values and an array of entry rows            Running time: ~19 min.      ### Using JSON             {% include youtubeSnippet.html id=page.youtubeId1 start=1116 end=1969 %}              Topics:              The JSON data type        Extraction using json_extract() and json_extract_scalar()        Casting and partial casting from JSON        Formatting as JSON            Running time: ~14 min.      "
 },

 {
  "title": "Apache Superset",
  "url": "/data-consumer/clients/superset.html",
  "content": "# {{ page.title }}You can use [Apache Superset™](https://superset.apache.org/) as well as thehosted cloud service [Preset Cloud](https://preset.io/product/) with{{site.terms.sep_first}} to explore and visualize your data. Superset is a dataexploration and visualization web application. It is typically run on a serverand used by many users accessing the application. It enables users to write SQLqueries, create new tables, visualizations and dashboards and download data intofiles.  {% include video.html content=&quot;Check out the Trino CommunityBroadcast interview with the Apache Superset team to learn more and see ademo.&quot; %}## RequirementsApache Superset has built-in support for {{site.terms.sep}}.{{site.terms.sep}} Versions 350-e and higher can use the preferred mechanism ofa [trino connection](https://superset.apache.org/docs/databases/trino). It ispowered by the [SQLAlchemy driver](https://pypi.org/project/sqlalchemy-trino/)and allows full usage of SQL as supported by {{site.terms.oss}}, and therefore{{site.terms.sep}}.Older versions need to use the legacy support with [prestoconnection](https://superset.apache.org/docs/databases/presto). It uses the[PyHive driver](https://pypi.org/project/PyHive/) and therefore does not supportall supported SQL syntax.{% include caution.html content=&quot;Users are strongly advised to upgrade toSEP 350-e or higher to take advantage of the superior Trino support.&quot; %}## ConnectionGet the necessary [connection information](./index.html#connection-information)for your cluster:* Username* Password* Host* PortSuperset understands the {{site.terms.sep}} concept of catalogs. You can createa connection with or without a specified catalog:* trino://{username}:{password}@{hostname}:{port}/{catalog}* trino://{username}:{password}@{hostname}:{port}Once you have gathered this information, you can [create a newconnection](https://superset.apache.org/docs/creating-charts-dashboards/first-dashboard#connecting-to-a-new-database)in Superset:* Select **Data &gt; Databases**, and click the **+ Database** button.* Enter the connection string formed from your connection information.* Test the connection, if desired.* Click **Add**.## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is required to useTLS/HTTPS. If the best practice of using a globally trusted certificate isimplemented, you can use the HTTPS URL of the cluster in the connectionstring.If the certificate is not globally trusted, you need to [configure the{{site.terms.sep}} certificate for TLS/SSL access from Superset on the Supersetserver](https://superset.apache.org/docs/databases/extra-settings).## AuthenticationYou can configure the same authentication provider for Superset and{{site.terms.sep}}. As a result, the same credentials can be used to log in toeither system.Authentication providers supported by both systems include LDAP and OAuth2:* [Apache Superset](https://superset.apache.org/docs/security)* [{{site.terms.sep_full}}](../../latest/security.html)## Resources* [Apache Superset website](https://superset.apache.org/)* [Apache Superset documentation](https://superset.apache.org/docs/intro)* [Preset](https://preset.io/), Powerful, easy to use data exploration and  visualization platform, powered by Apache Superset* [Trino Community Broadcast interview with the Apache Superset team, including  live demo](https://trino.io/episodes/12.html)"
 },
 {
  "title": "Starburst Support",
  "url": "/support.html",
  "content": "                    {{site.terms.support}}        Don&#39;t panic. Help is just a click away!                                      We love our customers!      At {{site.terms.sb}}, we offer our customers dedicated resources      for your organization with up to a 30 minute response time and 24×7      support from our team of experts.      Not a customer just yet? When you are ready to engage, Starburst is      here to help you chart a course, launch successfully, check in      regularly while you get started, and when you are in production.      Check it out!                                Get help with {{site.terms.sep_full}}                                                                              Your support cases              Our intuitive support portal offers customers fast ticket              submission and tracking, user-friendly forums, a knowledge              base, and easy downloads.                                                                                                    Our support policies              Learn more about support with fast response times for problems              and critical issues. Our expert engineers are ready to help you              whenever needed.                                                                                                    No login, no problem!              If you do not have a support portal login, contact us! And take              advantage of all the useful resources available in the              documentation.                                                              {{site.terms.sep_full}} releases and downloads                                                                              Supported releases              Learn about what versions we support, our release cycles,              and important support dates for specific versions.                                                                                                    Latest downloads              Get the latest long-term support (LTS) and short-term support              (STS) versions, and their clients.                                                             Use {{site.terms.sb}} on any of these ecosystems:                                                              Amazon AWS                                                                        Google Cloud                                                                        Microsoft Azure                                                                        Red Hat OpenShift                              "
 },
 {
  "title": "Tableau",
  "url": "/data-consumer/clients/tableau.html",
  "content": "# {{ page.title }}[Tableau](https://www.tableau.com) is a popular analytics tool with powerfuldata visualization capabilities. You can use it to access clusters in{{site.terms.sep}} or {{site.terms.sg}}.There are three ways to connect recent releasesof Tableau products to recent releases of a {{site.terms.sep_first}} cluster:  * **[Tableau data connector](#tableau-connector-with-jdbc-driver)**: The  recommended connection method uses a Tableau data connector file paired with  the {{site.terms.sb}} JDBC driver.* **[Legacy JDBC connection](#legacy-jdbc-driver-connection)**: For sites using  a plain JDBC connection using the legacy &quot;Other Databases (JDBC)&quot; method,  {{site.terms.sb}} strongly recommends migrating to the **[Tableau data  connector](#tableau-data-connector-with-jdbc-driver)** method, but  {{site.terms.sb}} also supports continued use of the legacy method.* **[ODBC connection](#odbc-driver-connection)**: {{site.terms.sb}} also  supports connecting to clusters with the {{site.terms.sb}} ODBC driver.## Before you begin1. Determine the [connection information](./index.html#connection-information)   for your cluster, including its network name, listening port (or [default   port](./index.html#default-ports)), and login credentials.2. Instructions to connect Tableau products to clusters vary slightly, depending   on the {{site.terms.sep}} version. See [Determine cluster   version](./index.html#determine-cluster-version).## Tableau data connector with JDBC driverYou can connect recent releases of Tableau Desktop to recent clusters with acombination of a Tableau data connector file and JDBC driver. This methodrequires:* {{site.terms.sg}} or {{site.terms.sep}} 354-e or later* Tableau Desktop 2020.4 or laterNote that a Tableau data connector is not the same as an {{site.terms.sep}}connector. The Tableau data connector is a bridge between Tableau and thestandard {{site.terms.sb}} JDBC driver. The two files work together to enableread-only access between Tableau and one or more {{site.terms.sep}} or{{site.terms.sg}} clusters. The Tableau data connector is a JAR file with namesimilar to ``StarburstEnterprise.taco``.The procedure to install and use the Tableau data connector depends on yourTableau Desktop version:* For Tableau Desktop 2021.2.0 and later, select {{site.terms.sep_full}}  directly from the **To a Server** menu and follow the [standard data connector  procedure](#standard-data-connector-procedure).* For Tableau Desktop 2020.4 to 2021.1, you must first download the data  connector file yourself, as described in  [download data connector](#download-data-connector).## Standard data connector procedureFor Tableau Desktop 2021.2.0 or later, follow these steps:1. If Tableau Desktop is open, close it and exit.2. Download the latest {{site.terms.sb}} [JDBC   driver](../../latest/installation/jdbc.html).3. Place the JDBC driver file in the Tableau drivers directory:        Windows     C:Program FilesTableauDrivers     macOS     /Users/username/Library/Tableau/Drivers      Do not store more than one {{site.terms.sb}} JDBC driver in this directory.   Delete any older drivers when you update to a newer version. Connections to   all connected data sources are made through the JDBC driver.4. Start Tableau Desktop. In the left column, under **To a Server**, click   **More**.5. In the list of server types, select **{{site.terms.sep_full}} by Starburst**.   This opens a **Connector Details** dialog that describes the data connector.6. Click **Install**. This opens a connection dialog:   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_initial-login-screen.png&#39;   img-id=&#39;tableau-initial-login&#39;   alt-text=&#39;Tableau Connector login dialog&#39;   descr-text=&#39;Image depicting Tableau Connector login dialog&#39;   screenshot=&#39;true&#39;   %}7. Fill in the connection dialog with the following parameters:                     Field        Value                            Server        Hostname or IP address of your cluster.                    Port        Port used by your cluster.                    Authentication        Use the drop-down list to select among three options:            Username for a cluster with no authentication,            Username and Password for a cluster with TLS            enabled and a password-based authentication method such as LDAP,            or Kerberos.                    Username        Your username for {{site.terms.sep}} or {{site.terms.sg}}.                    Password        (If selected) the password for the specified            Username.                    Require SSL        Select this check box if your cluster has TLS enabled. When            selected, the following field appears.                    SSL Verification        Select an entry in the drop-down list to specify how rigorously            the server’s certificate is to be validated.                If you select **Kerberos** in the **Authentication** drop-down, see [Kerberos   authentication](#kerberos-authentication).   The values for **SSL Verification** have the following meanings:        FULL     Confirm that the certificate&#39;s validity is chained all the way back         to a root Certificate Authority (CA).         CA     Confirm that the certificate is valid as far back as the included         intermediate CA.         NONE     Confirm that the server&#39;s certificate matches the DNS name and private         key of the server.          8. When the connection is made, the **Connections** panel shows a list of the   catalogs (data sources) configured in your cluster.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_connections_catalogs.png&#39;   img-id=&#39;tableau-initial-catalogs-list&#39;   alt-text=&#39;Tableau Connector list of catalogs&#39;   descr-text=&#39;Image depicting Tableau Connector list of catalogs&#39;   screenshot=&#39;true&#39;   %}9. Select a catalog and subsequent schema from that catalog, to see the   available tables.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_connections_catalog_schema_table.png&#39;   img-id=&#39;tableau-list-of-tables&#39;   alt-text=&#39;Tableau list of tables&#39;   descr-text=&#39;Image depicting Tableau Connector list of tables&#39;   screenshot=&#39;true&#39;   %}If your cluster and catalogs support it, you can use the **Initial SQL** panelto specify one or more SQL statements to run on connection to the cluster. Seethe **Learn More** link in Tableau for information on the limitations of thisfeature.### Kerberos authenticationAfter selecting **Kerberos** in the **Authentication** drop-down, additionalconnection options appear in the Connection dialog.{% include image.html  url=&#39;../../assets/img/general/Tableau_connections_kerberos.png&#39;  img-id=&#39;tableau-connections-kerberos&#39;  alt-text=&#39;Tableau Connector login dialog with Kerberos authentication&#39;  descr-text=&#39;Image depicting Tableau Conenctor login dialog with Kerberos authentication&#39;  screenshot=&#39;true&#39;%}The first four fields including **Username** are the same as described above.The Kerberos-specific fields are described in the following table:            Field      Value                  Kerberos Principal      The Kerberos principal to use when authenticating to the SEP          coordinator. If not set, the value is read from the keytab file.              Kerberos Remote Service Name      SEP coordinator Kerberos service name. This parameter is required          for Kerberos authentication.              Kerberos Service Principal Pattern      SEP coordinator Kerberos service principal pattern.          The default is ${SERVICE}@${HOST}. ${SERVICE} is replaced with          the value of Kerberos Remote Service Name          and ${HOST} is replaced with the hostname of the coordinator          (after canonicalization, if enabled).              Kerberos configuration file path      Local path to the Kerberos configuration file.              Kerberos keytab file path      Local path to the Kerberos keytab file.      Use the same paths for the keytab and configuration file are used afterpublishing a SEP data source to Tableau Server. If they are not specified,default locations are used.To troubleshoot Kerberos:* Set the `JAVA_TOOL_OPTIONS` environment variable on the Tableau host to  include:  ```   -Dsun.security.krb5.debug=true -Dtrino.client.debugKerberos=true    -Djava.security.debug=gssloginconfig,configfile,configparser,logincontext  ```* Check the Tableau log files that have `jprotocolserver` in their name, located  in the `datatabsvclogsdataserver` directory on the Tableau server.For further information on configuring and troubleshooting Kerberos on Tableauproducts, see:* [Kerberos](https://help.tableau.com/current/server/en-us/kerberos.htm)* [Enable Kerberos Run As Authentication for JDBC Connectors](https://help.tableau.com/current/server/en-us/kerberos_runas_jdbc.htm)* [Enable Kerberos Delegation for JDBC Connectors](https://help.tableau.com/current/server/en-us/kerberos_delegation_jdbc.htm)* [Troubleshoot Kerberos](https://help.tableau.com/current/server/en-us/kerberos_trouble.htm)### Customized JDBC connectionsIf you need to set additional connection properties that are not included inTableau&#39;s connection dialog, customize the connection using a properties file.For more information, see [Customize JDBC Connections Using a PropertiesFile](https://community.tableau.com/s/question/0D54T00000F339uSAB/customize-jdbc-connections-using-a-properties-file?_ga=2.13850551.1453931082.1617841062-354148074.1617841061)in the Tableau Community and the [list of available parameters for the JDBCdriver](../../latest/installation/jdbc.html#parameter-reference).## {{site.terms.sb}} advantageRemember that {{site.terms.sep_full}} and {{site.terms.sg}} are not a database.They are SQL query engines that can connect to multiple data sources at the sametime. Each  cluster can query multiple catalogs in a wide range of differentdata sources.Although Tableau is typically configured for one specific catalog and schema, itis possible to query more than one data source with a single Tableau connection.To query multiple catalogs, select **New Custom SQL** in Tableau, and thenreference the fully-qualified name of any table in the cluster using the full`catalog.schema.table` syntax.The following example query accesses four catalogs: `postgresql`, `hive`,`mysql`, and `sqlserver`.```sqlSELECT c.custkey  , c.state  , c.estimated_income  , cp.customer_segment  , a.cc_number  , pp.cc_type  , a.mortgage_id  , a.auto_loan_idFROM postgresql.burst_bank.customer cJOIN hive.burst_bank.account a on c.custkey = a.custkeyJOIN mysql.burst_bank.product_profile pp on a.custkey = pp.custkeyJOIN sqlserver.burst_bank.customer_profile cp on c.custkey = cp.custkeyWHERE c.country = &#39;US&#39;AND c.state NOT IN (&#39;AA&#39;, &#39;AE&#39;, &#39;AP&#39;)```This approach is faster because all data access is managed by {{site.terms.sep}}or {{site.terms.sg}}, and is executed on the cluster. Tableau can also join datadirectly from multiple data sources, but this creates an unnecessary workload onTableau, and can negatively impact Tableau performance.## Download data connectorFor Tableau Desktop 2020.4 through 2021.1, the Tableau data connector for{{site.terms.sep_full}} does not appear automatically in the **AdditionalConnectors** list in the **To a Server** menu. To get {{site.terms.sep_full}} toappear in that list, either:* Upgrade Tableau Desktop to version 2021.2.0 or later, or* Prepare your Tableau installation as described here before attempting to  connect to a cluster.Follow these steps to manually download the Tableau data connector for{{site.terms.sep_full}}:1. Close and exit Tableau Desktop.2. From the [Starburst Enterprise   page](https://extensiongallery.tableau.com/connectors/274) of Tableau&#39;s   web-based Extension Gallery, download the Tableau data connector file whose   name is similar to ``StarburstEnterprise.taco``. Use the **Download** button   on the upper right of the page. This requires logging into Tableau&#39;s site   with a free login.3. Move the data connector file to:        Windows     C:UsersusernameDocumentsMy Tableau RepositoryConnectors     macOS     /Users/username/Documents/My Tableau Repository/Connectors   4. Proceed from here to the [standard data connector   procedure](#standard-data-connector-procedure).## Legacy JDBC driver connectionTableau provides a generic connection method titled **Other Databases (JDBC)**.Although {{site.terms.sb}} strongly recommends using a [Tableau dataconnector](#tableau-data-connector-with-jdbc-driver), this generic JDBCconnection method is still available. Follow these steps:1. Download the {{site.terms.sb}} JDBC driver according to your   {{site.terms.sep}} version.   * For {{site.terms.sg}} or {{site.terms.sep}} 354-e and later, download the     [latest JDBC driver version](../../latest/installation/jdbc.html).   * For {{site.terms.sep}} 350-e or older, download     [``presto-jdbc-350.jar``](../../350-e/installation/jdbc.html).2. Place the JDBC driver file in the Tableau drivers directory:        Windows     C:Program FilesTableauDrivers     macOS     /Users/username/Library/Tableau/Drivers      Do not store more than one {{site.terms.sb}} JDBC driver in this directory.   Delete any older drivers when you update to a newer version. Connections to   all connected data sources are made through the {{site.terms.sb}} JDBC   driver.3. Start Tableau Desktop and select **Connect to a server** using the   **Other Databases (JDBC)** connector in Tableau.4. Fill in the connection dialog as shown in the following table. For the URL   field, if you downloaded the Trino JDBC driver file to connect to    {{site.terms.sg}} or newer {{site.terms.sep}} versions, use a JDBC connection   string in this format:   ```shell   jdbc:trino://cluster.example.com:8080/catalog?SSL=true   ```   If you downloaded the PrestoSQL JDBC driver file to connect to older   {{site.terms.sep}} versions, use a JDBC connection string in this format:   ```shell   jdbc:presto://cluster.example.com:8080/catalog?SSL=true   ```   For either driver, the JDBC connection string must include the initial   catalog to connect to. Once connected, you can select schemas and tables   within that catalog, or select a different catalog.   If your cluster has TLS enabled, append the property ``?SSL=true`` to the   connection string.                        Field         Value                                 URL         Full JDBC connection string for your cluster. Must include a catalog             name.                       Dialect         Must be ‘SQL92’                       Username         Your username                       Password         Your password                       Properties File         Specify or browse to the path of a JDBC properties file containing             further specifications for this connection. See            Customized JDBC Connections.                           &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_other-databases-dialog-catalog.png&#39;   img-id=&#39;tableau-other-databases&#39;   alt-text=&#39;Tableau Other Databases login dialog&#39;   descr-text=&#39;Image depicting Tableau Other Databases login dialog&#39;   screenshot=&#39;true&#39;   %}## ODBC driver connectionContact [Starburst Support](../../support.html) to obtain access to the[Starburst ODBC driver](./odbc.html). The Presto ODBC driver provided by Tableauis not supported.{% include note.html content=&quot;Instead of connecting with ODBC, the preferredconnection method is to use the Tableau data connector plus JDBC driver, asdescribed in the preceding section, [standard data connectorprocedure](#standard-data-connector-procedure).&quot; %}Open Tableau and begin the ODBC configuration. On Tableau&#39;s startup page, select**Other Databases (ODBC)**, and configure as follows:* Driver: Starburst ODBC Driver* Username: ``* String Extras: `Driver=Starburst ODBC    Driver;Catalog=;Host=;Port=;`The `String Extras` field supports any of the ODBC connection properties fromthe Starburst ODBC driver.Select **Sign In** to establish the connection. If you are prompted for apassword, the {{site.terms.sep}} server has authentication enabled.The [Tableau ODBC documentation](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases.htm)contains further information."
 },
 {
  "title": "Metadata tags for page front matter",
  "url": "/internal/tag-list.html",
  "content": "# {{page.titles}}The tags in this document are adopted for use at this time. They are used asmetadata to collect and list pages.## searchindex tagIf set to `false`, the page is not included in the simple search, see`search.json`.## persona tagValues:* platform-administrator* data-engineer* data-consumer* data-leader## product tagValues:* sep* galaxy* marketplace* amazon-marketplace* azure-marketplace* google-marketplace* redhat-marketplace## media-type tagValues:* text* video* graphics* screenshot"
 },
 {
  "title": "Try Starburst Enterprise with Tarball",
  "url": "/starburst-enterprise/try/tarball.html",
  "content": "# {{page.title}}You can install {{site.terms.sep_first}} on any 64-bit Linuxdistribution with the tarball.Perform the following steps:1. Obtain the most recent {{site.terms.sep}} `tar.gz` archive2. Unpack the archive as root3. Add configuration files4. Start the {{site.terms.sep}} server5. Obtain the {{site.terms.oss}} CLI client and run testsIf you are managing clusters on a [bare-metal](../../glossary.html) server or[virtual machine](../../glossary.html), consider installing {{site.terms.sep}}with [{{site.terms.sbadmin}}](../starburst-admin/index.html).## Prerequisites{{site.terms.sep_full}} requires a Linux distribution that:* Is no more than a few years old* Runs on 64-bit Intel hardware* Has Python 2.7 or later, needed to run the `launcher` utility.* Has Java 11.0.11 or a later Java 11 LTS release from Azul, OpenJDK, or Oracle  Java distributions. Newer Java releases may work but are not tested  or supported.{{site.terms.sep}} also installs *for evaluation purposes only* on a macOSrelease with the same prerequisites.## Download an {{site.terms.sep}} archiveTo gain access to {{site.terms.sep}} archives, existing customers contact{{site.terms.support}}. If you are not a customer yet, visit the[{{site.terms.sb}} website](https://www.starburst.io) and click either the **GetStarted** or **Download Free** buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form *using a valid email address*, then click **Free Download**.Open your email from {{site.terms.sb}} and click the link to the downloads page.{% include note.html content=&quot;You can optionally reply to the email fromStarburst to request a trial license that unlocks the performance and securityenhancements of Starburst Enterprise. However, to get a server up and runningquickly, you can postpone using a license for now.&quot; %}The Downloads page is organized into a Long-Term Support section at the top withSteps 1 and 2, and a Short-Term Support section at the bottom. Use the LTSbuttons.From the **Step 1: Starburst Enterprise** section, click the **Tarball** button.This starts the download of a file named with the pattern`starburst-enterprise-*.tar.gz`. If prompted to open or save the file, save itto your `/home//Downloads` directory.Next, from the **Step 2: Client applications** section, click the **CLI**button. This starts the download of `trino-cli-*-.executable.jar`; if prompted,save it to your `/home//Downloads` directory for later use.{% comment %}If you prefer to use the RPM installation type on a Red Hat family Linux, see[cross-ref-TBD].{% endcomment %}## Unpack the archiveThe contents of the `tar.gz` archive are by default owned by root and include atop-level container directory. The `--strip 1` option in the `tar` command shownhere strips that container directory off.First create an empty target directory, then extract the contents of the`tar.gz` file without its container directory into the target directory. Forexample:```shellcd /home//Downloadssudo mkdir -p /opt/starburstsudo tar xvzf ./starburst-enterprise-*.tar.gz --strip 1 -C /opt/starburst```You can replace the asterisk (*) in the file name with the version of{{site.terms.sep}} that you downloaded, such as `356-e.1`. The commandworks as shown, without the version string.The directory `/opt/starburst` is called the *installation directory*. Inspectthe new directory to find that it contains two top-level files and fourdirectories:```textNOTICEREADME.txtbinlibpluginstarburst-insights```## Add configuration filesEven small a {{site.terms.sep}} server must have a minimum set ofconfiguration files before it can start. To add those files, create a directoryin `/opt/starburst` named `etc`, parallel to `bin` and `lib`. Populate `etc`with the following configuration files, using contents suggested in [Configuring{{site.terms.oss}}](../../latest/installation/deployment.html#configuring-trino).  node.properties  Follow the sample in      Node      properties. For the node.properties line, the suggested      value production has no special meaning; use any value, such as      test.  jvm.config  Follow the sample in JVM      config. Make sure you are viewing the      deployment page      that matches the {{site.terms.sep}} version you downloaded, then use the      suggested text verbatim. An exception is to adjust the -XmX      value as appropriate for your test environment.  config.properties  Follow the sample in            Config properties. Use the third example in that section to specify a      combined coordinator and worker machine.  catalog property files  Create a subdirectory etc/catalog. In the catalog      subdirectory, create the following files, each containing a single line of      text.      See       Catalog properties for guidance.                        File name          Contents                          blackhole.properties          connector.name=blackhole                          jmx.properties          connector.name=jmx                          memory.properties          connector.name=memory                          tcpds.properties          connector.name=tcpds                          tpch.properties          connector.name=tpch                Start on [Configure and define catalogs](../../data-engineer/catalogs.html) tolearn more about the relationship between data sources, catalogs, andconnectors.## Alternate configuration filesAnother way to get started quickly is to use the set of configuration filesprovided as examples for the O&#39;Reilly book [Trino: The DefinitiveGuide](https://www.starburst.io/info/oreilly-trino-guide/).To use these ready-to-use configuration files, download the samples from their[GitHub location](https://github.com/trinodb/trino-the-definitive-guide)either as a zip file or a Git clone. Let&#39;s say you place your clone or unzipdirectory in `/home//bookfiles`.From `bookfiles`, copy the entire `etc` directory from the `single-installation`sample folder to your `installation` directory. For example:```shellcd /home//bookfiles/single-installationsudo rsync -av etc /opt/starburst/```## Start the serverOnce configuration files are in place, start the server. From the `installation`directory, run the following command:```shellsudo bin/launcher start```For a successful server start, the launcher script returns a process ID. Checkthe server&#39;s status to make sure the server finished its startup process:```shellsudo bin/launcher status```As an alternative, look for the exact phrase &quot;SERVER STARTED&quot; in the`server.log` file, which by default is in the `var/log` subdirectory of theinstallation directory:```shellgrep &quot;SERVER STARTED&quot; /var/log/server.log```## Verify the serverTo verify that your locally-run server is operating as expected, invoke the{{site.terms.oss}} UI as described in [Verify theserver](./verify.html).## Run queriesTo run queries against your server, use the {{site.terms.oss}} CLI as describedin [CLI](../../data-consumer/clients/cli.html)."
 },
 {
  "title": "ThoughtSpot",
  "url": "/data-consumer/clients/thoughtspot.html",
  "content": "# {{ page.title }}[ThoughtSpot Cloud](https://thoughtspot.com) is a browser-based analytics enginethat lets you build queries by selecting the table columns that you wantanalyzed. ThoughtSpot then generates SQL code to implement that query, sends theSQL to the data source, then returns results in grid or graphical form.ThoughtSpot generated queries integrate well with tables and views managed by{{site.terms.sep_full}}.  ## RequirementsThe requirements for using the {{site.terms.sb}} data connector in ThoughtSpotCloud:- Users of ThoughtSpot Cloud must connect to a cluster running  {{site.terms.sep_first}} 354-e or newer.- To take advantage of [joining tables in SEP](#join-in-sep) instead of in  ThoughtSpot, your {{site.terms.sep}} cluster must include a [Hive  connector](../../latest/connector/hive.html) that uses a Hive metastore  service.- Other ThoughtSpot Cloud requirements are described in the [ThoughtSpot  documentation](https://cloud-docs.thoughtspot.com/admin/ts-cloud/ts-cloud-requirements-support.html).## Add a {{site.terms.sep}} connectionUse the following steps to connect to ThoughtSpot and add your{{site.terms.sep}} cluster as a data source:1. Determine the [connection information](./index.html#connection-information)   for your {{site.terms.sep}} cluster, including its network name, port, and   your login credentials.2. Using a [supported web   browser](https://cloud-docs.thoughtspot.com/end-user/accessing.html#supported-web-browsers),   log into ThoughtSpot using the URL and login credentials provided for your   site by your network administrators. If you are evaluating ThoughtSpot and   have trial login credentials, use ``try.thoughtspot.cloud``.3. To add a connection to your {{site.terms.sep}} cluster, select **Data** in the   navigation bar.4. Select the **Connections** tab, then **Add a connection**.5. In the **Choose your data warehouse** page, in the first field, **Name your   connection**, such as ``{{site.terms.sep}} mirror cluster``. Optionally enter   a connection description.6. Select the **{{site.terms.sb}}** tile and click **Continue**.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/thoughtspot-01-choose-data-warehouse.png&#39;   alt-text=&#39;ThoughtSpot choose connection page&#39;   descr-text=&#39;ThoughtSpot choose connection page&#39;   pxwidth=&#39;666&#39;   screenshot=&#39;true&#39;   %}7. Enter the connection details for your {{site.terms.sep}} cluster. Leave the   **Database** field empty.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/thoughtspot-02-connection-details.png&#39;   alt-text=&#39;ThoughtSpot connection details page&#39;   descr-text=&#39;ThoughtSpot connection details page&#39;   pxwidth=&#39;535&#39;   screenshot=&#39;true&#39;   %}8. If your {{site.terms.sep}} cluster uses secure TLS/HTTPS connections, open   the **Advanced Config** drop-down. In the first row of **Key** and **Value**   fields, enter ``SSL`` and ``true``.9. Click **Continue**.10. In the **Select tables** page, ThoughtSpot connects to your cluster and    returns a list of catalogs configured for your cluster. Open a catalog    entry to see the schemas and tables it contains.    &amp;nbsp; {% include image.html    url=&#39;../../assets/img/general/thoughtspot-03-select-tables.png&#39;    alt-text=&#39;ThoughtSpot select tables page&#39;    descr-text=&#39;ThoughtSpot select tables page&#39;    pxwidth=&#39;216&#39;    screenshot=&#39;true&#39;    %}11. Use the check boxes to select one or more tables on the left and columns on    the right. Choose a set of tables and columns from which you can make    meaningful queries by dragging column names into ThoughtSpot&#39;s Search field.    You can return to this page under a different connection name to select    another set of tables and columns to support a different set of queries.    Therefore, do not select in this named connection every table you might be    interested in querying.    {% include note.html content=&quot;After you click **Confirm**, the ThoughtSpot       UI does not distinguish same-named tables from different catalogs. In       general, try to select a collection of tables with unique names, unless       you are configuring table joins in ThoughtSpot. However, see [Join tables       in SEP](#join-in-sep) for our alternative way to handle table joins.&quot;    %}    &amp;nbsp; {% include image.html    url=&#39;../../assets/img/general/thoughtspot-04-select-table-columns.png&#39;    alt-text=&#39;ThoughtSpot select table columns page&#39;    descr-text=&#39;ThoughtSpot select table columns page&#39;    pxwidth=&#39;417&#39;    screenshot=&#39;true&#39;    %}12. Click **Confirm**. This adds a named connection that includes only the    tables and columns you selected.    You can now perform a live query on this connection, following the    instructions in [ThoughtSpot&#39;s    documentation](https://cloud-docs.thoughtspot.com).    To query a different set of tables and columns, create a different data    connection to the same cluster.## Join tables in {{site.terms.sep}}Remember that {{site.terms.sep_full}} is not a database, it&#39;s a SQL queryengine that can connect to multiple data sources at the same time. Each{{site.terms.sep}} cluster can query multiple catalogs in a wide range ofdifferent data sources.ThoughtSpot is capable of performing table joins when selecting tables andcolumns in the **Select tables** page. However, there is a significant speedadvantage in reserving table joins for {{site.terms.sb}} before the data reachesThoughtSpot. Let the {{site.terms.sep}} engine&#39;s distributed architecture do thejoin work.To do this, use a SQL editor client such as [{{site.terms.sb}}Insights](../../latest/insights/) or[DBeaver](/data-consumer/clients/dbeaver) to create a view from a ``SELECT``query that includes the joins of interest. Then select that view by name in the**Select tables** page like any other table.To serve as a location to store the views, your cluster must have a [Hivecatalog](../../latest/connector/hive.html) that implements aHive metastore service such as the AWS Glue Data Catalog. Then create your viewbefore creating your {{site.terms.sep}} connection in ThoughtSpot.The following example saves a view of a ``SELECT`` statement that accesses threecatalogs: ``postgresql``, ``hive``, and ``sqlserver``.```sqlCREATE VIEW hive.savedviews.income_by_customer_segment_vw ASSELECT c.custkey,       c.estimated_income,       c.fico,       o.risk_appetite,       l.cc_typeFROM glue.burst_bank.customer c   INNER JOIN postgresql.burst_bank.customer_profile o     ON c.custkey = o.custkey   INNER JOIN sqlserver.burst_bank.product_profile l     ON o.custkey = l.custkey;```## Resources- [**Lightning demo - ThoughtSpot and  Starburst**](../../videos/2021-05-19-thoughtspot-staburst-demo.html) Video, 8  minutes- {{site.terms.sb}} Guide: [Query multiple data sources](../data-mesh.html)- {{site.terms.sb}} Guide: [Starburst insights](../clients/insights.html)"
 },
 {
  "title": "TPC-H dataset",
  "url": "/starburst-galaxy/catalogs/tpcds.html",
  "content": "# {{page.title}}The TPC-DS dataset provides a set of schemas to support the [TPC Benchmark™ DS(TPC-DS)](http://www.tpc.org/tpcds/). TPC-DS is a database benchmark used tomeasure the performance of complex decision support databases.The dataset includes numerous schemas that only vary in the amount of data.This allows you to develop and test SQL queries with a small number of records,and then run the same query against a larger dataset.The benchmark also includes numerous queries that can be used as examples forlearning and improving SQL.All data is generated when requested from the catalog. You can use it as a datasource and use `CREATE TABLE AS SELECT` statements to copy the data into othercatalogs, and subsequently run queries against those catalog for variouspurposes, including testing and benchmarks as desired.## Configure a catalogTo create a catalog with the TPC-DS dataset, select **Catalogs** in the mainnavigation, and click **Configure a catalog**. Click on the **TPC-DS** button inthe **Select a dataset** section.{% include_relative name-description.md %}{% include_relative save.md %}"
 },
 {
  "title": "TPC-H dataset",
  "url": "/starburst-galaxy/catalogs/tpch.html",
  "content": "# {{page.title}}The TPC-H dataset provides a set of schemas to support the [TPC Benchmark™ H(TPC-H)](http://www.tpc.org/tpch/). TPC-H is a database benchmark used tomeasure the performance of highly-complex decision support databases.The dataset includes numerous schemas that only vary in the amount of data.This allows you to develop and test SQL queries with a small number of records,and then run the same query against a larger dataset.The benchmark also includes numerous queries that can be used as examples forlearning and improving SQL.All data is generated when requested from the catalog. You can use it as a datasource and use `CREATE TABLE AS SELECT` statements to copy the data into othercatalogs, and subsequently run queries against those catalog for variouspurposes, including testing and benchmarks as desired.## Configure a catalogTo create a catalog with the TPC-H dataset, select **Catalogs** in the mainnavigation, and click **Configure a catalog**. Click on the **TPC-H** button inthe **Select a dataset** section.{% include_relative name-description.md %}{% include_relative save.md %}"
 },
 {
  "title": "Universe Sitemap",
  "url": "/universe-sitemap.xml",
  "content": "            https://docs.starburst.io        {% for post in site.posts %}                    https://docs.starburst.io{{ post.url }}            {% endfor %}    {% for page in site.pages %}        {% if page.url == &#39;/search.json&#39;            or page.url == &#39;/sitemap.xml&#39;            or page.url == &#39;/universe-sitemap.xml&#39;            or page.url == &#39;/404.html&#39;            or page.url == &#39;/assets/css/style.css&#39;            or page.sitemap_exclude == &#39;y&#39;        %}             {% continue %}         {% endif %}                    https://docs.starburst.io{{ page.url | replace:&#39;/index.html&#39;,&#39;/&#39; }}            {% endfor %}"
 },
 {
  "title": "Usage and billing",
  "url": "/starburst-galaxy/account/usage-billing.html",
  "content": "# {{page.title}}The **{{page.title}}** section allows you to manage billing information. anddisplays statistical information about the usage of clusters in  your{{site.terms.sg}}.## OverviewYou can configure to use **Direct billing** or **Marketplace billing** in the**Overview** tab. In all cases the charges are consumption-based.### Direct billingDirect billing means that you receive a separate bills for {{site.terms.sg}}from {{site.terms.sb}}.### Marketplace billingMarketplace billing means that you are charged for {{site.terms.sg}} by thesupported cloud infrastructure providers and their marketplace billing system.This means that your {{site.terms.sg}} costs are potentially a line item in yourlarger invoices from the providers.## UsageThe **Usage** tab shows the total number of credits used, and the accumulatedrunning time for all clusters in the current month in the **MONTH-TO-DATECREDITS USED** and **MONTH-TO-DATE ACTIVE TIME** sections.The **Usage details** shows total running time **Uptime** and the amount of**Credits used** for all clusters. You can narrow the display down to individualclusters and change the date range.## BillingThe **Billing** tab shows details for your configured invoicing and billingsetup."
 },
 {
  "title": "Users",
  "url": "/starburst-galaxy/account/users.html",
  "content": "# {{page.title}}The **Users** section displays a list of all configured users in your{{site.terms.sg}} account. Use the edit icon on the left of each row to edit ordelete the user.Use the **Add user** button to create a new user. The **Email** for each userserves as login username. Each user is assigned one or more [roles andprivileges](./roles-privileges.html) and a password. The user receives an emailnotification and reset the password with the **Forgot password** button.Use the {{site.terms.sg}} [security system](../security/index.html) to restrictand control access of all users."
 },
 {
  "title": "Verify server with Web UI",
  "url": "/starburst-enterprise/try/verify.html",
  "content": "# {{page.title}}To verify that your locally run {{site.terms.sep_full}} server is running, usethe Web UI.Open the Web UI---------------With any modern browser, go to [http://localhost:8080](http://localhost:8080).At the login screen, enter your current OS login name (or any string).{% include image.html  url=&#39;../../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}The default Web UI screen shows the version number, environment, and uptime ofthe server. The statistics fields show zeros until a query is run against theserver.{% include image.html  url=&#39;../../assets/img/general/web-ui-empty.png&#39;  img-id=&#39;webuiempty&#39;  alt-text=&#39;Empty Web UI screen&#39;  descr-text=&#39;Empty Web UI screen&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}Run queries-----------To run queries against the local server with the {{site.terms.oss}} CLI, see[CLI](../../data-consumer/clients/cli)."
 },
{
  "title": "Starburst Enterprise 365-e STS",
  "url": "/blog/2021-11-29-release-365.html",
  "date": "2021-11-29 00:00:00 -0600",
  "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris mattis quamfacilisis, tempor justo vel, bibendum mauris. Pellentesque sodales pulvinaraliquet. Aenean sed tellus augue. Nulla justo sem, auctor vitae arcu eu, mollisconvallis turpis. Integer faucibus rhoncus augue, et condimentum odio finibusnon. Etiam nec neque interdum, ornare massa vitae, interdum est. Ut felis enim,condimentum vitae pellentesque laoreet, molestie quis massa. Cras sit amet enimeu est sagittis fringilla. Phasellus laoreet erat quis eros consectetur iaculis.Morbi sagittis, est ut pretium tincidunt, lorem est laoreet tortor, nectristique risus erat a risus. Duis tempor pharetra nunc, sit amet ornare exrhoncus eget.Aliquam quis hendrerit magna. Curabitur vulputate viverra erat, infaucibus mi sollicitudin eu."
 },{
  "title": "November 2021 LTS backport release",
  "url": "/blog/2021-11-19-lts-backports.html",
  "date": "2021-11-19 00:00:00 -0600",
  "content": "We did it again! A bunch of new backport releases for our Starburst EnterpriseLTS users are available.Following are the new available versions linking to the relevant release notesentries:  350-e.17  356-e.10  360-e.7  360-e.4Of course the related Helm charts and other resources are all available as partof the update.And if you are still running 345-e, it is time to upgrade. We no longer backportto it as it reached its end of life date."
 },{
  "title": "Starburst Enterprise 364-e LTS",
  "url": "/blog/2021-11-16-release-364.html",
  "date": "2021-11-16 00:00:00 -0600",
  "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris mattis quamfacilisis, tempor justo vel, bibendum mauris. Pellentesque sodales pulvinaraliquet. Aenean sed tellus augue. Nulla justo sem, auctor vitae arcu eu, mollisconvallis turpis. Integer faucibus rhoncus augue, et condimentum odio finibusnon. Etiam nec neque interdum, ornare massa vitae, interdum est. Ut felis enim,condimentum vitae pellentesque laoreet, molestie quis massa.Cras sit amet enim eu est sagittis fringilla. Phasellus laoreet erat quis erosconsectetur iaculis. Morbi sagittis, est ut pretium tincidunt, lorem est laoreettortor, nec tristique risus erat a risus. Duis tempor pharetra nunc, sit ametornare ex rhoncus eget. Aliquam quis hendrerit magna. Curabitur vulputateviverra erat, in faucibus mi sollicitudin eu."
 },{
  "title": "Write the docs at Starburst",
  "url": "/blog/2021-02-16-write-the-docs.html",
  "date": "2021-02-16 00:00:00 -0600",
  "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris mattis quamfacilisis, tempor justo vel, bibendum mauris. Pellentesque sodales pulvinaraliquet. Aenean sed tellus augue.Nulla justo sem, auctor vitae arcu eu, mollis convallis turpis. Integer faucibusrhoncus augue, et condimentum odio finibus non. Etiam nec neque interdum, ornaremassa vitae, interdum est. Ut felis enim, condimentum vitae pellentesquelaoreet, molestie quis massa. Cras sit amet enim eu est sagittis fringilla.Phasellus laoreet erat quis eros consectetur iaculis. Morbi sagittis, est utpretium tincidunt, lorem est laoreet tortor, nec tristique risus erat a risus.Duis tempor pharetra nunc, sit amet ornare ex rhoncus eget. Aliquam quishendrerit magna. Curabitur vulputate viverra erat, in faucibus mi sollicitudineu."
 },{
  "title": "Personas, who are you?",
  "url": "/blog/2021-02-13-personas.html",
  "date": "2021-02-13 00:00:00 -0600",
  "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris mattis quamfacilisis, tempor justo vel, bibendum mauris. Pellentesque sodales pulvinaraliquet. Aenean sed tellus augue. Nulla justo sem, auctor vitae arcu eu, mollisconvallis turpis. Integer faucibus rhoncus augue, et condimentum odio finibusnon. Etiam nec neque interdum, ornare massa vitae, interdum est. Ut felis enim,condimentum vitae pellentesque laoreet, molestie quis massa.Cras sit amet enim eu est sagittis fringilla. Phasellus laoreet erat quis erosconsectetur iaculis. Morbi sagittis, est ut pretium tincidunt, lorem est laoreettortor, nec tristique risus erat a risus. Duis tempor pharetra nunc, sit ametornare ex rhoncus eget. Aliquam quis hendrerit magna. Curabitur vulputateviverra erat, in faucibus mi sollicitudin eu."
 },{
  "title": "Training videos for everyone",
  "url": "/blog/2021-02-12-training.html",
  "date": "2021-02-12 00:00:00 -0600",
  "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris mattis quamfacilisis, tempor justo vel, bibendum mauris. Pellentesque sodales pulvinaraliquet. Aenean sed tellus augue. Nulla justo sem, auctor vitae arcu eu, mollisconvallis turpis. Integer faucibus rhoncus augue, et condimentum odio finibusnon. Etiam nec neque interdum, ornare massa vitae, interdum est.Ut felis enim, condimentum vitae pellentesque laoreet, molestie quis massa. Crassit amet enim eu est sagittis fringilla. Phasellus laoreet erat quis erosconsectetur iaculis. Morbi sagittis, est ut pretium tincidunt, lorem est laoreettortor, nec tristique risus erat a risus. Duis tempor pharetra nunc, sit ametornare ex rhoncus eget. Aliquam quis hendrerit magna. Curabitur vulputateviverra erat, in faucibus mi sollicitudin eu."
 },{
  "title": "Launching our Starburst documentation universe",
  "url": "/blog/2021-02-11-launch.html",
  "date": "2021-02-11 00:00:00 -0600",
  "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris mattis quamfacilisis, tempor justo vel, bibendum mauris. Pellentesque sodales pulvinaraliquet. Aenean sed tellus augue. Nulla justo sem, auctor vitae arcu eu, mollisconvallis turpis.Integer faucibus rhoncus augue, et condimentum odio finibusnon. Etiam nec neque interdum, ornare massa vitae, interdum est. Ut felis enim,condimentum vitae pellentesque laoreet, molestie quis massa. Cras sit amet enimeu est sagittis fringilla. Phasellus laoreet erat quis eros consectetur iaculis.Morbi sagittis, est ut pretium tincidunt, lorem est laoreet tortor, nectristique risus erat a risus. Duis tempor pharetra nunc, sit amet ornare exrhoncus eget. Aliquam quis hendrerit magna. Curabitur vulputate viverra erat, infaucibus mi sollicitudin eu."
 },
 {
  "title": "",
  "url": "",
  "date": "",
  "content": ""
 }
]
