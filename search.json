[
 {
  "title": "Advanced SQL in Starburst",
  "url": "/videos/2020-07-29-advanced-presto-sql.html",
  "content": "Advanced SQL in StarburstHosts: David Phillips, Manfred MoserVideo date: 9 July 2020Running time: 2h14m    This training session is geared towards helping users understand how to run morecomplex and comprehensive SQL queries with Starburst. Delivered by DavidPhillips, this session covers the following topics:  Using JSON and other complex data types  Advanced aggregation techniques  Window functions  Array and map functions  Lambda expressions  Many other SQL functions and featuresDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.  Welcome - 0:00  General SQL Features - 9:45          Format function - 9:57      Case expressions - 12:48      Searched case expression - 13:34      IF expression - 14:24      TRY expression - 15:26      Lambda expression overview        Using JSON - 18:36          JSON data type - 19:13      Extraction using JSONPath - 23:11      Casting from JSON - 26:01      Parting casting from JSON - 27:07      Formatting as JSON - 29:40        Advanced Aggregation Techniques - 32:50          Counting distinct items - 33:06      Approximate percentiles - 36:02      Associated max value - 37:59      Associated max value using a row type - 38:39      Pivoting with conditional counting - 40:58      Pivoting with filtering - 42:08      Pivoting averages - 42:57      Aggregating a complex expression - 43:55      Aggregating into an array - 45:36      Aggregating into a lambda - 46:28      Order-insensitive checksums - 50:07      ROLLUP with single - 52:32      ROLLUP with multiple - 53:24      CUBE - 55:52      GROUPING SETS - 59:28        Array and Map Functions - 1:07:47          Creating arrays - 1:08:40      Accessing array elements - 1:09:07      Sorting arrays - 1:11:25      Matching elements - 1:13:26      Filtering elements - 1:14:57      Transforming elements - 1:16:25      Converting arrays to strings - 1:17:44      Computing array product - 1:19:45      Unnesting an array - 1:22:16      Unnesting an array with ordinality - 1:23:06      Creating maps - 1:23:44      Accessing map elements - 1:25:20      Unnesting a map - 1:26:16        Window Functions - 1:28:00          Window function overview - 1:28:58      Row numbering - 1:30:04      Row numbering order - 1:30:49      Row numbering with limit - 1:31:39      Rank - 1:32:30      Rank with ties - 1:33:11      Dense rank with ties - 1:33:54      Ranking without ordering - 1:34:20      Row numbering without ordering - 1:34:40      Assigning rows to buckets - 1:36:07      Percentage ranking - 1:37:12      Partitioning - 1:37:53      Partitioning on the same value - 1:39:26      Accessing leading and trailing rows - 1:40:12      Accessing leading and trailing rows with nulls - 1:42:56      Accessing leading and trailing rows without nulls - 1:43:56      Window frames - 1:45:13      Accessing the first value - 1:47:05      Accessing the last value - 1:47:35      Accessing the Nth value - 1:47:49      Window frame ROWS vs RANGE - 1:48:15      Rolling and total sum - 1:50:39      Partition sum - 1:52:00      "
 },
 {
  "title": "Understanding and tuning Starburst query processing",
  "url": "/videos/2020-08-12-query-performance.html",
  "content": "Understanding and tuning Starburst query processingHosts: Martin Traverso, Manfred MoserVideo date: 12 August 2020Running time: 2h09m    This training session is geared towards helping users understand howStarburst executes queries. That knowledge can help you improve queryperformance. For instance, the explain plan is a powerful tool. We explore howto access the explain plan and how to read it. We look at the work thecost-based optimizer performs and how you can potentially help run yourqueries even faster. Delivered by Martin Traverso, this session covers thefollowing topics:  Explain the EXPLAIN  Learn how queries are analyzed and executed  Understand what the optimizer does, including some of its limitations  Showcase the cost-based optimizerDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can skip tothat timestamp in the video player above.  Welcome - 0:00  Query lifecycle - 8:11          Parsing - 9:44      Analysis - 11:29      Planning - 15:25      Optimization - 16:44      Scheduling and execution - 18:50        Explain the EXPLAIN - 20:18          EXPLAIN command - 20:55      EXPLAIN vs EXPLAIN ANALYZE - 23:07      Fragment Structure - 24:06      Distribution - 26:55      Row layout - 29:37      Estimates - 31:54      Performance stats - 33:49      Exchanges - 39:33        Optimization - 41:10          Constant folding - 43:23      Predicate pushdown - 51:07      Predicate pushdown into connectors - 55:43      Predicate pushdown into the Hive connector - 1:08:24      Hive partition pruning - 1:10:18      Hive bucket pruning - 1:17:12      Row group skipping for ORC and Parquet - 1:19:17      Limit pushdown - 1:22:31      Partial limit pushdown - 1:26:11      Aggregation pushdown - 1:28:26      Skew - 1:33:41        Cost-based Optimizations - 1:39:01          Partitioned join - 1:41:41      Broadcast join - 1:44:01      Join type selection - Partitioned - 1:46:02      Join type selection - Broadcast - 1:47:07      Disabling cost-based optimizations - 1:48:10      Join reordering - 1:48:44      Table statistics - 1:50:21      Computing statistics - 1:51:19        Resources - 1:52:09"
 },
 {
  "title": "Securing Starburst Enterprise",
  "url": "/videos/2020-08-26-securely-deploy-presto.html",
  "content": "Securing Starburst EnterpriseHosts: Dain Sundstrom, Manfred MoserVideo date: 26 August 2020Running time: 2h07m    This training session is geared towards helping Starburst Enterprise platform (SEP) userssecurely deploy SEP at scale. We cover how to secureSEP as well as access to your underlying data. Delivered by DainSundstrom, this session covers the following topics:  Authentication, including password &amp;amp; LDAP Authentication  Authorization to access your data sources  Encryption including client-to-coordinator communication  Secure communication in the cluster  Secrets usage for configuration files including catalogsDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.  Welcome - 0:00  Tips and Notes - 5:06          Process for securing SEP - 7:34      What to secure - 11:02      Verify HTTP with the Web UI - 13:23      Verify HTTP with the CLI - 14:48                  CLI failures - 15:14                    Client to Server Encryption - 15:44                  Approaches for HTTPS - 15:58          HTTPS proxy or load balancer - 17:33          Add the SSL/TLS certificate to the coordinator - 20:28                          Inspect the PEM file - 22:40              Verify the PEM file certificate - 23:45              Verify the PEM private key - 26:08              Verify the JKS file - 26:38              Configure SEP - 27:59                                Verify HTTPS with the Web UI - 28:51          Verify HTTPS with the CLI - 29:36                          CLI failures - 29:46              CLI –insecure - 33:23                                          Authentication - 34:57                  Password file authentication - 36:08          LDAP Authentication - 41:19          Kerberos Authentication - 50:24          Client certificate authentication - 53:53          JSON Web Token authentication - 55:03          Multiple authenticators - 56:01          User mapping - 58:14                    Authorization - 1:00:08                  File-based system access control - 1:02:54                    Client to server summary - 1:07:23      Internal security and connector security - 1:18:14                  Securing the cluster itself - 1:18:30          Shared secret - 1:20:29                    Internal HTTPS - 1:23:58      Secrets Management - 1:27:53      Management Endpoints - 1:30:23      Hive Catalog Security - 1:33:29                  Hive Catalog Authorization - 1:34:45          Hive Metastore Authentication - 1:38:08          HDFS Authentication - 1:42:24          Hive Kerberos Debugging - 1:43:31          S3 Authentication - 1:45:53          Google Cloud Authentication - 1:49:31                    "
 },
 {
  "title": "Starburst cluster sizing, and performance tuning",
  "url": "/videos/2020-09-09-cluster-sizing-and-performance-video.html",
  "content": "Starburst cluster sizing, and performance tuningHosts: Dain Sundstrom, Manfred MoserVideo date: 9 September 2020Running time: 2h15m    This training session is geared towards helping users tune and size theirdeployment for optimal performance. Delivered by Dain Sundstrom, this sessioncovers the following topics:  Cluster configuration and node sizing  Memory configuration and management  Improving task concurrency and worker scheduling  Tuning your JVM configuration  Investigating queries for join order and other criteria  Tuning the cost-based optimizerDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.  Welcome - 0:00  General Strategy - 5:45  Baseline Advice - 11:53  Cluster Sizing / CPU and Memory - 14:57  Machine Sizing - 34:46          Memory - 34:58      Memory Allocations - 39:55      Shared Join Hash - 46:59      Distributed Join - 49:11      Skew - 50:58      Use bigger machines - 55:03      Machine Types - 58:40        Additional Thoughts - 1:03:20          Hash join vs (sort) merge join - 1:03:25      Spilling - 1:04:43      Small Clusters - 1:07:54        Tuning the Workload - 1:23:45          Query Plan - 1:24:52      Precomputing - 1:30:00      Connectors - 1:34:53        Hive Data Organization - 1:39:27          Organize the data for the Hive connector - 1:39:41      Hive Partitioning - 1:42:13      Hive bucketing - 1:43:59      Orc and Parquet - 1:46:26      File Size - 1:51:02      Bad Parquet Files - 1:53:13      Rewrite table with ORC writer - 1:54:29        Making Queries Faster - 1:55:37          What to look for in a query - 1:57:09      More hardware - 2:01:18      Under-utilization - 2:02:32      Hive Caching - 2:05:29        Sharing Resources / Resource Groups - 2:08:34"
 },
 {
  "title": "404",
  "url": "/404.html",
  "content": "                                  Oops ... 404        Don&#39;t panic. We brought a towel.        Try going home.        Or maybe you are looking for some specific documentation and resources:                  Starburst Enterprise          Starburst personas overview          Platform administrator          Data engineer          Data consumer                            &amp;nbsp;                                          Confused where you landed?          Go          straight to the Starburst Enterprise reference documentation!          A lot of new useful documentation is available here, but fear not,          all the comprehensive reference documentation for Starburst Enterprise is ready and constantly improving as always.                      Go there now!                                                    "
 },
 {
  "title": "Add a MySQL data source",
  "url": "/starburst-galaxy/data-sources/mysql/add-mysql.html",
  "content": "Add a MySQL data source  From the Data sources page, click Add a new data source or + New.  Select MySQL to begin adding your data source.Basic informationEnter the name and description you want for your data source. A unique catalogname allows you to tell it apart from other data sources and easily createqueries.  Enter a unique Catalog name for your data source. The naming requirementssupport lowercase letters, numbers, and underscores.  Add a Description to share more details about the data source thanjust the name.   Connection detailsEnter the information needed to connect with your MySQL database.  Add your Host and Port number.  The Connection URL auto-populates,you can choose to manually edit it.   AuthenticationTo authenticate, create your external secrets in yourAWS Secrets Manager. To ensure access, use the prefix starburst-galaxy-.  Enter your User information.      Add our AWS secret key name from your AWS Secrets Manager.       Click Connect data source.    Now that you’ve created a data source, you can createclusters.Navigate to the Data sources page to edit, delete, or add new data sourcesat any time."
 },
 {
  "title": "Add a PostgreSQL data source",
  "url": "/starburst-galaxy/data-sources/postgresql/add-postgresql.html",
  "content": "Add a PostgreSQL data source  On the Data sources page, click Add a new data source or + New.  Select PostgreSQL to begin adding your data source.Name and descriptionThe name and description of your data source allows you to tell it apart fromother data sources at a glance. Unique names are useful for SQL queries.  Enter a unique Catalog name for your data source. The naming requirementssupport lowercase letters, numbers, and underscores.  Add a Description to share more details about the data source thanjust the name.   Connection detailsEnter the information needed to connect with your PostgreSQL database.  Add your Host and Port number.  Enter the Database.  The Connection URL auto-populates,you can choose to edit it manually.   AuthenticationTo authenticate, create your external secrets inyour AWS Secrets Manager. To ensure access, use the prefix starburst-galaxy-.      Enter your User information.            Click Connect data source.      Now that you’ve created a data source, you can createclusters.Navigate to the Data sources page to edit, delete, or add new data sourcesat any time."
 },
 {
  "title": "Add an Amazon S3 data source with AWS Glue catalog",
  "url": "/starburst-galaxy/data-sources/s3/add-s3.html",
  "content": "Add an Amazon S3 data source with AWS Glue catalog  From the Data sources page, click Add a new data source or + New.  Select Amazon S3 + AWS Glue data catalog to begin adding your data source.Name and descriptionA unique catalog name is useful for SQL queries and a description allows you totell it apart from other catalogs.  Enter a unique Name for your data source. The naming requirements supportlowercase letters, numbers, and underscores.  Add a Description to share more details about the data source than justthe name.For example: &amp;lt;catalogname&amp;gt;_&amp;lt;schema&amp;gt;_&amp;lt;table&amp;gt;Authentication to AWS GlueAllow Starburst access to metadata and mapping information about theobjects stored in S3.External secrets must be created in your AWS SecretsManager and start with the prefix starburst-galaxy-.  Enter your AWS Access key ID.  Select the name of your secret key from your AWS Secrets Manager.Authentication to S3Choose and configure the authentication mechanism to connect to S3.Starburst Galaxy defaults to using the same authentication propertiesas AWS. The specific path to the storage is part of the Glue metadata andprovided during table creation.If you’re happy with the default S3 authentication mechanism:  Click Connect data source.Alternatively, you can add a different S3 authentication:  Enter your AWS Access key ID.  Select the name of your secret key from your AWS Secrets Manager.  Click Connect data source.Now that you’ve created a data source, you can create clusters.Navigate to the Data sources page to edit,delete, or add new data sources at any time."
 },
 {
  "title": "Trying Starburst Enterprise on any Linux",
  "url": "/starburst-enterprise/try/any-linux.html",
  "content": "Trying Starburst Enterprise on any LinuxYou can install Starburst Enterprise platform (SEP) on any 64-bit Linux distribution withthe following steps:  Obtain the most recent SEP tar.gz archive  Unpack the archive as root  Add configuration files  Start the SEP server  Obtain the Trino CLI client and run testsPrerequisitesStarburst Enterprise requires a Linux distribution that:  Is no more than a few years old  Runs on 64-bit Intel hardware  Has Python 2.7 or later, needed to run the launcher utility.  Has Java 11.0.7 or a later Java 11 LTS release from Azul, OpenJDK, or OracleJava distributions. Newer Java releases may work but are not testedor supported.(SEP also installs for evaluation purposes only on a macOSrelease with the same prerequisites.)Download an SEP archiveTo gain access to SEP archives, visit the Starburstwebsite and click either theGet Started or Download Free buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form using a valid email address, then click Free Download.A few moments later, you receive email from Starburst with a link to thedownloads page.    Note:   You can optionally reply to the email fromStarburst to request a trial license that unlocks the performance and securityenhancements of Starburst Enterprise. However, to get a server up and runningquickly, you can postpone using a license for now.The Downloads page is organized into a Long-Term Support section at the top withSteps 1 and 2, and a Short-Term Support section at the bottom. Use the LTSbuttons.From the Step 1: Starburst Enterprise section, click the Tarball button.This starts the download of a file named with the patternstarburst-enterprise-*.tar.gz. If prompted to open or save the file, save itto your ~/Downloads directory.Next, from the Step 2: Client applications section, click the CLIbutton. This starts the download of trino-cli-*-.executable.jar; if prompted,save it to your ~/Downloads directory. We will use this file later.Unpack the archiveThe contents of the tar.gz archive are by default owned by root and include atop-level container directory. The --strip 1 option in the tar command shownhere strips that container directory off.First create an empty target directory, then extract the contents of thetar.gz file without its container directory into the target directory. Forexample:cd ~/Downloadssudo mkdir -p /opt/starburstsudo tar xvzf ./starburst-enterprise-*.tar.gz --strip 1 -C /opt/starburstYou can replace the asterisk (*) in the file name with the version of StarburstEnterprise that you downloaded, such as 356-e.1. But the command works asshown, without the version string.The directory /opt/starburst is called the as the installationdirectory. Inspect the newdirectory to find that it contains two top-level files and four directories:NOTICEREADME.txtbinlibpluginstarburst-insightsAdd configuration filesEven the simplest Starburst Enterprise server must have a minimum set ofconfiguration files before it can start. To add those files, create a directoryin /opt/starburst named etc, parallel to bin and lib. Populate etcwith the following configuration files, using contents suggested in ConfiguringTrino.  node.properties  Follow the sample in      Node      properties. For the node.properties line, the suggested      value production has no special meaning; use any value, such as      test.  jvm.config  Follow the sample in JVM      config. Make sure you are viewing the      deployment page      that matches the SEP version you downloaded, then use the      suggested text verbatim. An exception is to adjust the -XmX      value as appropriate for your test environment.  config.properties  Follow the sample in            Config properties. Use the third example in that section to specify a      combined coordinator and worker machine.  catalog property files  Create a subdirectory etc/catalog. In the catalog      subdirectory, create the following files, each containing a single line of      text.      See       Catalog properties for guidance.                        File name          Contents                          blackhole.properties          connector.name=blackhole                          jmx.properties          connector.name=jmx                          memory.properties          connector.name=memory                          tcpds.properties          connector.name=tcpds                          tpch.properties          connector.name=tpch                Start on Configure and define catalogs tolearn more about the relationship between data sources, catalogs, andconnectors.Alternate configuration filesAnother way to get started quickly is to use the set of configuration filesprovided as examples for the O’Reilly book Trino: The DefinitiveGuide.To use these ready-to-use configuration files, download the samples from theirGitHub locationeither as a zip file or a Git clone. Let’s say you place your clone or unzipdirectory in ~/bookfiles.From bookfiles, copy the entire etc directory from the single-installationsample folder to your installation directory. For example:cd ~/bookfiles/single-installationsudo rsync -av etc /opt/starburst/Start the serverOnce configuration files are in place, start the server. From the installationdirectory, run the following command:sudo bin/launcher startFor a successful server start, the launcher script returns a process ID. Checkthe server’s status to make sure the server finished its startup process:sudo bin/launcher statusAs an alternative, look for the exact phrase “SERVER STARTED” in theserver.log file, which by default is in the var/log subdirectory of theinstallation directory:grep &quot;SERVER STARTED&quot; &amp;lt;installation&amp;gt;/var/log/server.logVerify the serverTo verify that your locally-run server is operating as expected, invoke theTrino UI as described in Verify theserver.Run queriesTo run queries against your server, use the Trino CLI as describedin CLI."
 },
 {
  "title": "Get started",
  "url": "/starburst-galaxy/aws-setup/aws-create-role.html",
  "content": "Get startedUsing Starburst Galaxy platform (SGP) requires you to configure your AWSaccount access for running the clusters.Configure your AWS accountYou need to configure a cross-account role for your AWS account. This enablesthe SGP to deploy and manage Starburst clusters inyour account.Create a cross account roleCreate a cross-account role using a cloud formation template (CFT) in theAWS management console.  In your AWS management console, go to IAM services. You may be prompted to sign in to AWS.  Download the Cloud formation template from Starburst Galaxy.  Upload the Cloud formation template in your AWS console.  Copy the External ID from Starburst Galaxy and paste it in yourAWS console.  Run the cloud formation template.  IMPORTANT: Copy the AWS Account ID where you created the role. Youneed it for the next steps.Enter the cross-account role informationAdd the cross-account information to Starburst Galaxy.  Navigate back to Starburst Galaxy  In the AWS Account ID field, enter the AWS Account ID from the roleyou created.  Select the default AWS region for your AWS resources.  Click Finish.You can edit your AWS region from Settings or on individual clusters asneeded."
 },
 {
  "title": "Configure and define catalogs",
  "url": "/starburst-enterprise/data-engineer/catalogs.html",
  "content": "Configure and define catalogsYou need to understand data sources and how they are connected toStarburst Enterprise platform (SEP), to take advantage of the query performance availableto your data consumers. The following content provides an overview of datasources and catalogs and how they work with connectors in SEP.DefinitionsBefore you begin, here are definitions and explanations the key concepts of fordata sources, catalogs, and connectors.Data sourcesA data source is a system where data is retrieved from. You can query systemssuch as distributed object storage, RBDMSs, NoSQL databases, document databases,and many others.In SEP, you must connect to a data source so you can query fromthat source. To query those sources, you need to create a catalog propertiesfile to define a catalog.CatalogsCatalogs define and name the configuration to connect to and query a datasource. They are a key concept inSEP.Without catalogs there is nothing to query.The catalog name is defined by the name of the catalog properties file, locatedin etc/catalog. You can have as many catalogs as you want, and there are norestrictions for naming outside of valid character types. For example, afilename of etc/catalog/mydatabase.properties results in the catalog namemydatabase.The catalog properties file content defines the access configuration to the datasource. Properties are key=value pairs.Each catalog defines one and only one connector using the requiredconnector.name property. You must select the correct connector for your datasource. For example, the PostgreSQL connector is defined by usingconnector.name=postgresql, and enables the catalog to access a PostgreSQLdatabase. Another example is the Hive connector defined byconnector.name=hive-hadoop2. It can enable a catalog to access Hadoop,Amazon S3 and many other object storage systems.    Note:   While a catalog can have only one connector, asingle connector may be used by multiple catalogs.Access a list of catalogs with theSHOW CATALOGS command.Configuration propertiesEach connector has a small set of required properties. While properties canvary, they minimally define the connection to the data source. Depending on yourconnector and data source, there are a number of different configurationproperties available.Optional properties enable further configuration of the catalog in areas such assecurity, performance, and query behavior. These connector-specific propertiesare defined in the documentation for each connector. For more information onconnector-specific configuration properties, start with the list of allconnectors.Catalog session propertiesYou can further customize the behavior of the connector in your catalog usingcatalog session properties. A session is defined by a specific user accessingSEP with a specific tool such as the CLI. Catalog sessionproperties can control resource usage, enable or disable features, and changequery processing.Most of the session properties are similarly named to their config propertiescounterparts in a catalog file, mostly differing by the use of underscores (_)in the name to be SQL compliant. Session properties override catalog propertiesin certain circumstances.You can view current session properties using theSHOW SESSION command. Thenimplement your session properties usingSET SESSION.ConnectorsA connector is specific to the data source it supports. It transforms theunderlying data into the SEP concepts of schemas, tables,columns, rows, and data types.Connectors provide the following between a data source and SEP:  Secure communications link  Translation of data types  Handling of variances in the SQL implementation, adaption to a provided API,or translation of data in raw filesSetup of connectorMost SEP connectors include their configurations and anythingelse you might need by default, and therefore no setup is required.If you’re using a connector that requires additional set up, such as theaddition of a proprietary JDBC driver, you find that documented with thespecific connector.Create a catalog properties fileYou can create catalog to access a data source with a few simple steps:  Create the catalog properties file in etc/catalog/, for exampleetc/catalog/mydatabase.properties  Specify the required connector with connector.name= in file, for exampleconnector.name=postgresql  Add any other properties required by the connector  Add any optional properties as desired  Copy the file onto the coordinator and all worker nodes  Restart the coordinator and all workers  Confirm the catalog is available with SHOW CATALOGS;  List the available schemas with SHOW SCHEMAS FROM mydatabase;  Start writing and running the desired queriesMany connectors use the similar properties in catalog properties files.Most JDBC-based connectors require these minimum properties for their catalogfiles:connector.name=[connectorname]connection-url=[connectorprotocol]//&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;;database=&amp;lt;database&amp;gt;connection-user=rootconnection-password=secretExamples of connector protocols include:# JDBC-based connectorconnection-url=jdbc:postgresql://example.net:5432/database# Endpoint-driven connectorhive.cos.service-config=etc/catalog/cos-service.properties"
 },
 {
  "title": "Choosing the right deployment",
  "url": "/choosing-the-right-deployment.html",
  "content": "Choosing the right deploymentStarburst Enterprise is available foron-premise usage or in private clouds, and can be run on bare metal servers,virtual machines and containers, all managed by you.You can also run Starburst Enterprise on public cloud provider systems, andtheir virtual machine or container offerings, or for further simplicity andconvenience use their marketplaces.That’s a lot of choice! This guide helps you select the best solution for yourorganization.Which product you choose depends on a couple of key factors:  People and available skills in your organization  Variety and location of data sources  Location of your computing resources  Security and governance requirementsPeopleA very important deciding factor for which Starburst product suits yourorganization best is whether or not your organization has necessary peopleavailable. You need the expertise of of platformadministrators and dataengineers to stand up, configure andmaintain clusters capable of running Starburst Enterprise, connect thedesired data sources, and ensure the data security and governance. Typically,people with these skill sets sit within your IT organization.Starburst Enterprise offers the mostcontrol of your deployments at the cost of being the most complex to install andmaintain.With limited availability using a public cloud provider or even marketplaceoffering can help reduce the workload.Data sourcesThe data sources you plan to query have a large impact on your choice. You needto understand what databases, object storage or others systems your users needto query. In addition, you need to know where these system are deployed toensure that Starburst Enterprise or Starburst Galaxy can accessthem with sufficient network performance and capacity.For example, if all your data is stored in your private network and data center,you should run Starburst Enterprise there as well. However, if all your datais hosted on a public cloud provider, you can choose to runStarburst Enterprise on the same cloud provider yourself, use a marketplaceoffering or even use Starburst Galaxy.Starburst Enterprise includes a very large variety ofconnectors to support the most common datasources. They include many RDBMSs, Hadoop/Hive and other object storage systems,and commercial platforms such as Snowflake or Teradata. In addition,Starburst Enterprise can be operated anywhere. If your organization needs toquery large collection of data sources, Starburst Enterprise is the rightchoice for you. You can choose various options on how to run it, and ensure itis located closely to your data sources.Starburst Galaxy only supports a limited set of datasources, and can only run in yourAWS account.LocationIf you run exclusively on-prem, then your choice is easy -Starburst Enterprise. You can run it on bare metal servers, on virtualmachines, in private clouds, or even in your private Kubernetes clusters. Yourchoice depends entirely on your requirements, data sources, people and skills.If you are using a single cloud provider, then your choice is easy. You cansimply run Starburst Enterprise, again with the options to manage the virtualmachines yourself and use the tar.gz or RPM archive.You can also use a Kubernetes offering from cloud providers, and combine it withthe our Kubernetes support with Helm charts.Alternatively, you can use cloud-specific offerings such our AmazonCloudformation support.Starburst also makes Starburst Enterprise available for all majormarketplaces.If you use a multi-cloud or hybrid cloud strategy, the locality of your datashould be your first consideration to reduce data transfer costs. You can evenrun multiple Starburst Enterprise clusters, and potentially connect them withthe Starburst RemoteconnectorHere are some things to consider in choosing a location for yourStarburst Enterprise cluster, keeping minimizing intra-cloud data transfer aslow as possible front-of-mind:  Can you place your cluster in a cloud that contains multiple data sources?  What intra-cloud data sources are most likely to be federated? Where is thelargest of them?  Is one of your cloud provider pricing models more favorable than the others?The closer you can put Starburst Enterprise to the bulk of your data, themore you can reduce the amount of data being returned, and save on data transfercosts.SecurityWhat does data security and governance look like at your organization?Starburst Enterprise supports a wide range of security features.  Different authentication platforms such as LDAP, Kerberos or OAuth  Data access control with user impersonation, credential passthrough and others  Authorization management with Ranger or Privacera platform  Event logger for auditing and tracking  and othersOur securityguideprovides a good introduction.The location and mode of running Starburst Enterprise specifically determineswhich security features can be used. You need to specifically verify if afeature can be used based on your determined location.For example, if your company relies on ApacheRanger or Privaceraplatform to secure your data,then a self-managed Starburst Enterprise in a private cloud or on-prem ismost likely the right choice for you.Depending on your particular security needs, one or more marketplace offeringsmay also work. Starburst Enterprise includes Helm charts for Apache Rangerinstallation and usage onKubernetes.Starburst GalaxyAs our most convenient solution, Starburst now offers a hosted andmanaged solution - StarburstGalaxy.Starburst Galaxy is the most hands-off approach as it provides asimple user interface and most complexity is completely taken care of for you byStarburst."
 },
 {
  "title": "CLI",
  "url": "/data-consumer/clients/cli.html",
  "content": "CLIThe Trino command line interface (CLI) provides a terminal-based,interactive shell for running queries and inspecting catalog structures in anySEP cluster.Setup to use the CLIThe CLI is distributed as an executable JAR file that you download, rename, andplace in a directory in the PATH.Connection informationGet the connection information and thecluster version for the cluster you wantto connect to and use to run queries.RequirementsThe CLI requires a java command on the PATH from Java 8 or newer. It isusually easiest to use the same Java 11 required by SEP itself,described on Java runtimeenvironment.The CLI needs to be compatible with the version of the cluster you areconnecting to.Download the CLIYou can request the CLI for a specific version fromStarburst Support.Alternatively, to gain direct access to SEP archives, visit theStarburst website and click either the GetStarted or Start Free buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form using a valid email address, then click Free Download.Click on the link to the Downloads page in the resulting email. It isorganized into Long-Term and Short-Term Support sections.Download a specific version of the CLI, based on your cluster version:  Version 350, if your cluster version is 350 or older  Same or newer version than the cluster for versions 354 and newerYou can also download the CLI binary straight from the link in the referencedocumentation:  Version 350  Version 354  Latest versionUse the version selector in the documentation for different releases, or contact|support|.Rename and place the CLIYou downloaded an executable JAR file that is usable as-is. To make it practicalwith having to call Java, copy it to a directory in the PATH, rename it, andmake it executable. For example:cd /usr/local/bincp -p /home/&amp;lt;yourname&amp;gt;/Download/trino-cli-*-executable.jar .mv trino-cli-*-executable.jar trinochmod +x trino    Note:   Older versions of the CLI use the name prestoinstead of trino for the command as well as the prompt.First stepsNow you are ready to verify that the CLI runs. Check if you get a similar outputto the following listing:$ trino --versionTrino CLI 354The CLI works. Now gather the connection information for your cluster, and connect to it. You can learn more about usingthe CLI from the following tutorial or the Command line interfacesection in the SEP referencedocumentation.CLI tutorialThe following sections provide a brief introduction to using the CLI.Interactive CLIAt the shell prompt, enter trino with no arguments. By default, this connectsto the running SEP server at the default address and port,localhost:8080.trinoIf your SEP server uses a different port or is running elsewhereon your network, specify the server’s URL with --server. For example:trino --server=cluster.example.com:8082This opens a Trino CLI shell, with trino&amp;gt; prompt. All commandsentered here must be terminated with a semicolon.Exit the CLI interactive mode with quit; or exit; or Ctrl+D:trino&amp;gt; quit;Usage helpTo see the available commands in interactive mode:trino&amp;gt; help;This returns:Supported commands:QUITEXPLAIN [ ( option [, ...] ) ] &amp;lt;query&amp;gt;    options: FORMAT { TEXT | GRAPHVIZ | JSON }             TYPE { LOGICAL | DISTRIBUTED | VALIDATE | IO }DESCRIBE &amp;lt;table&amp;gt;SHOW COLUMNS FROM &amp;lt;table&amp;gt;SHOW FUNCTIONSSHOW CATALOGS [LIKE &amp;lt;pattern&amp;gt;]SHOW SCHEMAS [FROM &amp;lt;catalog&amp;gt;] [LIKE &amp;lt;pattern&amp;gt;]SHOW TABLES [FROM &amp;lt;schema&amp;gt;] [LIKE &amp;lt;pattern&amp;gt;]USE [&amp;lt;catalog&amp;gt;.]&amp;lt;schema&amp;gt;Show configured resourcesEvery SEP server is configured to connect to one or more datasources by means of a catalog that defines the connection type. Each cataloghas at least one schema; each schema has at least one table.To see the list of catalogs configured for the current server, run:trino&amp;gt; SHOW CATALOGS;For the Starburst-providedDocker image, this returns:  Catalog----------- jmx memory system tpcds tpch(5 rows)(Each query also returns a set of performance metadata, which is not repeatedhere.)Explore the tpch catalogStart with the tpch catalog, which allows you to test the capabilities andquery syntax of SEP without configuring access to an externaldata source. The TPCH connector is described on its documentationpage.Use the SHOW SCHEMAS command to list schemas provided by the TPCH connector:trino&amp;gt; SHOW SCHEMAS FROM tpch;This returns:       Schema-------------------- information_schema sf1 sf100 sf1000 sf10000 sf100000 sf300 sf3000 sf30000 tiny(10 rows)To see the tables in one of these schemas:trino&amp;gt; SHOW TABLES FROM tpch.sf100;Save typing with USETo avoid typing the catalog and schema name every time:trino&amp;gt; USE tpch.sf100;trino:sf100&amp;gt;To see the tables in sf100:trino:sf100&amp;gt; SHOW TABLES;which returns:  Table---------- customer lineitem nation orders part partsupp region supplier(8 rows)To see the structure of the customer table:trino:sf100&amp;gt; SHOW COLUMNS FROM customer;which returns:   Column   |     Type     | Extra | Comment------------+--------------+-------+--------- custkey    | bigint       |       | name       | varchar(25)  |       | address    | varchar(40)  |       | nationkey  | bigint       |       | phone      | varchar(15)  |       | acctbal    | double       |       | mktsegment | varchar(10)  |       | comment    | varchar(117) |       |(8 rows)CLI with argumentsYou can submit a valid SQL script on the trino command line:trino --execute &#39;SELECT custkey, name, phone, acctbal FROM tpch.sf100.customer LIMIT 7&#39;You can send a SQL script file to the CLI from the command line by specifyingits name as an argument to the -f or --file command line options.trino -f filename.sqlTwo TPCH scripts are included with the sample files for the O’Reilly bookTrino: The DefinitiveGuide.To use these scripts, download the book’s samples from their GitHublocation either as azip file or a git clone. Let’s say you place your clone or unzip directory in~/bookfiles. Then use the TPCH scripts as follows:cd ~/bookfiles/tpchtrino -f nations.sqlThe second sample script requires the Black holeconnector to be configured for thecurrent server. This connector is designed to operate like /dev/null and/dev/zero for performance testing and similar use cases.You can configure a local Docker-hosted SEP server to use thisconnector by following the steps in Map a local etcdirectory.If the target SEP server has the Black Hole connector, then runthe second sample script:trino --file=tpch-queries.sqlTo run this second script more than once, you must stop and restart the{site.terms.sep}} server.For a locally running tar.gz installation, in the sep-root directory foryour server, run:sudo bin/launcher restartFor a locally running Docker server:docker restart sepdock    Note:   To see the Web UI in action, keep the Web UI openand visible in a browser window while the tpch-queries script runs.Further studyMore information on the CLI is available in the Command line interfacesection in the SEP referencedocumentation."
 },
 {
  "title": "Starburst Enterprise cluster basics",
  "url": "/starburst-enterprise/platform-administrator/cluster.html",
  "content": "Starburst Enterprise cluster basicsStarburst Enterprise platform (SEP) is powerful and highly configurable. We have extensivedocumentation to help you ensure that SEP works as efficientlyand securely as possible in your environment.ArchitectureA SEP cluster consists of a coordinator and many workers. Usersconnect to the coordinator with their SQL query tool. The coordinatorcollaborates with the workers. The coordinator as well as all the workers accessthe connected data sources. This access is configured incatalogs.You can learn more in the conceptssection of the reference documentation.Processing each query is a stateful operation. The workload is orchestrated bythe coordinator and spread parallel across all workers in the cluster. Each noderuns SEP in one JVM instance, and processing is  parallelizedfurther using threads.Memory and CPU resource considerationsTypical workloads for SEP require large amounts of memory and CPUfor processing. For optimal scheduling all workers need to have the same largeamount of memory allocated.OS and software requirementsSEP requires Linux, Java, and Python. Specific details vary foreach version and deployment platform.NetworkingSEP has a few networking aspects you need to consider:  Access to the coordinator for users requires HTTP/HTTPS access  Access from the cluster to any external authentication system such as LDAP  Access from the all cluster nodes to any queried data sourcesSecuringStarburst has an array of powerful, comprehensive security features toensure that your data governance and security are top-notch. We strongly suggestyou begin by watching the training video below. After that, our extensivesecurity documentation will help you get started securing your data withSEP.  Securing Starburst training video  Security documentationConfiguration basicsWhen you are ready to install, we’ve got you covered with detailed referencedocumentation covering everything from deployment to setting up data sourceconnections:  Coordinator and worker configuration  Node properties  JVM configuration  Catalog properties introductionDeployment optionsSEP can be deployed in several ways, depending on yourorganization’s infrastructure, skills, and existing tooling. Follow the linkmost appropriate to your environment to learn more about deployingSEP:We highly suggest using a cloud provider marketplace offering, and self-managedKubernetes deployments in the cloud for production clusters:  Google Cloud Marketplace  Red Hat Marketplace  Microsoft Azure Marketplace  Self-managed Kubernetes deployment on anycloud provider platform including AKS, EKS, GKS and OpenShift.The following options are available for baremetal servers or virtual machines inyour own data center or in a cloud environment:  RPM with your own custom tooling  Tarball with your own custom tooling  RPM or tarball deployments managed withStarburst Admin  AWS deployment with AMI and CFT"
 },
 {
  "title": "Clusters",
  "url": "/starburst-galaxy/clusters.html",
  "content": "ClustersA cluster in Starburst Galaxy provides the resources to run queries against numerous datasources. Clusters define the number of workers, theconfiguration for the JVM runtime, configured data sources, and other aspects.The Starburst Galaxy platform (SGP) allows you to create, edit, and delete clustersfrom the interface. Access your clusters at any time by clicking Clusters onthe left hand menu.Add a new clusterBefore you can create a cluster in the SGP, you need toadd one or more data sources.  On the Clusters page, select + New. If this is your first cluster,you can select + New cluster from the Dashboard or Clusters pages.      Enter a unique Cluster name. Names can use lowercase letters,numbers, and hyphens.        Select your cluster profile (review cluster profile details below):          Cost optimized      General purpose      Memory optimized      Compute optimized            From the Add data source(s) drop down menu, select the data sources foryour cluster. If you don’t have a data source, click Connect a new datasource to add one.            Select your Availability zone.        Set your number of workers. Starburst Galaxy recommends a minimumof two workers. Check the box next to Use autoscaling to provide andminimum and maximum number of workers for your cluster.      Select Create cluster to finish.    You can start, stop, and edit your cluster at any time from the Clusters page.Cluster profile overviewStarburst Galaxy provides four cluster profiles to allow you to createa cluster that is right for your purposes. Review the cluster profiledifferences:  Cost optimized: Balance your workloads with total cost. Ideal forperforming the most work for the price.          Instance type: r5a.large      vCPUs: 2      Memory (GiB): 16      Instance Storage (GiB): EBS Only      Network Bandwidth (Gbps): Up to 10      EBS Bandwidth (Mbps): Up to 650        General purpose: Balance your compute, memory, and networking resources.Suitable for a variety of diverse workloads.          Instance type: r5a.4xlarge      vCPUs: 16      Memory (GiB): 128      Instance Storage (GiB): EBS Only      Network Bandwidth (Gbps): Up to 10      EBS Bandwidth (Mbps): Up to 2,880        Memory optimized: Deliver fast performance for workloads that processlarge data sets in memory.          Instance type: m5a.12xlarge      vCPUs: 48      Memory (GiB): 192      Instance Storage (GiB): EBS Only      Network Bandwidth (Gbps): 10      EBS Bandwidth (Mbps): 6,780        Compute optimized: Ideal for compute bound applications that benefit fromhigh performance processors.          Instance type: r5a.xlarge      vCPUs: 4      Memory (GiB): 32      Instance Storage (GiB): EBS Only      Network Bandwidth (Gbps): Up to 10      EBS Bandwidth (Mbps): Up to 1,085      "
 },
 {
  "title": "Developing custom connectors",
  "url": "/starburst-enterprise/data-engineer/custom-connectors.html",
  "content": "Developing custom connectorsStarburst Enterprise platform (SEP)  comes with an array of built-in connectors for avariety of cloud-based and on-prem data sources. Because SEP’sarchitecture fully abstracts the data sources it can connect to, compute andstorage are separated.That separation means that you can use the SEP connector serviceprovider interface (SPI) to build plugins for file systems and object stores,NoSQL stores, relational database systems, and custom services without anoff-the-shelf connector. As long as you can map data into relational conceptssuch as tables, columns, and rows, it is possible to create your ownSEP connector.To learn more, read our latest developerdocumentation."
 },
 {
  "title": "DBeaver",
  "url": "/data-consumer/clients/dbeaver.html",
  "content": "DBeaver    The open source tool DBeaver Community is a powerful SQLeditor and universal database tool. It is installed as a local application onyour workstation. You can use it to access Starburstclusters, since it supports the JDBC driver.    RequirementsUsers of Starburst Enterprise 354-e or newer need to use DBeaver 21.0 ornewer.ConnectionUse the following simple steps to access your cluster:  Get the necessary connectioninformation for your cluster  Start DBeaver  Start to create a new database connection in the Database Navigator withright-click dialog using Create - Connection, or File menu,New and select Database Connection in the dialog  In the selection dialog type Trino for Starburst Enterprise 354-e ornewer, locate the icon for Trino, click on it and select Next      Users of Starburst Enterprise 350-e or older, search for PrestoSQL,use the PrestoSQL icon, , click on it and select Next        Update Host, Port, Username and Password with your connectioninformation details and press Finish to connectQueryingYou can click on the defined connection in the Database Navigator toconnect. The initial connection downloads all metadata about catalogs, schema,tables, columns and more. You can browse the information is loaded.As a next step you can open the SQL Editor and start to write and executeyour queries and inspect the returned results."
 },
 {
  "title": "Deploying and updating Starburst Enterprise",
  "url": "/starburst-enterprise/platform-administrator/deploy-update.html",
  "content": "Deploying and updating Starburst EnterpriseStarburst Enterprise platform (SEP) can be deployed in several ways, depending on yourorganization’s infrastructure. Most organizations choose to install, configureand deploy SEP on Kubernetes. Follow the link most appropriate toyour environment to learn more about deploying SEP:  Directly into Google Cloud, or throughGoogle Cloud using Helmcharts  Directly into OpenShift, or throughRed Hat Marketplace usingour Kubernetes operator  On AWS using our AMI or using EKS  On Microsoft AzureSee our latest reference manual forcomprehensive documentation on installing and configuringStarburst Enterprise in your environment."
 },
 {
  "title": "Disclaimers",
  "url": "/disclaimers.html",
  "content": "DisclaimersThe following information applies to this site hosted athttps://docs.starburst.io.Copyright© 2017 - 2021, Starburst Data, Inc. Starburst, and Starburst Data are registeredtrademarks of Starburst Data, Inc. All rights reserved.PrivacyThe privacy policy of Starburstapplies to all content on and usage of the site.TrademarksProduct names, other names, logos and other material used on this site areregistered trademarks of various entities including, but not limited to, thefollowing trademark owners and names:  Apache Software Foundation  Apache Hadoop, Apache Hive, Apache Kafka, and other names  Amazon  AWS, S3, Glue, EMR, and other names  Azul Systems Inc  Zulu  Docker Inc.  Docker  Google  Google Cloud, GKE, YouTube, and other names  Microsoft  Azure, AKS, and others  Oracle  Java, JVM, OpenJDK, and other terms  The Linux Foundation  Kubernetes, Helm, Presto, Linux, and other names  Starburst Data  Starburst, Starburst Data, Starburst Galaxy, and other names"
 },
 {
  "title": "Trying Starburst Enterprise with Docker",
  "url": "/starburst-enterprise/try/docker.html",
  "content": "Trying Starburst Enterprise with DockerYou can install Starburst Enterprise platform (SEP) in a Docker container and runit on any Docker host. This is especially useful as a test or demo environmentor to evaluate SEP.With custom configuration of the containers for the SEPcoordinator and worker nodes, you can run a set of Docker containers toimplement a Linux-hosted SEP cluster on your local network or ona cloud provider.    Note:   You can run SEP with Docker on Mac or Windows computers for evaluation purposes only. Understand that these are likely to be memory-limited devices that cannot run complex queries. Production implementation of a SEP cluster on Docker is supported only on Linux servers.This page presumes you have Docker installed and running, and presumes somefamiliarity with Docker commands. For Mac and Windows evaluations, make sure youhave Docker Desktop installed and running.Initial run of SEP on DockerStarburst provides a Docker image on Docker Hub that contains a trialconfiguration of SEP. Use the following command to download andrun this image:docker run -d -p 8080:8080 --name sepdock starburstdata/starburst-enterprise:latestIf you know you need a particular release, substitute its number for latest.For example, starburstdata/starburst-enterprise:354-e. For release 350 andearlier, you must use the identifier starburstdata/presto:350-e.The docker run options have the following meanings:            Option      Meaning                  –d      Detach              –p      Map ports as hostport:containerport              ‑‑name       Provide a name for the image to use in subsequent Docker commands      To make sure the server starts in Docker, continually view the Docker logs untilyou see “======== SERVER STARTED ========”.docker logs sepdockdocker logs sepdock...Use the docker ps command to verify that the your sepdock service isrunning. Run docker ps -a to locate a server that failed to start.Docker image featuresThe default SEP image has the following characteristics:  There is no Trino CLI command installed in the Docker imageitself. You can, of course, run the CLI on the host.  The jvm.config file is set to use 1G maximum  The following catalogs are installed:          jmx      memory      system      tpcds      tpch      Verify the serverTo verify that your Docker-hosted SEP server is operating asexpected, run the Trino UI as described in Verify theserver.Run queriesTo run queries against your server, use the Trino CLI as describedin CLI.Map a local etc directoryThe Starburst-provided Docker image gives you a certain set ofconfiguration files. But the whole point of running Starburst Enterprise isto query your own data sources. How do you provide your own configuration filesto the Docker-hosted server?Starburst does not recommend going into the Docker image to changeconfiguration files there. Those changes would be lost the next timeStarburst updated the public Docker image to a new version, which yournext docker run command would automatically download and use.Instead, you can map the etc directory used by the SEP instancerunning in Docker to a local directory. Once configured, the Docker-hostedSEP uses the local etc directory as its primary source ofconfiguration files, and ignores the default settings in the Docker image.To do this requires one extra docker run option. If your Docker-hostedSEP is running now, first stop and remove it:docker stop sepdockdocker rm sepdockPopulate the local etc directoryCreate a local directory to contain your custom SEP configurationfiles. For example:mkdir -p ~/sepdockThe simplest way to test your local etc directory is to use the set ofconfiguration files provided as examples for the O’Reilly book Trino: TheDefinitive Guide.Download these samples from their GitHublocation either as azip file or a git clone.Unzip or clone the files into a local directory, such as ~/bookfiles. Fromthere, copy the entire etc directory from the single-installation sample toyour ~/sepdock directory. For example:cd ~/bookfiles/single-installationrsync -av etc ~/sepdock/Edit local configuration filesNavigate to your local etc directory. For example:cd ~/sepdock/etcInspect the configuration files provided there for suitability with Docker. Forexample, the jvm.config file sets -Xmx16G, which might be too large forDocker running on Docker Desktop on Mac or Windows. Set that to a lower valuesuch as -Xmx2G.    Note:   Make sure Docker is configured to reserve enoughmemory to handle the -Xmx setting you choose for SEP. For example, DockerDesktop’s default RAM setting is 2G. Use Docker Desktop Preferences, Resourcestab to specify enough RAM to allow the Docker-hosted server to start and run.Restart to use the local etc directoryThe docker run command has another useful option:            Option      Meaning                  ‑‑volume      Specify a directory in the container to mount to a host directory      The syntax for the ‑‑volume option is:localPath:containerPath:optionsSpecify localPath first, followed by containerPath, separated by a colon. Nooptions are needed. Options are described in Dockerdocumentation.If your Docker-hosted server is running, stop and remove it:docker stop sepdockdocker rm sepdockMake sure you’re still in the ~/sepdock/etc directory. Then rerun the Dockerimage, this time including a ‑‑volume option that maps the current directoryto the etc directory inside the Docker image, /usr/lib/presto/etc.cd ~/sepdock/etcdocker run -d -p 8080:8080 --volume $PWD:/usr/lib/presto/etc --name sepdock starburstdata/starburst-enterprise:latestContinually view the Docker logs as before, waiting to see “======== SERVERSTARTED ========”:docker logs sepdock...docker logs sepdock...Verify that the server is running by using the Web UI as described in Verifythe server.To run queries, connect the TrinoCLI to the Docker-hostedSEP instance. Run the show catalogs;, show schemas;, andshow tables; commands to confirm the assets of the server.Configure custom data sourcesThe local etc directory feature described in the previous section lets youwork locally to make changes to your Docker environment.Continue adding new catalogs and configuring features that customize connectionsto the data sources you need SEP to see."
 },
 {
  "title": "Google Kubernetes Engine",
  "url": "/marketplace/google/gke.html",
  "content": "Google Kubernetes EngineGoogle Cloud, and specifically the Google KubernetesEngine (GKE), is certified to workwith Starburst Enterprise platform (SEP).IntroductionThis user guide provides information on using Starburst Helm charts todeploy SEP, and eventually a cluster in Google Cloud -Kubernetes Applications.Before you get started installing SEP, we suggest that you readour customization guide to learnhow to customize SEP clusters.We also have a helpful installationchecklist for an overview of the generalHelm-based installation and upgrade process.We recommend that you use Google Cloud’s marketplace UI with Click to Deployon GKE to deploy. The command line Deploy via command line installation isnot supported.Getting startedSetting up and deploying SEP to a Google Kubernetes Enginecluster using Google Cloud is pretty straightforward.You can get started by simply following the UI instructions in themarketplace.Cluster requirementsSEP can be deployed to an existing GKE cluster, or to a newlycreated one. If using an existing GKE cluster, make sure it is configured withthe appropriate number and type of instances needed to deploy anSEP cluster. Review the SEPrequirements to ensure that you havesufficient resources. With insufficient resources available in the cluster, thedeployed pods may fail to be scheduled.You can also choose to deploy a new cluster with minimal resources and defaultconfiguration from the marketplace offering configuration page. After thedefault SEP chart deploys successfully in this cluster, you mustreview the cluster configuration before updating it and making any adjustmentsto ensure that the cluster is sufficiently large.DeployingThe default values of the SEP configuration in theUIwork for most of the cases.   Warning:If you are enabling global access control using Ranger, make sure you note downthe values of the admin and service user passwords you provide in the UI forlater use.Once done, you can click on Deploy to install SEP in thecluster. You can track the progress of the deployment in the applicationspage.Once the application is successfully deployed, SEP is deployedusing the Helm charts. If you enable either or both ofRanger and Hive MetastoreService these are also deployed. Thedeployment uses the values you provide in the UI and some default values whendeployed the first time. If you want to upgrade or change the deploymentconfiguration, you can either use the Application info from the applicationpage or the followingSEP upgrade instructions.Upgrading SEPYou can upgrade SEP using the cloudshell.You need to download the Helm charts and extract them to modify thedeployment, as shown in the following:export TAG=2.0.1export APP_INSTANCE_NAME=&amp;lt;name of the application instance&amp;gt;wget https://console.cloud.google.com/storage/browser/starburst-enterprise-presto/helmCharts/presto-gcp/starburst-enterprise-presto-charts-$TAG.tgz;tar -zxvf starburst-enterprise-presto-charts-$TAG.tgz;Now update the values.yaml in the umbrella chart folder(starburst-enterprise-presto-charts-$TAG) before you apply the manifestfile. Alternatively, you can use --set parameter in helm templatecommand to update the chart configuration. The values set in --set takesprecedence over the ones set in values.yaml.Once you have the manifest created, apply (kubectl apply) the file to updatethe deployment.More details about the SEP configuration are found in the reference documentation.Make sure you have usagemetricsenabled in your SEP configuration. Our Google Cloud offeringrelies on the metrics and if disabled, the deployment is terminated after 36hours.The minimum set of parameters required for the upgrade are shown below:helm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set [PUT OTHER CHART PARAMETERS YOU WANT TO UPDATE] &amp;gt; presto_manifest.yamlThe following example shows how to update the number of SEP workers:helm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set worker.count &amp;gt; presto_manifest.yamlConfigure kubectl to point at your cluster and then apply the configuration:kubectl apply -f presto_manifest.yamlEnabling global access control with Apache RangerYou can deploy or upgrade Ranger for globalaccess control by creating a manifest file with Ranger parameters set.You can either update the values.yaml in the umbrella chart folderstarburst-enterprise-presto-charts-$TAG for the configuration(starburst-ranger) prior to generating the manifest file or or use --setparameter while creating the manifest file. The --set parameters must beprefixed by starburst-ranger.The complete reference of the chart configuration can be found in theRanger chart documentation.Example with Ranger enabled:helm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set starburst-ranger.enabled=true   --set [PUT OTHER CHART PARAMETERS YOU WANT TO UPDATE] &amp;gt; presto_manifest.yamlEnabling the Hive Metastore ServiceSimilar to Ranger, to deploy or upgrade the Hive Metastore Service (HMS)provided by Starburst, generate a manifest file with Hive chartparameters set. You can either use values.yaml (for starburst-hive) oruse the --set parameters. For Hive charts, the --set parameters must beprefixed by starburst-hive.The complete reference of the chart configuration for the metastore can be foundin the HMS documentation.Example with HMS enabled:helm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set starburst-hive.image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/hive&quot;   --set starburst-hive.image.tag=&quot;$TAG&quot;   --set starburst-hive.enabled=true &amp;gt; presto_manifest.yamlConfigure kubectl to point at your cluster and then apply the configuration:kubectl apply -f presto_manifest.yamlDeleting a deploymentTo delete a GKE deployment, run the following command:kubectl delete -f presto_manifest.yaml"
 },
 {
  "title": "Glossary",
  "url": "/glossary.html",
  "content": "GlossaryTerms A-EAmazon AWS marketplaceA provider for all aspects of the required infrastructure. This includes usingAWS CloudFormation for provisioning, Amazon Simple Storage Service (S3) forstorage, Amazon Machine Images (AMI), and Amazon Elastic Compute Cloud (EC2) forcomputes, Amazon Glue as metadata catalog, and others. For more information, seeAmazon AWS Marketplace.CatalogCatalogs define and name the configuration to connect to, and query a datasource. For more information, seeCatalogs.ClusterA cluster provides the resources to run queries against numerous data sources.Clusters define the number of workers, the configuration for the JVM runtime,configured data sources, and others aspects. For more information, see clusterbasics.ConnectorTransforms the underlying data into the SEP concepts of schemas,tables, columns, rows, and data types. A connector is specific to the datasource it supports, and are named as properties incatalogs.COTSCommon off-the-shelf. Refers to commodity hardware components.Data consumer personaOwns data products such as reports, dashboards, models, and the quality ofanalysis. For more information, see Starburstpersonas.Data engineer personaOwns schemas and is responsible for the source data quality and ETL SLA. Formore information, see Starburst personasData sourceA data source is a system from which data is retrieved. In SEP,you must connect to a data source so you can query that source by using acatalog. See Configure and definecatalogsExternal ID (AWS)An external ID is an identifier in AWS that is required for usingStarburst Galaxy. It is used to ensure that only trusted AWS accountsare given permission to operate the SGP clusters based ontheir assigned role and trust policy. For more information on AWS Identity andAccess Management, see How to use an external ID when granting access to yourAWS resources to a thirdparty.Terms F-JGoogle Cloud marketplaceDeploy in the Google Cloud Marketplace or using the Starburst Kubernetessolution on the Google Kubernetes Engine (GKE). GKE is a secure, productionready, managed Kubernetes service in Google Cloud managing for containerizedapplications. For more information, see Google Cloudmarketplace.Terms K-OMarketplacePurchase a preconfigured set of machine images, containers, and other neededresources to run SEP on their cloud hosts under your control. SeeMarketplace deployments.Microsoft Azure marketplaceDeploy using in the Azure Marketplace or using the Starburst Kubernetes solutiononto the Azure Kubernetes Services (AKS). AKS is a secure, production-ready,managed Kubernetes service on Azure for managing for containerized applications.For more information, see Microsoft AzureMarketplace.Terms P-TPlatform administrator personaOwns platforms and services (ITIL-style). Has service SLA responsibility for theinfrastructure supporting the cluster. For more information, see Starburstpersonas.Presto and PrestoSQLOld name for Trino.Red Hat OpenShift marketplaceA container platform using Kubernetes operators that automates the provisioning,management and scaling of applications to any cloud platform or even on-prem.Starburst Enterprise is available on Red Hat marketplace as of OpenShift version 4.For more information, see Red HatMarketplace.Starburst Enterprise platformHelps companies harness the value of open source Trino, the fastestdistributed query engine available today. Starburst adds connectors, security,and support that meets the needs for fast data access at scale. For moreinformation, seeStarburst Enterprise.SEPAbbreviation of Starburst Enterprise platform. For more information, seeStarburst Enterprise.SQLStructured Query Language. The standard language used with relational databases.For more information, see SQL.TrinoFast distributed SQL query engine for big data analytics, formerlyPrestoSQL."
 },
 {
  "title": "Create an IAM user and attach a policy",
  "url": "/starburst-galaxy/data-sources/iam-policies-users.html",
  "content": "Create an IAM user and attach a policyCreate an IAM user and attach a policy to allow it to access specificusers—identities in your AWS account. For example, your custom IAM policy couldinclude full or read only access to S3 resources.  Navigate to your IAM service. Youmay need to sign in.  Select Users from the left side menu.  Click Add user.  Enter a starburst-galaxy- prefixed user name, select Programmatic access, and click Next: Permissions  Select Attach existing policies directly.  Filter and select the name of the policy or policies using the check box.  Click Next: Tags and, optionally, add tags.  Click Create user.Copy your newly created Access key ID and Secret access key.  The Secret access key is the value in the plain text field when you createa secret.  The Access key ID is used during Starburst Galaxy data sourcecreationNext step  Create a secret in AWS"
 },
 {
  "title": "Image usage",
  "url": "/internal/images.html",
  "content": "ImagesDisplay images with:  Our own include image syntax  HTML syntax  Markdown syntaxinclude image syntaxThis is our in-house and preferred image syntax.This syntax provides a screenshot attribute that places a single pixel borderaround the image, which is best used for screenshots that have white space up totheir edges, so that the image is clearly delineated from the page’s text.This syntax also allows you to specify valid CSS to tailor a particular image.If no pxwidth value is passed, the image defaults to the file’s inherent widthup to a maximum of 90% of the current container for very large images.Another option of the include image syntax is to turn very large images intotwo-size modal images. The specified image is reduced for normal display on thepage, limited to pxwidth size, but the image is also now a button. Whenclicked, the image expands and is shown in a larger modal window. Thescreenshot and modal options are mutually exclusive; use one or the other.Use include image.html in a separate tag block with the parameters shown inthe following table:* Parameters may be used in any order.            Key      Value      Required      Notes                  url      Relative path to the asset.      Required                     alt‑text      String for alt tag content.      Required                     descr      String for description tag content for accessibility support.      Optional      Use if alt‑text does not convey enough information              pxwidth      Width value in pixels, integer only. See the notes below      Optional                     screenshot      Set to ‘true’ adds 1px solid border. No effect if modal.      Optional                     modal      Set to ‘true’ adds modal ability.      Optional      Must assign img‑id if true              img‑id      Adds ID string.      Optional      Required for modal. Must be unique per page.              style      Pass any valid CSS to the image.      Optional             For the pxwidth key:  Don’t include a pxwidth line, or leave its value empty to specify using thenative image resolution by default. pxwidth=&#39;&#39;  Do not specify pxwidth=&#39;0&#39;, which disables the image.  For modal, pxwidth specifies the non-modal display size on the page beforeexpansion.Copy these imagesFeel free to copy the include tag blocks that display the following images.Paste these as templates into your working documents, then edit as appropriate:{% include image.html  url=&#39;../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;Starburst cluster overview&#39;  descr-text=&#39;Image depicting Starburst architecture&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}                                                              ×                                              HTML syntaxThis syntax allows you to add further tweaks such as sizing. If you copy thisexample, remember to surround site.baseurl with double open and close brace{ } characters.&amp;lt;img src=&quot;site.baseurl/assets/img/logo/starburst.png&quot; alt=&quot;Starburst logo&quot;description=&quot;Starburst Logo&quot; width=&quot;200&quot;/&amp;gt;Markdown syntaxOnly for very simple image cases, use Markdown syntax, which is:![title](../assets/img/image.png). Use a relative path to the image from thecurrent location, or use the site.baseurl substitution token. (If you copythe following example, remember to surround site.baseurl with double open and closebrace { } characters.)![Starburst](site.baseurl/assets/img/logo/starburst.png)LogosThe following conditions should be met when optimizing and saving brand logos touse on the site:            Property      Value                  Bounding      Trim/Cropped              Width      400px              Height      Auto (Constrained proportions)              Background      Transparent              Format      PNG              Example            When adding a brand logo to the beginning of a page, for example theRed Hat marketplace page, the initialcontent section should contain the logo using the following HTML structure:&amp;lt;div class=&quot;row-with-logo&quot;&amp;gt;  &amp;lt;div markdown=&quot;1&quot; class=&quot;row-with-logo-text-container&quot;&amp;gt;    &amp;lt;-- Add text content here --&amp;gt;  &amp;lt;/div&amp;gt;  &amp;lt;div class=&quot;row-with-logo-image-container&quot;&amp;gt;    &amp;lt;img      class=&quot;img-fluid&quot;      src=&quot;../assets/img/logo/dbeaver.png&quot;      alt=&quot;{{page.title}}&quot;    /&amp;gt;  &amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;Similarly, the image.html include may also be used by changing the HTMLsnippet above to:&amp;lt;div class=&quot;row-with-logo&quot;&amp;gt;  &amp;lt;div markdown=&quot;1&quot; class=&quot;row-with-logo-text-container&quot;&amp;gt;    &amp;lt;-- Add text content here --&amp;gt;  &amp;lt;/div&amp;gt;  &amp;lt;div class=&quot;row-with-logo-image-container&quot;&amp;gt;    {% include image.html      url=&#39;../assets/img/logo/dbeaver.png&#39;      alt-text=&#39;DBeaver Logo&#39;    %}  &amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;This HTML structure ensures that the logo will be placed to the right of thecontent, creating a two column layout that allows the text to not produce astaggered wrapping effect. Once a certain breakpoint has been reached the logowill then move to above the text content.All content after the above HTML snippet will take up the full width of thecontainer.IconsYou can and should use fontawsome icons.There are thousands available.Syntax is &amp;lt;i class=&quot;fa fa-user&quot;&amp;gt;&amp;lt;/i&amp;gt;.You can change size, color and more.This is a user icon  in a paragraph.Also be careful, we use fontawesome 5 and therefore some icons are in adifferent name space.  &amp;lt;i class=&quot;fab fa-twitter&quot;&amp;gt;&amp;lt;/i&amp;gt; for   &amp;lt;i class=&quot;fab fa-youtube&quot;&amp;gt;&amp;lt;/i&amp;gt; for   &amp;lt;i class=&quot;fab fa-linkedin&quot;&amp;gt;&amp;lt;/i&amp;gt; for   &amp;lt;i class=&quot;fab fa-github&quot;&amp;gt;&amp;lt;/i&amp;gt; for Coloring can be done with CSS or an embedded style.  &amp;lt;i class=&quot;fab fa-github&quot; style=&quot;color: red;&quot;&amp;gt;&amp;lt;/i&amp;gt; for Persona based icons          Platform administrator            Data consumer            Data engineer  Topic based icons          Admin            Catalog            Clients            Clusters            Connectors            Data sources            Deploying            Migration            Performance tuning            Query performance  "
 },
 {
  "title": "Red Hat OpenShift",
  "url": "/marketplace/redhat/index.html",
  "content": "Red Hat OpenShift    Red Hat Marketplace    Starburst offers the following support for our marketplace subscriberswithout an enterprise contract:          Support through the OpenShift Support portal      Five issues per month      First response SLA of one business day      Support hours of 9 AM - 6 PM US Eastern Time        OpenShift from Red Hat Marketplace (RHM) is a container platform usingKubernetes operators that automate the provisioning, management, and scalingof applications to any cloud platform or even on-prem. Starburst Enterprise platform (SEP)is available onRHMas of OpenShift version 4.          PrerequisitesBefore you get started, here are some things you need:  Access to an OpenShift cluster with with correctly-sizednodes,using IAM credentials, and with sufficient Elastic IPs  Previously installed and configured Kubernetes, including access tokubectl  An editor suitable for editing YAML files  Your SEP license fileBefore you get started installing SEP, we suggest that you readour reference documentation and our helpfulcustomization guide.Quick startAfter you have signed up through RHM, download the latest OpenShift ContainerPlatform (OCP) client for your platform from the OpenShift mirrorsite, andcopy the oc executable into your path, usually /usr/local/bin. Once thisis done, you are ready to install the operator in OCP4.Using your administrator login for Red Hat OCP, log in to the OCP web consoleand click Operators &amp;gt; OperatorHub in the left-hand menu.Once there, select “Starburst Enterprise” from the Project: drop-down menu, and navigatethrough the projects to All Items &amp;gt; Big Data &amp;gt; Starburst until yousee Starburst Enterprise. Click on that tile, then click the Installbutton.When the Create Operator Subscription page appears, select theStarburst project as the specific namespace on the cluster, leaveall other options as default, and click Subscribe.When the operation is complete, you are subscribed to the SEPoperator, and it is installed and accessible to you in OCP4.Getting up and runningInstallationOnce you have your operator subscription in place, it’s time to install. Thereare several steps to getting SEP installed and deployed:  Installing the SEP cluster  Installing the Hive Metastore Service (HMS)You must install the HMS to connect and query any objects storage with the Hiveconnector. This is typically a core use case for SEP, and then arequired step. The HMS is used by SEP to manage the metadata ofany objects storage.Step by step guide to Install Starburst Enterprise Operator via Red HatMarketplace:  On the main menu, click Workspace &amp;gt; My Software &amp;gt; Product &amp;gt;Install Operator  On the Update Channel section, select an option, ‘stable’ or‘alpha’  On the ApprovalStrategy section, select either Automatic orManual, the approval strategy corresponds to how you want to processoperator upgrades)  On the TargetClustersection:          Click the checkbox next to the clusters where you want to install theStarburst Enterprise Operator      For each cluster you selected, under** Namespace Scope, on the **SelectScope list, select an option        Click Install,  it may take several minutes for installation to complete  Once installation is complete, the status changes from Installing to Up to dateConfigurationWhen the operator installation is complete, you can proceed to deploy two customresources:  Starburst Enterprise  Starburst HiveJust like with installation, there are several steps to configuringStarburst Enterprise:  Configuring SEP  Configuring the Hive metastoreEach of these steps uses a specific Helm chart values.yaml configuration.Click on the links for detailed instructions on configuring each of the customresources.The following setup steps are required:  Configure the resource requirements based on your cluster and node sizes  Update the image repository and tags to use the RedHat registry:          Starburst Enterprise: registry.connect.redhat.com/starburst/starburst-enterprise:354-e-ubi      Starburst Enterprise Init: registry.connect.redhat.com/starburst/starburst-enterprise-init:354.0.0-ubi      HMS: registry.connect.redhat.com/starburst/hive:354.0.0-ubi      Next stepsYour cluster is now operational! You can now connect to it with your clienttools, and start querying your datasources.Additional steps to quickly test your deployed cluster:  Create a route to the default ‘starburst’ service. If you changed the name inthe expose section, use the new name.  Run the following command using the CLI with theconfigured route:    trino --server &amp;lt;URL from route&amp;gt; --catalog tpch        Next run SHOW SCHEMAS; in the CLI, and you can see a list of schemasavailable to query with names such as tiny, sf1, sf100, and others.We’ve created an operations guide toget you started with common first steps in cluster operations.It includes some great advice about starting with a small, initial configurationthat is built upon in our cluster sizing and performance videotraining.TroubleshootingSEP is powerful, enterprise-grade software with many movingparts. As such, if you find you need help troubleshooting, here are some helpfulresources:  LDAP authentication  Data consumer guide with clients, SQL and other tipsFAQQ: Once it’s deployed, how do I access my cluster?A: You can use the CLI on aterminal or the WebUI to access your cluster. Forexample:      Trino CLI command: ./trino --serverexample-starburst-enterprise.apps.demo.rht-sbu.io --catalog hive        Web UI URL: http://example-starburst-enterprise.apps.demo.rht-sbu.io        Many other client applications canbe connected, and used to run queries, created dashboards and more.  Q: I need to make administrative changes that require a shell prompt. How to Iget a command line shell prompt in a container within my cluster?A: On OCP, you’ll get a shell prompt for a pod. To get a shellprompt for a pod, you’ll need the name of the pod you want to work from. To doso, log in to your cluster as per your RHM documentation. For example:oc login -u kubeadmin -p XXXXX-XXXXX-XXXXX-XXXX https://api.demo.rht-sbu.io:6443Get the list of running pods:❯ oc get pod -o wideNAME                                                 READY   STATUS    RESTARTS   AGE   IP            NODE                                         NOMINATED NODE   READINESS GATEShive-XXXXXXXXX-lhj7l        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;starburst-enterprise-coordinator-example-XXXXXXXXX-4bzrv   1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;starburst-enterprise-operator-7c4ff6dd8f-2xxrr                     1/1     Running   0          41m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;starburst-enterprise-worker-example-XXXXXXXXX-522j8        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;starburst-enterprise-worker-example-XXXXXXXXX-kwxhr        1/1     Running   0          27m   10.130.2.XX   ip-10-0-162-XXX.us-west-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;starburst-enterprise-worker-example-XXXXXXXXX-phlqq        1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;The pod name is the first value in a record. Use the pod name to open ashell:❯ oc rsh starburst-enterprise-coordinator-example-XXXXXXXXX-4bzrvA shell prompt will appear. For example, on OCP 4.4:sh-4.4$Q: Is there a way to get a shell prompt through the OCP web console?A: Yes. Log in to your OCP web console and navigate toWorkloads &amp;gt; Pods. Select the pod you want a terminal for, and click theTerminal tab.Q: I’ve added a new data source. How do I update the configuration torecognize it?A: Using the making configurationchanges section to edit your YAML configuration,find additionalCatalogs, and add an entry for your new data source. Forexample, to add a PostgreSQL data source called mydatabase:    mydatabase: |      connector.name=postgresql      connection-url=jdbc:postgresql://172.30.XX.64:5432/pgbench      connection-user=pgbench      connection-password=postgres123Once your changes are complete, click Save and then Reload to deployyour changes. Note that this restarts the coordinator and all workers on thecluster, and might take a little while."
 },
 {
  "title": "Starburst",
  "url": "/index.html",
  "content": "                    Everything about Starburst products        What are you going to explore today?                              Guide to choosing the right deployment                                Get started with Starburst Enterprise                                &amp;nbsp;                      Confused where you landed?       Go       straight to the Starburst Enterprise reference documentation!       A lot of new useful documentation is available here, but fear not,       all the comprehensive reference documentation for Starburst Enterprise is ready and constantly improving as always.              Go there now!                                                What can you learn here?      No matter which Starburst product you use, and what you are trying to achieve,        you can find help on these pages:              Installing and operating a Starburst cluster on your own infrastructure        Using Starburst on public cloud provider infrastructure        Connecting data sources to query your relational database or your object storage        Using a BI tool to query data with Starburst        Writing SQL statements to get more insights from your data            Explore the site to find all this and more information in articles,      reference documentation, videos and other materials                          Which crew member best describes you?                                                                              Data consumer              You use data from Starburst with your BI and data science tools to create important business insights                                                                                                    Data engineer              You provide the source data to data consumers using SEP, and ensure its quality, availability, and performance                                                                                                    Platform administrator              You install and manage the clusters that serve the data, and ensure everything is performing well                                                Are you a data leader? You can learn more    about how Starburst products can impact your data insights, ops processes, budget, and efficiency.        Still not sure where to start? Head over to our user personas page to learn which fits you best!              Other resources                                                Video library                                                                    Reference docs                                                                    SQL                              "
 },
 {
  "title": "Starburst for data platform administrators",
  "url": "/platform-administrator/index.html",
  "content": "Starburst for data platform administratorsStarburst Enterprise platform (SEP) is a fast, interactive distributed SQL query enginethat decouples compute from data storage. SEP lets you query datawhere it lives, including Hive, Snowflake, MySQL and even proprietary datastores. A single SEP query can combine data from all these datasources and more. SEP can run on-prem as well as in many cloudenvironments.SEP can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast. It can pull your landing times forward, and help youmeet or beat your SLAs. And, SEP has robust access controloptions for your organization, from integrating with LDAP to using yourRanger-managed policies.How does this work?SEP is a distributed system that runson COTS hardware. The coordinator parses, analyzes and plans query execution,and then distributes the query plan for processing among worker machines in thecluster. Workers use connectors specific to your data sources, such asSnowflake, Postgres, and Hive to transform queries and return data.                                                              ×                                              SEP uses ANSI-compliant SQL, and takes care of translating yourqueries to the correct SQL syntax for your data sources.SEP’s ability to federate data sources in a single query reducesyour organization’s reliance on temporary tables and more complex ETL pipelines.Because SEP query processing works in memory, your diskinvestment is light.How do I get started?As a first step you should read our guide to choosing the right deployment.You can spin up a trial instance following thesteps in Trying Starburstproducts. If you already haveSEP, you can go straight to our SEP administrator’s userguide.When you are ready to create a production cluster and you’ve chosen one of ourKubernetes-based deployments, read our handy introduction to SEP withKubernetes guide. It explainsSEP cluster design and sizing, as well as best practices forcustomization and configuration with Helm.We also have some great training videos to get youstarted, and some articles on topics you are likely to have hard questions on:  Data architecture philosophy andapproach  Referencearchitectures  Securityguide anddeep dive  SEP and Helm  SEP administration"
 },
 {
  "title": "Video library",
  "url": "/videos/index.html",
  "content": "                    Video library                                      Explore the growing list of videos to learn about Starburst        products, installation, tuning, management, SQL, and more.        Use timestamps to drill down and learn about a specific topic:              Advanced SQL in Starburst        Understanding and tuning Starburst query processing        Securing Starburst Enterprise        Starburst cluster sizing, and performance tuning              Cluster sizing and performance tuningLearn how to tune and size your deployment for optimal performance.Click on the video below to view the full video in YouTube, or goto the performancetuning section to view the video in segments, with links to the relevantdocumentation.                                        Watch the video and see more details now!                    Topics:              Cluster configuration and node sizing        Memory configuration and management        Improving task concurrency and worker scheduling        Tuning your JVM configuration        Investigating queries for join order and other criteria        Tuning the cost-based optimizer            Video date: 9 September 2020      Running time: 2h15m      Securing StarburstLearn how to securely deploy Starburst at scale. Click on the video below toview the full video in YouTube, or goto the security section toview the video in segments, with links to the relevant documentation.                                    Watch the video and see more details now!                    Topics:              Authentication, including password &amp;amp; LDAP Authentication        Authorization to access your data sources        Encryption including client-to-coordinator communication        Secure communication in the cluster        Support for Kerberos        Secrets usage for configuration files including catalogs            Video date: 26 August 2020      Running time: 2h07m      Advanced SQLLearn how to run more complex and comprehensive SQL queries with Starburst.Click on the video below to view the full video in YouTube, or go toview the video in segments,with links to the relevant documentation in our SQL section.                                    Watch the video and see more details now!                    Topics:              Using JSON and other complex data types        Advanced aggregation techniques        Window functions        Array and map functions        Lambda expressions        Many other SQL functions and features            Video date: 29 July 2020      Running time: 2h14m      Query tuningLearn how Starburst executes queries to help you improve query performance.Click on the video below to view the full video in YouTube, or goto the query performance sectionto view the video in segments, with links to the relevant documentation.                                    Watch the video and see more details now!                    Topics:              Explaining the EXPLAIN        Learning how queries are analyzed and executed        Understanding what the optimizer does, including some of its limitations        Showcasing the cost-based optimizer            Video date: 12 August 2020      Running time: 2h09m      "
 },
 {
  "title": "Data consumer user guide",
  "url": "/data-consumer/index.html",
  "content": "                    Starburst                                      Data consumer user guide      Anytime you need help with using Starburst products to drive        your day-to-day analytics, data science or business intelligence, start        here. This guide always takes you to the most up-to-date information.                              What you need to know                                                                                Introduction              Use Starburst              products with your favorite tools                                                                                                    Clients              JDBC and ODBC drivers, and tools that use them                                                                                                    Starburst SQL              The SQL language, syntax, and functions and operators                                                                                                    Migration              How to easily move your analytics to Starburst                                                      Other resources      We&#39;ve also gathered some resources on popular topics for you.                                                Query federation                                                                    Query optimizer                              "
 },
 {
  "title": "Amazon AWS",
  "url": "/marketplace/amazon/index.html",
  "content": "Amazon AWS    Amazon AWS marketplace    Starburst offers the following support for our marketplace subscriberswithout an enterprise contract:          Email-only support      Five email issues to awssupport@starburstdata.com per month      First response SLA of one business day      Support hours of 9 AM - 6 PM US Eastern Time        Starburst Enterprise platform (SEP) is available on AWSmarketplace.    Amazon Cloud Formation    As an alternative, you can manage SEP directly on AWS with thehelp of the Amazon Cloud Formation template and all relateddocumentation.    Amazon Elastic Kubernetes Service    You can also use the Amazon Elastic Kubernetes Service (EKS) directly; EKS iscertified to work with SEP. More information applicable to bothscenarios in available in our Kubernetes referencedocumentation, including our customizationguide for SEP.    We also have a helpful installationchecklist for an overview of the generalHelm-based installation and upgrade process.          "
 },
 {
  "title": "Starburst for data leaders",
  "url": "/data-leader/index.html",
  "content": "Starburst for data leadersStarburst Enterprise platform (SEP) is a fast, interactive distributed SQL query enginethat decouples compute from data storage. SEP lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single SEP query can combine data from all these datasources and more. SEP can run on-prem as well as in many cloudenvironments, and comes with 30+ supported enterpriseconnectors, providinghigh performance SQL-based access to most of the data platforms in yourorganization such as Snowflake, Postgres, and Hive.Maybe your organization relies on a single variant of SQL, or maybe they use afew. With SEP, you only need to know Starburst SQL, whichis ANSI-compliant and should feel comfortable and familiar. SEPtakes care of translating your queries to the correct SQL syntax for your datasource.SEP can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast, and requires a minimal disk investment. It can pullyour landing times forward, and help you meet or beat your SLAs. And, Starbursthas robust access control options for your organization, from integrating withLDAP to using your existing Ranger-managed policies.Your analysts and data scientists can connect to most of their favorite toolsusing only the SEP-specific JDBC or ODBC driver, much like theones they already use, making the transition as frictionless as possible.Want to know more? No problem! Here are some resources to answer commonquestions about SEP’s capabilities and value:  What problems does Starburst solve?  Will this help me future-proof my data ops?  How does Starburst deliver value?  Is there a comprehensive product review available?"
 },
 {
  "title": "User guide",
  "url": "/starburst-enterprise/index.html",
  "content": "                    Starburst Enterprise                              Get started                      Reference documentation                                                  Welcome to the user guide for Starburst Enterprise platform (SEP). We&#39;ve charted a course for you that is tailored to your duties as a data crew member. Select your role below to start your journey.                                                                              Data consumer              I use data from Starburst with my BI and data science tools to create important business insights                                                                                                    Data engineer              I provide the source data to data consumers, and ensure its quality, availability and performance                                                                                                    Platform administrator              I install and manage the clusters that serve our data,                 and ensure everything is performing well                                                Still not sure where to start? Head over to our user personas page to learn which fits you best.              Browse topics      We&#39;ve also gathered some resources on popular topics for you.                                                Starburst Admin                                                                    Videos                                                                    Clusters                                                                    SQL                              "
 },
 {
  "title": "MySQL data source",
  "url": "/starburst-galaxy/data-sources/mysql/index.html",
  "content": "MySQL data sourceYou can connect MySQL databases in SGP as a data source,Requirements  MySQL data source is required to be hosted on Amazon RDS.  User with sufficient access to the data source must be configured with asecret.Operations  Add a MySQL data source"
 },
 {
  "title": "Platform administrator user guide",
  "url": "/starburst-enterprise/platform-administrator/index.html",
  "content": "                    Starburst Enterprise                                      Platform administrator user guide      Anytime you need help with an SEP administration         task or concept, you should start here. This guide takes         you to the most up-to-date information on running your SEP cluster.      And if you have not seen it already, read our        guide to choosing the right        deployment.                          What you need to know                                                    Cluster basics          Everything you need to know to get started                                                            Tune your cluster          Advanced configuration for even better performance                                                            Deploy and update          From initial install to keeping SEP up-to-date                                                            Secure your cluster          Client access, data sources, and more                                Other topics      We&#39;ve also gathered some resources on popular topics for you.                                                    SQL in Starburst                                                                    Query optimizer                                                                    Clients                              "
 },
 {
  "title": "Amazon S3 data source with AWS Glue data catalog",
  "url": "/starburst-galaxy/data-sources/s3/index.html",
  "content": "Amazon S3 data source with AWS Glue data catalogYou can connect data from Amazon S3 object storage and metadata from the AWSGlue catalog in SGP as a data source.Requirements  AWS Access key ID and secret with access to S3  AWS Access key ID and secret with access to AWS GlueOperation  Add an Amazon S3 data source with AWS Glue catalog"
 },
 {
  "title": "Marketplace deployments",
  "url": "/marketplace/index.html",
  "content": "                    Marketplace deployments                                           Starburst Enterprise on any cloud      Many major cloud providers include      Starburst Enterprise platform (SEP)      in their      marketplace offerings. You can purchase a preconfigured machine images,      containers and other needed resources to run SEP on their      cloud hosts under your control. A free trial configuration is available      from most providers. For all marketplace implementations, you need the      following:              Access credentials from your cloud provider        The URLs and access credentials for the data sources you want your            cloud-hosted SEP cluster to access        All appropriate ACLs must be in place                                      Starburst is available in the following marketplaces:                                                              Amazon AWS                                                                        Google Cloud                                                                        Microsoft Azure                                                                        Red Hat OpenShift                              "
 },
 {
  "title": "PostgreSQL data source",
  "url": "/starburst-galaxy/data-sources/postgresql/index.html",
  "content": "PostgreSQL data sourceYou can connect PostgreSQL databases in SGP as a data source.Requirements  PostgreSQL data source is required to be hosted on Amazon RDS.  User with sufficient access to the data source must be configured with asecret.Operation  Add a PostgreSQL data source"
 },
 {
  "title": "Google Cloud",
  "url": "/marketplace/google/index.html",
  "content": "Google Cloud    Google Cloud marketplace    Starburst offers the following support for our marketplace subscriberswithout an enterprise contract:          Email-only support      Five email issues to gcpsupport@starburstdata.com per month      First response SLA of one business day      Support hours of 9 AM - 6 PM US Eastern Time        Starburst Enterprise platform (SEP) is available and certified on Google Cloud.As an alternative, you can deploy SEP in a private cloud withGoogle Kubernetes Engine (GKE) directly.    More information applicable to both scenarios in available in our Kubernetesreference documentation, including ourcustomization guide forSEP.    We also have a helpful installationchecklist for an overview of the generalHelm-based installation and upgrade process.          "
 },
 {
  "title": "Data sources overview",
  "url": "/starburst-galaxy/data-sources/index.html",
  "content": "Data sources overviewYou can create a catalog by selecting a data source and configuring theconnection to various external databases and other systems.Once the data source is defined and used in your Starburst cluster, youcan query the data source by accessing the catalog and the nested schemas andtables.Initial data source setupSelect a data source below to see requirements and instructions for adding it toStarburst Galaxy:  Amazon S3 + AWS Glue data catalog  MySQL  PostgreSQLData source types and propertiesData source types define the connector that Starburst Galaxy uses forthe specific data source. The connector in turn, defines the configurationproperties to use.A number of properties are used for all data source configurations.            Property      Description                  Type      Defines the connection type of the external data source. Type is        related to the connector used to access the  data in the external data        source.              Name      The name of the data source that is also used as the name of the        catalog. This name is visible to when querying PrestoSQL and        the underlying data source.              Description      A short paragraph that provides more details about the data source        than the name alone.              Connection properties      Properties required by the connectors that enable connection to a        specified data source, such as access keys.      "
 },
 {
  "title": "Hitchhiker&#39;s Guide to editing Starburst content",
  "url": "/internal/index.html",
  "content": "Hitchhiker’s Guide to editing Starburst content  Markdown guide with all sorts of examples and usage  Our documentation iconography guide  Personas we write for  Metadata tags for page front matterBreadcrumbsThe site currently uses a simple path based breadcrumb system implemented inbreadcrumbs-simple.html.The breadcrumbs systemfrom Sean Hammond is also included as inspiration for future improvements ormigration or a merge of the two systems. It does not work for plain pages andpaths but does work nicely for posts and collections."
 },
 {
  "title": "Overview",
  "url": "/starburst-enterprise/starburst-admin/index.html",
  "content": "OverviewStarburst Admin is a collection of Ansibleplaybooks for installing and managing Starburst Enterprise platform (SEP) orTrino clusters.The following features are available:  Installation and upgrade of Starburst Enterprise platform (SEP) or Trinousing the RPM or tar.gz archives  Addition and update of coordinator and worker nodes configuration files,including catalog properties files for data source configuration  Service management of the cluster and on all nodes (start/stop/restart/status)  Collection of logs  Addition of custom binary files, such as custom connectors or UDF  Addition of custom configuration filesStarburst Admin does not manage the creation of the servers, theoperating system installation and configuration, and the Python and Javainstallation.It is most suitable for managing clusters installed on bare metal servers orvirtual machines. Use the Kubernetes with Helmsupport instead of Starburst Admin if you usecontainers and Kubernetes.Contact Starburst Support for the binary package, and further help.    Note:   The legacy Presto Admin is deprecated, and nolonger supported for Starburst Enterprise version 354-e and higher.RequirementsDeep knowledge of Ansible is not expected for usage, but familiarity withAnsible is helpful. At a minimum, it is assumed you have Ansible installed, andare familiar with running Ansible playbooks.Requirements for control machineThe control machine is used to run Starburst Admin, and therefore Ansibleplaybooks. Standard Ansible requirements apply:  Ansible 2.10 or higher  Linux/Unix operating system  Python 2.7 and higher, or Python 3.5 and higherIn addition, the following resources are needed:  SSH connectivity to the cluster nodes  Starburst Enterprise platform (SEP) or Trino tar.gz or RPM archiveRequirements for cluster nodesAll cluster nodes need to fulfill the normal Starburst Enterprise platform (SEP) orTrino requirements:  Linux operating system  Java runtime environment  PythonInspect the requirements for the specificversion for detailedversion information.Additional requirements:  Enabled ssh access and connectivity from the control machine  rsync  bashWhen using Starburst Admin with an RPM archive:  RPM-based Linux distribution  rpm command, yum, dnf, or others are not requiredWhen Starburst Admin with an tar.gz archive:  GNU tar command  unzip command"
 },
 {
  "title": "Notebook manager",
  "url": "/starburst-galaxy/notebook-manager/index.html",
  "content": "Notebook managerThe Starburst Galaxy platform (SGP) notebook manager enables query management,visualizations, data source exploration, and querying your connected datasources.You can access a notebook manager for each cluster in the SGPuser interface.SQL queries are stored in notes. One or more notes are managed as a collection -a notebook. Notebooks are stored in folders.Each note contains a SQL query statement. The notes can be used to actuallyexecute queries and inspect the results in table format, or createvisualizations.Here’s a quick overview of the available feature helps you accomplish:  Query management: Create and organize your folders, notebooks and notesfor easy access and sharing.  Visualizations: Create diagrams and charts with the data from your datasources, including bar charts, time series plots, and others.  DB exploration: Browse all connected data sources, schemas, tables, andthe contained data.  Search: Search the notes of all users.  Important: The notebook manager displays in a new tab in your browserStarburst GalaxyThere are three main sections of the notebook manager:  My Notebooks: A list of all of your notebooks.  Favorites: A list of your saved notebooks for quick access. Select the icon to favorite a notebook.  History: The query logs. The history shows all of your queries whether yousaved them in a note or not."
 },
 {
  "title": "Manage users",
  "url": "/starburst-galaxy/admin/index.html",
  "content": "Manage usersUsers gain access to Starburst Galaxy by invitation fromSGP admins. Invitations specify the role of the new user:  User: Write SQL queries and run them in the notebook manager or otherdata analytic tools. Users are often calleddata consumers.  Admin: Manages all clusters, data sources, notebooks, users, and settings.Admins provide users with connection details so they can run queries in notebooks or the clients of their choice.Admin capabilitiesOn the Users page an admin can invite, edit, or delete users and otheradmins. Admins can also track user logins to Starburst Galaxy.Invite usersInvite a user at any time from the Users page in the Admin section.  Click Admin and select Users.  Click + New.  Enter the Email address and Username.  Change to the new role type, either User or Admin.  Click Save to send the user an invitation email. To invite more than one user,select Add another and repeat the process.Edit usersChange the user role types for other users and admins.If you change the role type from user to admin, the former admin can no longeredit, add, or delete data sources and clusters.If you upgrade a user to an admin, the former user can now access theStarburst Galaxy dashboard and the Data sources and Clusterspages. They can now add, edit, and delete data sources and clusters.  Click Admin and select Users.  Click  options and select Edit user nextto the user whose role you want to edit.  Select the new role type User or Admin.  Click Save to update the user account.Delete usersIf you delete an admin, the resources that the admin added toStarburst Galaxy stay but no longer list an owner. This means that anyadmin can manage those resources.Delete other users and admins from Starburst Galaxy.  Click Admin and select Users.  Click  options and select Delete user.  Confirm and click Save to complete removing a user.Delete multiple users or admins at a time from Starburst Galaxy.  Click Admin and select Users.  In the users list, select the users you wish to delete by checking the boxnext to the email address.  Click Actions and select  Delete. TheActions button only displays when one or more users is selected."
 },
 {
  "title": "Starburst for data engineers",
  "url": "/data-engineer/index.html",
  "content": "Starburst for data engineersStarburst Enterprise platform (SEP) is a fast, interactive distributed SQL query enginethat decouples compute from data storage. SEP lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single SEP query can combine data from all these datasources and more.Starburst can greatly reduce reliance expensive, complex and oftenbrittle ETL frameworks and their pipelines. Because it uses data instead of diskto execute queries across the cluster, it’s also fast. SEP canpull your landing times forward, and help you meet or beat your SLAs.How does this work?SEP comes with 30+ supported enterpriseconnectors includingexclusive connectors not available in open source, providing high performanceSQL-based access to most of the data platforms in your organization - such asTeradata, Postgres, and Hive. Each data platform is defined as aSEP catalog. Catalogs, in turn, define schemas and their tables.Catalogs also, at a minimum, define the connector that SEP usesto connect to that data source:connector.name=sqlserverconnection-url=jdbc:sqlserver://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;;database=&amp;lt;database&amp;gt;connection-user=rootconnection-password=secretOnce you have your connection established, many connectors have configurationproperties that help you tune the connector’s performance, such as for timeouts,retries and connection limits.                                                              ×                                              SEP uses an ANSI-compliant SQL that should feel comfortable andfamiliar. SEP takes care of translating your queries to thecorrect SQL syntax for your data source. If you are migrating from Hive, we havea migration guide in ourdocumentation.How do I get started?As a first step you should read our guide to choosing the right deployment.If your organization does not already have SEP, you can request atrial instance, following the steps in TryingStarburst products.If you do already have SEP, as a data engineer, you need thenames and access credentials to the cluster’s coordinator and worker nodes, aswell as our JDBC orODBC driver. We also have ahandy CLI for you to use.For your next stop, see our data engineer’s userguide."
 },
 {
  "title": "Metadata overview",
  "url": "/starburst-galaxy/metadata/index.html",
  "content": "Metadata overviewThe Starburst Galaxy supports metadata, including metastores and tags.Add, edit, or remove:  Metastores  Tags"
 },
 {
  "title": "Client overview",
  "url": "/data-consumer/clients/index.html",
  "content": "Client overviewStarburst let’s you query all the connected data sources with the mostwidely used and best supported query language - standard SQL. You can even querymultiple data sources with the same query. Simple queries, and these morecomplex federated queries all access the data right in the source. There is noneed for ETL processes.As a data consumer you can use any supported client tool to connect to yourStarburst cluster, and execute your queries. Many tools allow you muchmore powerful usage than simply running custom written queries. You can createcomplex reports, charts, dashboards, and many other useful results.Connection informationThe details you need to connect to Starburst are independent of yourtools of choice:  URL of the Starburst cluster, including the port used.  Credentials, typically username and password.Ask your Starburst platform administrator for this information.Let’s look at some examples:A simple test installation on your local computer, using the default port and noTLS configuration:  http://localhost:8080  username can be a random string like your first name since no authorization isconfigured  no passwordThe same simple test application running on a different server:  http://starburst.example.com:8080  random username string  no passwordIf you configure TLS, you typically use a load balancer or proxy. In this case,the default port is generally used, and the protocol changes to https:  https://starburst.example.com  random username string  passwordTLS is a requirement for authorization against a provider’s data, such as yoursite’s LDAP directory. In this case, you must use real credentials:  https://starburst.example.com  LDAP username  LDAP passwordOther authorization providers may require further credentials. Support for otherproviders varies among client tools.Determine cluster versionThe version of SEP running on the cluster determinescompatibility of suitable clients. Specifically important is whether the clusteris using SEP 354-e or newer, or an older release prior to therename of the open source project. Newer versions use the name Trino, whileolder releases up to 350-e use the Presto or PrestoSQL name.You can determine the version of the cluster by asking your platformadministrator, or with one of the following methods, depending on your access:  Connect a client or browser to the v1/info REST endpoint. For example,connect to http://starburst.example.com:8080/v1/info.  Connect a modern web browser to the cluster and log in to view theSEP Web UI. Forexample, connect to http://starburst.example.com:8080/ui. The versionnumber is shown on the right side of the top row.  Connect the SEP CLI and run the following query:select * from system.runtime.nodes;General adviceGeneral advice for using clients is the following:  For clusters running SEP version 350 or earlier, use version350 of the client.  For clusters running SEP version 350, you can use clientversion 350, 354 or newer to help with migration.  For clusters running SEP versions 354 and later, use the sameversion of the client as the cluster or a newer client.This applies for the CLI and JDBC driver. Some clients, such as ODBC driver,have separate versioning, and details are documented with the client. Otherclients, such open source tools like DBeaver, use the Trinoname for versions 354 or newer and PrestoSQL or Presto for older versions.Examples:Starburst Enterprise version 345-e LTS recommended clients:  CLI 350  JDBC driver 350  DBeaver with PrestoSQL connectionStarburst Enterprise version 354-e LTS recommended clients:  CLI 354  JDBC driver 354  DBeaver with Trino connectionStarburst client tools and driversStarburst offers a number of supported clients and tools  Command line interface (CLI) for shell scripts and manualexecution using a terminal  Java Database Connectivity (JDBC) driver, typically forJVM-based applications and others with JDBC support  Open Database Connectivity (ODBC) driver, typically for forWindows-based applications and others with OBDC supportThe JDBC and ODBC driver can be used for many other clients, and you can findinstructions and specifics tips for the following tools:  Tableau  Microsoft Power BI  DBeaverOther client tools and driversIn addition, the Trino community provides the following clients:  trino-python-client  trino-go-clientThe wider open source community maintains numerous other clients andtools that can be used.Other resourcesThe O’Reilly book, Trino, the DefinitiveGuide, was writtenhere at Starburst, and contains some great information on gettingstarted with using different types of clients with Starburst. It’savailable for free!"
 },
 {
  "title": "Data engineer user guide",
  "url": "/starburst-enterprise/data-engineer/index.html",
  "content": "                    Starburst Enterprise                                      Data engineer user guide      Anytime you need help configuring and securing      catalogs or developing custom connectors, start here.      This guide takes you to the most up-to-date information.      And if you have not seen it already, read our        guide to choosing the right        deployment.                          What you need to know                                                                                          Catalogs              Defining, configuring, and securing data sources                                                                                                                Query performance              Using the query optimizer and session properties to tune SEP                                                                  Search by topic      We&#39;ve also gathered some resources on popular topics for you.                                                Migrating to SEP                                                                    Query federation                                                                    Custom connectors                                                                    SQL in Starburst                              "
 },
 {
  "title": "Starburst Galaxy",
  "url": "/starburst-galaxy/index.html",
  "content": "                    Starburst Galaxy                              Learn more                                Request access                                          All the power of        Starburst Enterprise        and much more, running in the cloud, managed for you by Starburst.                                                            Setup AWS                                                                                Data sources                                                                                Clusters                                                                                Notebooks                                                  Learn about querying and using Starburst Galaxy with your BI,        reporting, or SQL tool of choice.                  Data consumer resources                    "
 },
 {
  "title": "Trying Starburst Enterprise",
  "url": "/starburst-enterprise/try/index.html",
  "content": "Trying Starburst EnterpriseStarburst makes it easy to set up trial configurations ofStarburst Enterprise platform (SEP).Up and running fastUse a local machine installation to getStarburst Enterprise running quickly as a demo or trial system for yoursite’s developers.On any Linux distro, you can:      Set up SEP as a cluster of one or moreDocker containers        Set up SEP with the tar.gz installation  Both installation methods can be installed without a SEP license,or to unlock the performance and security enhancements of SEP,you can request a license as described in the linked pages above.For initial evaluation only, you can use either method on recent releases ofmacOS. MacOS is explicitly not supported to run SEP servicesfor any other purpose.Consider the following ways to install and evaluate SEP,listed in ease of use order.SEP on cloud provider infrastructureYou can deploy and run Starburst Enterprise on the infrastructure of manycloud providers using their marketplaceofferings.SEP managed on a Kubernetes clusterYou can install Starburst Enterprise on a Kubernetes cluster, takingadvantage of Kubernetes abstractions to free you from the details of servers,hosts, and IP addresses.Starburst recommends using Helm charts to configureyour Kubernetes cluster as described in Kubernetes withHelm.You can install Starburst Enterprise on a private Kubernetes configuration ona local network or on a cloud provider’s Kubernetes offering, including:  Amazon Elastic Kubernetes Service (EKS)  Google Kubernetes Engine (GKE)  Microsoft Azure Kubernetes Service (AKS)  Red Hat OpenShiftYou must provide:  Access credentials from your cloud provider.  The URLs and access credentials for the data sources you want yourKubernetes-hosted SEP cluster to access.SEP with full local controlYou can install and configure a Starburst Enterprise cluster yourself on alocal network or on a cloud installation that you control.The advantage of this is that you manage and control all aspects of yourSEP cluster.The disadvantage is the same: troubleshooting cluster problems requiresknowledge of and access to your configuration settings.You can configure a local Starburst Enterprise cluster:  On a local Kubernetes cluster using Helm charts.  On a local network of individual machines without Kubernetes.To install on one or more local machines, you have several options:  For RPM-based Linux distros, use thepresto-admin tool.  For any Linux distro, set up SEP with thetar.gz installation.  For any Linux distro, set up SEP as a cluster ofDocker containers."
 },
 {
  "title": "Microsoft Azure",
  "url": "/marketplace/microsoft/index.html",
  "content": "Microsoft Azure    Microsoft Azure marketplace    Starburst offers the following support for our marketplace subscriberswithout an enterprise contract:          Email-only support      Five email issues to azuresupport@starburstdata.com per month      First response SLA of one business day      Support hours of 9 AM - 6 PM US Eastern Time        Starburst Enterprise platform (SEP) is available in the Microsoft Azuremarketplace.    As an alternative, you can use the Microsoft Azure Kubernetes Service (AKS)directly. AKS is certified to work with SEP.    More information applicable to both scenarios in available in our Kubernetesreference documentation, including ourcustomization guide forSEP.    We also have a helpful installationchecklist for an overview of the generalHelm-based installation and upgrade process.          "
 },
 {
  "title": "Starburst for data consumers",
  "url": "/data-consumer/introduction.html",
  "content": "Starburst for data consumersIf you champion data-driven decisions in your org, Starburst hasthe tools to connect you to the data you need. Starburst brings allyour data together in a single, federated environment. No more waiting for dataengineering to develop complicated ETL. The data universe is in your hands!Starburst Enterprise is a distributed SQL query engine. Maybe you know asingle variant of SQL, or maybe you know a few. Starburst’s SQL isANSI-compliant and should feel comfortable and familiar. It takes care oftranslating your queries to the correct SQL syntax for your data source. All youneed to access all your data from a myriad of sources is a single JDBC or ODBCclient in most cases, depending on your toolkit.Whether you are a data scientist or analyst delivering critical insights to thebusiness, or a developer building data-driven applications, you’ll find you caneasily query across multiple data sources, in a single query. Fast.How does this work?Data platforms in your organization such as Snowflake, Postgres, and Hive aredefined by data engineers as catalogs. Catalogs, in turn, define schemas andtheir tables.  Depending on the data access controls in place, discovering whatdata catalogs are available to you across all of your data platforms can beeasy! Even through a CLI, it’s asingle, simple query to get you started with your federated data:presto&amp;gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows)After that, you can easily explore schemas in a catalog with the familiar SHOWSCHEMAS command:presto&amp;gt; SHOW SCHEMAS FROM hive_sales LIKE `%rder%`; Schema--------- order_entries customer_orders(2 rows)From there, you can of course see the tables you might want to query:presto&amp;gt; SHOW TABLES FROM order_entries; Table------- orders order_items(2 rows)You might notice that even though you know from experience that some of yourdata is in MySQL and others in Hive, they all show up in the unified SHOWCATALOGS results. From here, you can simply join the data sources fromdifferent platforms as if they were from different tables. You just need to usetheir fully qualified names:SELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &amp;gt; 300LIMIT 2;How do I get started?The first order of business is to get the latest StarburstJDBC orODBC driver and get itinstalled. Note that even though you very likely already have a JDBC or ODBCdriver installed for your work, you do need the Starburst-specificdriver. Be careful not to install either in the same directory with other JDBCor ODBC drivers!If your data ops group has not already given you the required connectioninformation, reach out to them for the following:  the JDBC URL - jdbc:presto://example.net:8080  whether your org is using SSL to connect  the type of authentication your org is using - username or LDAPWhen you have that info and your driver is installed, you are ready to connect.What kind of tools can I use?More than likely, you can use all your current favorite client tools, and evenones on your wishlist with the help of our tips andinstructions.                                                              ×                                              How do I migrate my data sources to Starburst?In some cases, this is as easy as changing the sources in your FROM clauses.For some queries there could be slight differences between your data sources’native SQL and SQL, so some minor query editing is required. Rather thanchanging these production queries on the fly, we suggest using your favorite SQLclient or our ownCLI to test yourexisting queries before making changes to production.If you are migrating from Hive, we have a migrationguide in ourdocumentation. To help you learn how others have made the switch, here is ahandy walk-through of usingLookerand Starburst Enterprise together.Where can I learn more about Starburst?From our documentation, of course! Visit our data consumer’s user guide."
 },
 {
  "title": "JDBC driver",
  "url": "/data-consumer/clients/jdbc.html",
  "content": "JDBC driverThe Java Database Connectivity (JDBC) driver enables any application supportinga JDBC driver to connect to Starburst clusters. The application can thenissue SQL queries and receive results.Typically the applications are JVM-based, or support JDBC driver usage with adifferent mechanism. Using an applications with the JDBC driver generallyfollows the same steps.Connection informationGet the connection information and thecluster version for the cluster you wantto connect to and use to run queries.Configure the driverAdding the JDBC driver to your application varies for each application.Some applications, such as DBeaver, automatically download andconfigure the driver, or include it by default and no action is necessary.Many applications, however, require you to download the driver and place it in adirectory that varies for each application. Refer to the documentation of yourclient application for the necessary details.The JDBC driver needs to be compatible with the version of the cluster you areconnecting to.Download the JDBC driverYou can request the CLI for a specific version fromStarburst Support.Alternatively, to gain access to SEP archives, visit theStarburst website and click either the GetStarted or Start Free buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form using a valid email address, then click Free Download.Click on the link to the Downloads page in the resulting email. It isorganized into Long-Term and Short-Term Support sections.Download a specific version of the CLI, based on your cluster version:  Version 350, if your cluster version is older than 350  Same or newer version than the cluster for versions 350, 354 and newerYou can also download the JDBC driver binary straight from the link in thereference documentation:  Version 350  Version 354  Latest versionUse the version selector in the documentation for different releases, or contact|support|.Create the connection to the clusterWith a configured driver you can now configure a connections to a cluster.Typically you only need the connection information. Other details such as theJDBC URL are automatically configured in many applications, or can be seen andupdated in an user interface dialog.The main parameters are the driver classname, which is typically preconfiguredby your client driver integration, the JDBC URL and the credentials from theneeded connection information.Driver version 350 and older:  Classname: io.prestosql.jdbc.PrestoDriver  JDBC URL: jdbc:presto://host:portDriver version 354 and newer:  Classname: io.trino.jdbc.TrinoDriver  JDBC URL: jdbc:trino://host:portIf your application does not include these out of the box, check the referencedocumentation for the JDBC driver forthe correct values and other supported parameters.Start queryingThe configured connection, can now be opened and you can start running queries.For example, if you application allows you to write and run queries you can seewhat catalogs are available:SHOW CATALOGS;Many applications provide a user interface to see the list of catalogs, schema,tables and much more.Next stepsNow you can take advantage of the features of your application, and potentiallylearning more about the SQL support in Starburst."
 },
 {
  "title": "Trying SEP on Kubernetes",
  "url": "/starburst-enterprise/try/k8s.html",
  "content": "Trying SEP on Kubernetesusing the Helm charts on Kubernetes clusteron local workstation using kind ?"
 },
 {
  "title": "LDAP authentication",
  "url": "/starburst-galaxy/ldap-auth.html",
  "content": "LDAP authenticationThe LDAP configuration enables users to authenticate to theStarburst Galaxy platform (SGP).It also allows you to use the same credentials to connect with client tools,such as the PrestoSQL CLI, or an application using the JDBC or ODBCdriver. Once connected you can use them to run queries against the data sourcesconfigured as catalogs.The following properties need to be configured for your LDAP authenticationssupport. Work with the administrator of your LDAP server to determine thecorrect values.      Property    Description        URL    The combination of the protocol, FQDN, and port used by your LDAP server.      Protocol can be ldaps:// or ldap://        User bind pattern    The pattern used to look up the relevant users in your LDAP directory.      An example is ${USER}@ldap.example.com        Group authorization query    Queries the LDAP user groups        Distinguished name    Starburst Galaxy service user identity        Bind password    Authorizes the user bind pattern          User distinguished name    Service user identity        Cache TTL    Time interval to cache credentials after first authentication, and      before authentication is required again  Consult the LDAP documentation forSEP for more details,and work with your LDAP server administrator.Edit LDAP cluster settingsEdit your cluster’s LDAP settings from the Clusters page.  Locate the cluster whose LDAP settings you want to edit  Select the Options icon (three stacked dots), click Edit  Select the LDAP configuration tab  In the Configure LDAP properties section, edit your properties  When you complete your edits, click Update to save your new configuration  To apply your changes, restart your cluster"
 },
 {
  "title": "Markdown usage",
  "url": "/internal/markdown.html",
  "content": "Markdown usageThis page shows the output of all the supported markdown syntax usage. The siteuses CommonMark.Markdown source files  Standard extension is .md  80 character hard wrap width for markdown code  Embedded HTML can be wider  Use 2 space indent for HTML (no tab, not wider)Section titlesTitles are marked with one or more #.Before title..Level 1after title and before next one, should only be used for page titleLevel 2after title and before next oneLevel 3after title and before next oneLevel 4after title and before next oneLevel 5after title and before next one, too deep, probably should never be usedLevel 6after title and before next one, too deep, probably should never be usedText formattingNormal text in a paragraph. You can just write along. Spaces between word orline breaks don’t matter. An empty line starts a new paragraph.Use _ or * to highlight words in italics text. Use __ or ** tohighlight words in bolded text. Don’t mix bolding and italics.  bold with underscore  bold with double asterisk  italics with underscore  italics with asteriskStandard usage at Starburst is to use asterisk *bold*.Code blocks and other source codeInline usage of source code uses simple backticks to surround the variable orsimple command:Add ~/bin to the PATH with EXPORT PATH=~/bin:$PATH.Separate code block with three backticks:cd /opt/dev/myprojectmvn clean installYou can declare a language like shell after the initial three backticks toget syntax highlighting.Here is a shell fenced block:cd /opt/dev/myprojectmvn clean installA java fenced block:String a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an intA yaml fenced block:image:  repository: &quot;harbor.starburstdata.net/starburstdata/hive&quot;  tag: &quot;338.2.2-rc.1-SNAPSHOT&quot;  pullPolicy: &quot;IfNotPresent&quot;expose:  type: &quot;clusterIp&quot;  clusterIp:    name: &quot;hive&quot;    ports:      http:        port: 9083  nodePort:    name: &quot;hive&quot;    ports:      http:        port: 9083        nodePort: 30083A sql fenced block:SELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]If for some reason you really must have line numbers, then you must use a liquidhighlight block. This java highlight block uses the linenos keyword todisplay line numbers:12String a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an intHowever, if you find yourself needing to refer to line numbers, your codeblockis likely too long for your purpose. Consider instead a one- to two-line excerptto illustrate what you are writing about, even if you keep the longer codeblockintact.Unordered listsUse - or * for the items.  Apple  Pear  BananaYou can also indent:  Pets          Dog      Cat        Farm animal          Cow      Sheep      Ordered listsThe actual numbers in your document’s source code aren’t used as the line numbervalues in static documents. The line numbers you see in the generated documentsare  computed. So you can use e.g. 1. all the time. This example uses 1. for alllines in the source document, but the generated document are numbered correctly:  Item 1  Item 2          Nested 1 (nested items in lists require 4 spaces to indent properly)      Nested 2        Item 3Definition listsUse dl/dt/dd HTML:  Term 1  the description for term 1  Term 2  the description for term 2Links  Markdown syntax with [text](url) is preferred  Relative links are preferred to avoid issues with the root context  Ideally use (/file.html  for absolute linksTablesDirect layout markup:            Tables      Are      Cool                  col 3 is      right-aligned      $1600              col 2 is      centered      $12              zebra stripes      are neat      $1      Semantic markup:            Markdown      Less      Pretty                  Still      renders      nicely              1      2      3      Raw HTML:            First name      Last name                  Alison      Loney              Manfred      Moser      VideosEmbedding YouTube videos is supported courtesyhttps://github.com/nathancy/jekyll-embed-video. video-embed.css is included tomake it responsive, but that is untested. Other sources can be supported, butYouTube is all we need for now.You must include the video ID you want to use in the front matter.---youtubeId1: buqtdpuZxvk---You can have multiple videos on a page, you just have to give them differentnames in the front matter.If you want to use an entire video, just use youtubePlayer.html in your include,with the :include youtubePlayer.htmlIf instead you want to play just a portion, use youtubeSnippet.html in yourinclude, and provide start and end values in seconds after the video id:id=page.youtubeId1 start=22 end=31At this point, you might be wondering to yourself, “But what if I want to startat a particular place, then play until the end?” Great question! In that case,just set the value for end to -1:id=page.youtubeId1 start=22 end=-1You’ll have to read the .md file for now to see the entire include, as it justrenders the included HTML as html in a comment:          If you are looking to include a Wistia video, use the wistiaPlayer.html in your include.You can customize the start time by using the start property with the following format start=&#39;1m30s&#39;.*You must use either single or double quotes when adding a property.{% include wistiaPlayer.html id=&#39;j38ihh83m5&#39; %}  &amp;nbsp;If you just want to add a admonition with a text to a video, use thevideo.html. Careful about the apostrophes used for the embedded URL.    Video: Check out the TrinoCommunity Broadcast interview with the Apache Superset teamto learn more and see a demo.AdmonitionsThe simplest admonition is just using a blockquote.  This is a simple blockquote.And after this insightful blockquote, you can have a look at a even biggerblockquote.  And here is a longer one.  Its has two sentences. And also      a list item    another item  Beyond that you can use specific warning, caution and note admonitions.As per Google developer documentation(GDD) a warning is: “Stronger thana Caution; it means “Don’t do this.”Note that multi-line, paragraph-shaped passages work in the styling:   Warning:Like a red morn that ever yet betokened,Wreck to the seaman, tempest to the field,Sorrow to the shepherds, woe unto the birds,Gusts and foul flaws to herdmen and to herds.William ShakespeareA caution, which, as per GDD “Suggests proceeding with caution”:   Caution:  Do not be tricked into thinking that there areno crocodiles just because the water is still. - Malaysian proverbA note (GDD: “An ordinary note or tip”):    Note:   Try using a small LIMIT when testing out a newquery.A note with a long link to see if it breaks/wraps:    Note:   This is an example note to see how the line breakis handled by css.https://starburstdata.atlassian.net/wiki/spaces/PM/pages/1080688861/Starburst+Galaxy+UX+Research-let+saddareallylong+linkYou can also use capture to create a large section ofcontentand then pass that to the admonition.Side navigationTo include multiple levels of navigation you will need to use the following ymlsyntax:nav:  - title: SEP Overview    context: /starburst-enterprise/index  - title: Platform administrators    id: admins    collapse: /starburst-enterprise/platform-administrator    subnavigation:      - title: Home        context: /starburst-enterprise/platform-administrator/index      - title: SEP clusters        id: test        collapse: /starburst-enterprise/platform-administrator/cluster        subfolderitems:         - title: Test           context: /starburst-enterprise/platform-administrator/cluster/test"
 },
 {
  "title": "Metabase",
  "url": "/data-consumer/clients/metabase.html",
  "content": "Metabase"
 },
 {
  "title": "Metastore overview",
  "url": "/starburst-galaxy/metadata/metastores.html",
  "content": "Metastore overviewThere are two places in the Starburst Galaxy platform (SGP) where you can addmetastores:  Metadata page  Data sources pageAdd metastore from the Metadata pageAdd an AWS Glue metastore or an external Hive metastore starting from theMetadata page.Add an AWS Glue metastoreAdding an AWS Glue metastore connection details allows access to metadata andmapping information about the objects stored in AWS to SGP.  Click Admin and select Metadata.  In the Metastores section, click + New.  Select the AWS Glue metastore location.  Enter a Name for your AWS Glue metastore.  Select the AWS Glue region from the drop down menu.  Select Secret key or Assumed IAM role.          Secret key: Enter the Access key ID and your Secret access key name. The secret key ID is the name of your secret key in your secret manager.      Assumed IAM role: Enter the IAM role ARN.        Click Finish to save and return to the Metadata page.Review metastore information on the Metadata page.Add an external Hive metastoreAdding an external Hive metastore connection details allows access to metadataand mapping information about the objects stored in AWS to SGP.  Click Admin and select Metadata.  In the Metastores section, click + New.  Select the External Hive metastore metastore location.  Enter the Hive metastore URI.  Click Show authentication options to add other authentication details, such as:          Hive metastore username.      Thrift client SSL enable (check the True box).      Thrift client SSL trust certificate.      Thrift client SSL trust certificate password.        Click Next.Review metastore information on the Metadata page.Add a new metastore while adding a data sourceInstead of adding a metastore from the Metadata page, add one while addinga data source SGP.  Add an AWS Glue metastore  Add an external Hive metastoreLearn about AWS Glue metastoresand external Hive metastores."
 },
 {
  "title": "Migrating your analytics to Starburst",
  "url": "/data-consumer/migration.html",
  "content": "Migrating your analytics to StarburstIn some cases, migrating your analytics to Starburst is as easy asswapping out your client, such as a JDBC driver, and changing the sources inyour FROM clauses. But because not every data source implements the SQLstandard the same, there could be slight differences between your data sources’native SQL and Starburst’s SQL. The information on this page will getyou started moving your tools and workflows to Starburst.Getting and installing Starburst clientsStarburst has JDBC and ODBC drivers to connect your favorite tools to yourfavorite data. We have an entire documentsection that covers downloading, installing andconnecting clients.Migrating queries from ANSI-standard SQL implementationsFor SQL implementations that follow the ANSI standard closely, only minor queryedits are likely. Rather than changing these production queries on the flythough, we suggest using your favorite SQL client or our ownCLI to test your existing queriesbefore making changes to production.Our full Starburst SQL referencemanual is available to help youresolve any small implementation differences as you migrate your queries toStarburst.Migrating queries from HiveIn other cases where a SQL implementation deviates significantly from the ANSIstandard, such as with Hive’s HiveQL, there some things you’ll want tokeep in mind. Our documentation covers the syntactic and semantic differencesbetween HiveQL and other non-ANSI standard implementations such as:  Array syntax and handling  Syntax for strings and identifiers  CAST considerations  Differences in datediff  Complex expressions and subqueries  INSERT and OVERWRITE operationsRead the Hive migration page in ourreference documentation for detailed information on these topics.Reducing ETLWith Starburst, there are many opportunities to both reduce yourreliance on increasingly complex ETL pipelines and the intermediate storage tocentralize data that you must transform and clean up tech debt while you do so.These are complex problems and it can be hard to know where to begin. Thefollowing sections offer some strategies that make approaching the problemeasier.Reduce or remove duplicative or similar processingIf you have multiple pipelines that ultimately produce the similar data with thesame grain, start there by combining those queries. For example, if youcalculate budget for ads at an ad_id level, and someone else is calculatinginvoices for the same ad_id grain, combine those queries. You save the doublecompute and storage demands so that you don’t scan the data twice, then move itaround twice, and finally write it twice with just one column being different.Optimize around end usageAs you are doing your due diligence for this migration, whether you choose to leave some pipelines in your current framework or not, take the opportunity to review your pipeline design against their purpose:  For end usage purely for downstream ETL, optimize for writes  For end usage for dashboards and reports, optimize for readsFor in-depth information on optimizing queries, dive into our trainingvideo onoptimizing query performance, and reference the queryoptimizer section of our referencedocumentation.Reduce or remove the need for temp tables and intermediate disk storageData products that are derived from disparate sources often need to land in anintermediate schema where they can be combined locally. With Starburst,you can simplify this type of processing and remove the need for temp tables andschemas by taking advantage of Starburst’s powerful query federationabilities. This has the additional, positive side effect of obviating the needfor managing temp table cleanup jobs, too.Reduce or remove little-used queries and data productsIf you are not already, start measuring the usage for dashboards and reports. Have any been abandoned? Has the usage shrunk enough on any of them so that there is no longer a justifiable ROI on maintaining them? If so, work with your stakeholders on a sunset plan, and remove the surfaces and pipelines.Next stepsOnce you have cleaned up your pipelines, start looking at where you can delightyour customers with more aggressive data landing times. Upstream data ETL andavailability will continue to set a lower bound on your landing times, but thereis a positive, domino effect from reducing pipeline complexity and the needto wait for slow compute and storage operations."
 },
 {
  "title": "ODBC driver",
  "url": "/data-consumer/clients/odbc.html",
  "content": "ODBC driverThe Open Database Connectivity (ODBC) driver enables any application supportinga ODBC driver to connect to Starburst clusters. The application can thenissue SQL queries and receive results.Typically the applications are Windows-based, or support ODBC driver usage on other operating systems. Using an applications with the ODBC driver generallyfollows the same steps.Connection informationGet the necessary connection informationfor your cluster.Download and configure the driverUnlike the ODBC driver, the ODBC driver requires a license. Get a licensed copyof the driver installation package from the Starburst support team.You can find more information in the ODBC driver referencedocumentation.Typically you have to install the OBDC driver and add it to your application.Adding it to your application varies for each application. Some applications,such as Power BI include the driver. Refer to thedocumentation for details for your application.Create the connection to the clusterWith a configured driver you can now configure a connections to a cluster.Typically you only need the connection information. Other details such as theODBC connection are automatically configured in many applications, or can beseen and updated in an user interface dialog.Start queryingThe configured connection, can now be opened and you can start running queries.For example, if you application allows you to write and run queries you can seewhat catalogs are available:SHOW CATALOGS;Many applications provide a user interface to see the list of catalogs, schema,tables and much more.Next stepsNow you can take advantage of the features of your application, and potentiallylearning more about the SQL support in Starburst."
 },
 {
  "title": "Tuning your cluster performance",
  "url": "/starburst-enterprise/platform-administrator/performance-tuning.html",
  "content": "Tuning your cluster performanceStarburst Enterprise platform (SEP) is a more feature-rich version ofTrino, formerly PrestoSQL, providing enhanced query performance, security,connectivity, and ease of use.Learn how to size your cluster and the machines in it to ensure the bestperformance possible for your workload in this training video presented by oneof our founders, Dain Sundstrom. For your convenience, we’ve divided the videotraining course up into topic sections, and provided links to the relevant partsof our documentation below.General tuning strategy &amp;amp; baseline advice                                    Topics:              Starting big        Stabilizing, then tuning        Options to disable            Running time: ~9 min.      Cluster sizing, and how SEP uses CPU and memory resources                                         Topics:              How memory affects JOIN, GROUP BY, ORDER BY and window functions        Availability        Concurrency            Running time: ~19 min.      Machine sizing and its impact                                     Topics:              Memory and memory allocation        Shared join hash        Distributed join        Skew        Machine sizes and types        Spilling        Small clusters            Running time: ~38 min.      Additional resources on memory management and spilling in SEP:  Memory management properties  Memory configuration  Spilling properties  Spill to disk  JVM SettingsTuning the workload                                    Topics:              Query plan        Precomputing        Connectors            Running time: ~16 min.      Hive data organization                                     Topics:              Organize your data for the Hive connector        Hive partitioning and bucketing        ORC and Parquet        File size        Bad parquet files        Rewrite table with the ORC writer            Running time: ~16 min.      Making queries faster                                     Topics:              What to look for in a query        Using more hardware        Underutilization        Hive caching            Running time: ~13 min.      For more in-depth information on this topic, watch our query optimization trainingvideo.Sharing resources, and resource groups                                     Topics:              Concurrency        User experience, expectations and satisfaction        Social engineering            Running time: ~3 min.      "
 },
 {
  "title": "Personas used for documentation and design",
  "url": "/internal/personas.html",
  "content": "Personas used for documentation and designThere are three primary audiences we write and design for:  Starburst platform administrators  Data engineers  Data consumers: analysts &amp;amp; scientistsThere is a single, secondary audience: “Data leaders.” This persona is largely acheck writer for our purposes (VP, CDO, CIO), and does not actually use SEP anydifferently than the primary personas. They are included here for completeness,and should be kept in mind when creating marketing content such as case studies,white papers and ROI-focused materials.This document describes these audiences as personas - fictional amalgamations -that you can empathize with and solve problems on behalf of. In practice,product personas are given names, faces and backgrounds to aid in discussingtheir needs as if they were real people, representative of our customers.Primary personasData consumers - analysts and scientistsData analysts and scientists will approach Starburst in very similar ways.However, their backgrounds and skill sets are different, so we will separatethose out. Their pain points will be treated together.              Chris Consumer    Early career    BSc in physics from University of Toledo, currently working on online MBA    Chris is a Business Analyst. He&#39;s responsible for delivering    visualizations and reports to ensure that his leadership is making    well-informed, data-driven decisions. Chris cares very deeply that not only    are the right questions being asked (and answered), but that the right    data is being used to answer the questions. With the wealth of data    available, it can be easy to overlook and misuse data. The quality of    Chris&#39;s work ultimately rests on the quality and reliability of the data he    uses, so Chris keeps good working relationships with his data engineering    team and often communicates discrepancies and SLAs issues to them. Chris has    some solid SQL chops and is often able to prototype a new data source to be    productionalized by data engineers. Chris feels that he has just the right    combination of technical skills and business acumen.  When you write, design and build for Chris, here are some of the skillsets you can expect him to have:  A reasonable level of skill with SQL, with some knowledge of more advancedqueries  Limited programming skills and methodologies  Tells stories with data  Expert with data visualization tools  Excellent spreadsheet skills, including some modeling  A good ability to detect and articulate issues with data, even if he cannotremedy them or trace the cause              Cameron Consumer    Early career    PhD in statistics, Stanford    Cameron is a data scientist. She&#39;s responsible for creating data models    and that forecast and describe the business. Cameron worries about the impact    of seasonality on sales, and feels compelled to deliver models that    reflect that impact with a high degree of accuracy. Cameron feels like she    brings the answers to &quot;Why?&quot; and &quot;How?&quot; to the table. Her machine learning    models help her find the levers that the business can pull - the &quot;how,&quot;    and her models account for why the business behaved as it did, or will.    She feels more like an academic than an engineer, and is very proud of her    scientific approach to business. Her digital sales data knowledge is    formidable, and her reputation as an SME ensures that she has a robust    stream of opportunities in her field.  When you write, design and build for Cameron, here are some of the skillsets you can expect her to have:  A reasonable level of SQL skills, with some knowledge of more advanced queries  Reasonable programming skills  Expert in statistical methods and/or machine learning  Some understanding of code repositories  Competence with data visualization tools  A good ability to detect and articulate issues with data, even if they cannotremedy them or trace the causeCameron’s and Chris’s pain points include, in no particular order:  Having to retrofit tools onto multiple data sources  Long waits for ETL to deliver useable data  Can’t dive into data quality issues  Data engineers sometimes kill their queries because of resource contention  Complex, periodic reports and models are often delayed past due datesData engineers                Donna Data Engineer      Mid-career      BSc in computer engineering, University of Illinois at Chicago      Donna Data Engineer is responsible for designing performant data      sources that can answer a broad range of business questions at XYZ, Inc.      Donna found her way to data engineering through internships in college; it      felt like a good blend between the technical chops required for      programming jobs, and the big picture, organizational nature of data that      she is naturally drawn to. As part of her job, she must understand what      data is currently available from what sources, and what new data is needed      to fill in any gaps. Donna has to work with stakeholders to source that      new data, be it from third parties or through new log entries, message      streams or product endpoints. Donna works pretty closely with data      analysts and scientists, and tries to anticipate their needs in order to      keep up with burgeoning data demands.                  Daniel Data Engineer      Mid-career      BSc in computer science, University of New Hampshire      Daniel Data Engineer is responsible for delivering data to data      analysts and data scientists at Acme Corp. Up until a few years ago,      this mostly entailed writing complex ETL in frameworks such as Informatica      and Alteryx. Over the last few years, he&#39;s worked mostly in python-based      frameworks such as Airflow and Bonobo as well as diving into Apache Spark.      Daniel really cares about data landing times, because him and his      coworkers hear from PagerDuty way more than they would like to.   When you write, design and build for Donna and Daniel, here are some of theskill sets you can expect them to have:  Creating and monitoring pipeline health metrics to ensure SLAs are met.  Enabling automated self-service pipelines using Infrastructure as Code (IaC)  Design schemas, data lake and data warehouse solutions in collaboration withstakeholders.  Building and managing Kafka-based streaming data pipelines  Building and managing Airflow- and Spark-based ETLs  Creating and updating data models &amp;amp; data schemas that reduce system complexityand cost, and increase efficiency  Preparing and cleaning data for prescriptive and predictive modeling anddescriptive analytics  Identifying, designing, and implementing internal process improvements such asautomating manual processes, optimizing data delivery for greater scalability  Creating data tools for analysts and data scientists  Building data integrations between various 3rd party systems such as Salesforceand WorkdayDonna’s and Daniel’s pain points, in no particular order:  Keeping up with the changing landscape of data delivery technology  Managing SLAs for data pipelines in environments where the data growth rateand complexity constantly increases, data pipeline and platform performance  Aligning and negotiating with upstream data sources and infrastructure SLAowners  Sussing out detailed data requirements from folks with a wide range of dataknowledge  Long, brittle pipelines  Productionalizing non-performant analyst queries  Constantly responding to resource constraint issues  Designing ETL around siloed data  Data cleansingPlatform administrators              Art Administrator    Late career    BSc in computer science, BYU    Art Administrator is responsible for XYZ, Inc&#39;s Starburst cluster. He was    an SRE for the data team for years, and switched roles to platform    engineering after leading the SREs for a bit. Art really cares about    scalability and reliability, especially since XYZ has super aggressive    SLAs both on data landing times and of course availability. Art works    closely with his colleagues in IT to ensure that his systems adhere to    XYZ&#39;s strict access policies and support audit requirements.                   Ada Administrator      Late career      BSc in computer science, University of Washington      Ada Administrator is responsible for both Acme Corp&#39;s Starburst and      Postgres clusters. Ada was a DBA from early to mid-career, and it fell to      her at Acme to figure out the HDFS ecosystem when it came along. Now she      builds and maintains big data clusters for a living. Ada cares a lot      about the using right data platform for the data.  When you write, design and build for Art and Ada, here are some of the skillsets you can expect them to have:  Building and maintaining scalable data platform architectures to support theingest, storage and querying of large heterogenous datasets  Creating and monitoring cluster health metrics to ensure optimal performanceand reduce any downtime  Writing clean, production-ready code (in Java, Go etc.) with a strong focus onquality, scalability and high performance  Using and building scalable asynchronous REST API’s  Working with cloud providers like AWS, Azure and Google Cloud  Implementing and working with persistence technologies like AWS S3, HDFS,Kafka and ElasticSearch  Designing for data integrity and security through all environments as well asthe data lifecycle  Partnering with data engineers to enable automated self-service pipelinesusing Infrastructure as Code (IaC)  Partnering with data engineers to design and improvement schemas, data lakeand data warehouse solutions in collaboration with stakeholdersArt’s &amp;amp; Ada’s pain points, in no particular order:  Sorting through an overload of information to master complex data platforms  Ensuring data platforms can scale to demand and with growth  Architecting solutions that can provide disaster recovery and businesscontinuity for complex, critical data systems, in conjunction with ITstakeholders  Assisting in managing budgets and licensing cycles for massiveenterprise-scale software vendors, bandwidth and hardware leases  Constantly tackling inherently complex and highly-visible tasks  Delivering against stringent infrastructure SLAs  Doing more with less, or at least the same team size  Implementing data governance requirements for all data systemsSecondary persona - data leader              Lauren Leader    Mid-to-late career    MBA, Haas School of Business    Lauren is CIO at the newly IPO&#39;ed Clouds &#39;R Us. She&#39;s responsible for    data infrastructure, data governance and delivery, as well as    enabling SOX, GDPR and CCPA compliance. Prior to stepping into her current    role, Lauren was a VP of IT at Acme Corp., where she owned the budget for    all data infrastructure. She calls this her &quot;real-life MBA,&quot; because she    learned the hard way from being caught off-guard by explosive growth in    under-specified legacy systems in multiple budget cycles. Lauren is also    sensitive to scaling, platform lock-in, and staffing around particular    technologies.  When you write for Lauren, here are some of her pain points to keep in mind, inno particular order:  Constantly fighting Shadow IT, up to and including small, narrow-scopeone-off data warehouse solutions which she inevitably must absorb  Architecting around legacy systems, particularly monolithic services  Changing regulatory climate  Staffing for innovation while keeping legacy systems running and trying toautomate  Managing, defending and demanding a budget with rapid growth  Balancing buy-vs-build, including for contracting services  Balancing private cloud vs hosted cloud solutions for cost-effectiveness,regulatory compliance and security"
 },
 {
  "title": "Microsoft Power BI",
  "url": "/data-consumer/clients/powerbi.html",
  "content": "Microsoft Power BI    You can use the popular analytics platform Microsoft PowerBI with your Starburst Enterprise cluster.    You can use the Starburst Power BI DirectQuery driver with the ODBCdriver to connect to your cluster.    Starburst Power BI DirectQuery driverThe Power BI DirectQuery driver can be used with the following tools:  Microsoft Power BI Desktop  Microsoft Power BI ServiceIt allows Power BI to let SEP perform all query processing. Thiscombines the scalability and power of SEP with the businessintelligence reporting features of Power BI.Requirements  Power BI 2.87.720.0 and higher, older versions of PowerBI support only importmode.  The driver requires the OBDC driver installed on yourworkstation.Connecting with Power BI DesktopThe driver is included as part of Power BI 2.87.720.0 and higher, and you canconnect with it easily.  Access the Get Data dialog  Select Other  Use Starburst Enterprise  Choose your Data Connectivity mode and proceed with OK  Configure the necessary connection and authentication detailsfor your cluster. More information on authentication can be found in thefollowing sections.  After the connection is established, you can use the Data Navigator tobrowse catalogs, query data sources and more.The driver always tries to connect with an encrypted connection first. If youare connecting without TLS/SSL, the connector offers the option to connect usingan unencrypted connection afterwards.To use a TLS encrypted connection to your cluster, make sure the server uses aglobally trusted certificate.If this is not the case, the server’s certificate needs to be added to thesystem trust store (Certificates -&amp;gt; Trusted Root Certification Authorities)​before connecting. The certificate can be added for the machine, or for eachuser running the Power BI connector.  In many organizations this handledautomatically as part of the operating system and browser configuration.LDAP authentication with Power BI DesktopIf your cluster is configured to use LDAP authentication, select LDAP in theauthentication window when connecting and provide your username and passwordcredentials.Kerberos authentication with Power BI DesktopTo use Kerberos authentication, Kerberos must be installed for the user andinitialized using kinit, before using the driver. This establishes your usercredentials on the machine.Select Kerberos in the authentication window, when connecting, and providethe Kerberos Service name.Connecting with Power BI serviceUsing the Power BI service, requires you to have the on-premises datagateway_ for Power BI installedon your machine.You have to request the driver as a .mez file from Starburst Support,when using the Power BI service.As a next step you have to copy the file into %USER%DocumentsPower BIDesktopCustom Connectors for each user. Create the directory, if it does notexist.The gateway service account needs to have sufficient permissions to access theCustom Connectors directory, C:Users&amp;lt;Name&amp;gt;DocumentsPower BIDesktopCustom Connectors inside your Power BI installation.Update the directory properties in the Security - Advanced tab. Add accessthat grants Basic Permissions - Full Control to Everyone/AuthenticatedUsers. Alternatively, you can also create a new group and add the gatewayservice account to a new group.Update the value for Load custom data connectors from folders in theConnectors configuration of the gateway to point at the folder in your PowerBI Desktop installation.Login to Power BI Online and update the Gateway Cluster Settings. Enable thesetting Allow user’s custom data connector to refresh through this gatewayclusters.Now you can add a data source:  Login to the Power BI service  Navigate to Setting -&amp;gt; Manage gateways  Select the gateway -&amp;gt; Add data source  Set the Data Source Type to Starburst Enterprise  Enter the corresponding host and port  Set the authentication as desired  Click Add to create the data source and begin using itLimitations  Self-signed certificate usage for TLS/SSL connections is not supported.  Writing and using custom SQL statements is not supported.  ODBC driver can be used for custom SQL but it does not support direct querying.  Authentication type and field name customization does not apply on Power BIService. The following is the mapping of the Service field names to theirDesktop counterparts:            Service name      Desktop name                  Basic      LDAP              Key      Kerberos              Key: Account Key      Kerberos: Service Name      Release notesVersion 2.0.0April 2021:  Remove beta flag.  Change name to Starburst Enterprise as breaking change. All existingreports break, and all users need to recreate their queries."
 },
 {
  "title": "Introduction to query federation",
  "url": "/data-consumer/query-federation.html",
  "content": "Introduction to query federationStarburst Enterprise is the world’s fastest distributed SQL query engine. It letsdata consumers query anything, anywhere, and get the data they need in a singlequery, no matter where it lives. This idea of combining data from disparatesources is called query federation. It allows you to combine, for instance,historical data from HDFS or objects stores with the most recent incoming datafrom Kafka one query.Federating data is simple. You just need to use the fully-qualified name of thetables in your FROM clause. Table names are fully-qualified when theyinclude the catalog name:&amp;lt;catalog&amp;gt;.&amp;lt;schema&amp;gt;.&amp;lt;table&amp;gt;A catalog defines the schemas in a data source such as Snowflake,Oracle and Hive.Here’s an example of data from two different sources, Hive and MySQL, federatedinto a single query:SELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &amp;gt; 300LIMIT 2;This query uses data from the following sources:  The orders table in the order_entries schema defined in the hive_sales catalog  The customer_master table in the sf_history schema defined in the mysql_crm catalogTo help you learn how Starburst uses federated queries in popularanalytics tools, here is a handy walk-through of federating queries for rapidvisualization inLookerwith Starburst Enterprise."
 },
 {
  "title": "Optimizing query performance",
  "url": "/starburst-enterprise/data-engineer/query-performance.html",
  "content": "Optimizing query performanceStarburst Enterprise platform (SEP) is fast. But did you know that there are stillmany opportunities to make it even faster depending on how you write yourqueries?Learn how to use EXPLAIN and ANALYZE to improve your query performance inthis training video presented by one of our founders, Martin Traverso. For yourconvenience, we’ve divided the video training course up into topic sections, andprovided links to the relevant parts of our documentation below.The query lifecycleKnowing what’s happening under the hood in SQL can help you to write queriesthat capitalize on possible optimizations and avoid approaches that will costyou performance. This section provides an overview of what happens as a query isexecuted.                                     Topics:              Parsing        Analysis        Planning        Optimization        Scheduling and execution            Running time: ~12 min.      The EXPLAIN statement in detailIf you want to understand what the SEP engine is basing its decisions onas it executes a query, you need to use the EXPLAIN statement. This sectionwalks you through this very informative tool in detail.                                     Topics:              EXPLAIN        EXPLAIN vs EXPLAIN ANALYZE        Fragment            structure, distribution, row layout, estimates, and performance stats in EXPLAIN ANALYZE        Exchanges            Click the links to read more on that topic in our reference manual.      Running time: ~20 min.      General optimizationsThe content in this section is more technique-oriented, and is a complexsubject. We strongly suggest watching it all the way through thoroughly first togain a broad awareness of how you write a query can affect its performancebefore trying these on your own. For further reading, we recommend ourpushdowndocumentation.The SQL engine relies on table statistics to make decisions on optimizations.Enabling dynamic filtering can take optimizations even further. We recommendreading about these powerful features to ensure you are getting the bestperformance possible out of your cluster:  Dynamic filtering  Table statistics                                         Topics:              Constant folding        Predicate pushdown        Predicate pushdown into the Hive connector        Hive partition pruning        Hive bucket pruning        Row group skipping for ORC and Parquet        Limit, partial limit, and aggregation pushdown        Skew            Running time: ~58 min.      SEP offers several properties to control how theoptimizer handles certainoperations.Cost-based optimizationsThis section presents on overview of how cost-based optimizations work in SEP,and provides great context for the following recommended reading:  Cost-based optimizations  Cost in EXPLAIN                                     Topics:              Partitioned and broadcast joins        Disabling cost-based optimizations        Join reordering        Table statistics        Computing statistics with ANALYZE            Running time: ~13 min.      "
 },

 {
  "title": "Create a secret in AWS",
  "url": "/starburst-galaxy/data-sources/secrets.html",
  "content": "# {{ page.title }}A secret consists of a set of credentials, user name and password, and theconnection details used to access a secured service.{% include note.html content=&quot;S3 requires that you create an [IAM policy](../data-sources/iam-policies-users.html) before you add a secret.&quot; %}## Create a secretCreate a secret in the AWS console or in the AWS CLI. Secrets must be saved with`starburst-galaxy-` as the prefix.### AWS consoleCreate a secret in the AWS console. You need the account credentialsto create a secret.1. Open the [AWS console](https://console.aws.amazon.com/secretsmanager/) and   navigate to Secrets Manager.2. Confirm or change your region if necessary.3. On either the service introduction page or the **Secrets list** page, choose   **Store a new secret**.4. Select **Other types of secrets**.5. Click the **Plaintext** tab and delete any existing characters in the   plaintext area. Enter your password (secret value). 6. Select **DefaultEncryptionKey** to encrypt your secret information.7. Click **Next**.8. Name the secret `starburst-galaxy-`, names must be lowercase and   include dashes and normal characters. Special characters and underscores are   not supported. It is important that you always use the `starburst-galaxy-`   prefix. 9. Navigate through the next screens and create the secret.### AWS CLICreate a secret from the AWS CLI. You need the account credentialsto create a secret.1. In the AWS CLI, access the account credentials.2. Run the following command:```    aws secretsmanager create-secret --region us--    --name starburst-galaxy-    --secret-string ```The output of the following command includes the ARN, name, and versionID."
 },
 {
  "title": "Securing Starburst Enterprise",
  "url": "/starburst-enterprise/platform-administrator/security.html",
  "content": "# {{ page.title }}Learn how to safeguard your data with {{site.terms.sep_first}}&#39;s securitytoolkit in this training video presented by one of our founders, Dain Sundstrom.For your convenience, we&#39;ve divided the video training course up into topicsections, and provided links to the relevant parts of our documentation below.## Introduction             {% include youtubeSnippet.html id=page.youtubeId1 start=306 end=943 %}              Topics:              SEP security process        What to secure        Preparing: Verifying HTTP            Running time: ~11 min.      ## Client to server encryption             {% include youtubeSnippet.html id=page.youtubeId1 start=944 end=2096 %}              Topics:              Approaches for HTTPS, including proxies and load balancers        Adding SSL/TLS certificates        Handling PEM and JKS files        Verifying HTTPS for SEP            Running time: ~19 min.      ## Authentication and authorization in {{site.terms.sep}}             {% include youtubeSnippet.html id=page.youtubeId1 start=2097 end=4141 %}              Topics:              Password file authentication        LDAP authentication (See also: group providers)        Kerberos authentication (See also: passthrough)        Client certificate authentication        JSON Web Token authentication        Using multiple authenticators        Authentication with user mapping        Overview of authorization        File-based system access control            Running time: ~34 min.      ## Securing {{site.terms.sep}}&#39;s internal communications and management endpointsDocumentation for the material covered in this section is found[here]({{site.sep_url}}security/internal-communication.html).             {% include youtubeSnippet.html id=page.youtubeId1 start=4694 end=5608 %}              Topics:              Securing the Starburst cluster itself        Shared secret        Internal HTTPS        Secrets management        Management endpoints            Running time: ~16 min.      ## Hive catalog securityWe recommend the following additional reading, which covers enabling{{site.terms.sep}}&#39;s powerful role-based global access control:* [Access control overview]({{site.sep_url}}security/access-control.html)* [Global access control with Apache Ranger]({{site.sep_url}}security/global-ranger.html)* [Global access control with Privacera]({{site.sep_url}}security/global-privacera.html)* [Built-in system access control]({{site.sep_url}}security/built-in-system-access-control.html)While we strongly recommend implementing global access control, you can stillsecure Hive at the catalog level if your particular situation makes thatnecessary. Documentation covering the various options for securing Hive at thecatalog level can be found as follows:* [Configuring Hive security]({{site.sep_url}}connector/hive-security.html)* [Hive-level security with Apache Ranger]({{site.sep_url}}security/hive-ranger.html)* [Hive-level security with Privacera]({{site.sep_url}}security/hive-privacera.html)* [Hive-level security with Apache Sentry]({{site.sep_url}}security/hive-sentry.html)             {% include youtubeSnippet.html id=page.youtubeId1 start=5609 end=6650 %}              Topics:              Authorization        Metastore authentication        HDFS authentication        Kerberos debugging        S3 authentication        Google Cloud authentication            Running time: ~18 min.      "
 },
 {
  "title": "Sitemap",
  "url": "/sitemap.xml",
  "content": "            https://docs.starburst.io/universe-sitemap.xml              https://docs.starburst.io/latest/sitemap.xml                https://docs.starburst.io/350-e/sitemap.xml                https://docs.starburst.io/345-e/sitemap.xml                https://docs.starburst.io/338-e/sitemap.xml                https://docs.starburst.io/355-e/sitemap.xml                https://docs.starburst.io/354-e/sitemap.xml      "
 },
 {
  "title": "Starburst user personas",
  "url": "/starburst-personas.html",
  "content": "# {{ page.title }}No matter what their job title is, most {{site.terms.sb}} users fit one of ouruser personas:* *Data consumers* query data through existing catalogs.* *Data engineers* create catalogs that connect data sources to Starburst.* *Platform administrators* run and maintain the Starburst cluster.These personas embody a set of focused workflows and assume a nominal skillset. Your role may comprise some or all of one or more of these personas&#39;workflows, and that is ok! In fact, we can easily describe this overlap:* All users in the course of their job are data consumers at some level.* Data engineers might do a bit of platform administration.* Platform administrators might do a bit of data engineering.Rather than repeat information or throw every possible workflow at every user,{{site.terms.sb}} user guides are organized to answer a very important question:&quot;Where do I start?!&quot; We use personas to accomplish this.Let&#39;s meet the {{site.terms.sb}} personas so that you can start with the oneclosest to your day-to-day workflows.## Data consumerData analysts, data scientists, and casual report wranglers all use{{site.terms.sb}} in very similar ways. In a nutshell, a data consumer focuseson one or more of the following:* Delivering visualizations and reports.* Making well-informed, data-driven decisions.* Creating forecast and machine learning models that describe the business.* Performing ad hoc analyses.In our guides, we assume a reasonable level of skill with[SQL](./glossary.html#sql), including some knowledge of more advanced queries,and some combination of the following:* Limited to reasonable programming skills* Excellent spreadsheet skills, including some modeling* Knowledge of statistical methods and/or machine learning* Competence with data visualization and/or reporting tools* Ability to detect and articulate issues with data, even if unable to  remedy them or trace the causeIt&#39;s worth noting that downstream users who only consume data through generatedreports and visualizations and don&#39;t actively query data themselves directly aredirect customers of data consumers. Our documentation does not teach basic SQLskills.Our [data consumer user guide]({{site.baseurl}}./data-consumer) provides muchinformation and in-depth training that covers:* Starburst clients* Query federation* Query optimization* Migrating queries to Starburst SQL## Data engineerData engineers deliver data to data consumers in a performant and suitableformat, and in a timely manner with an expectation of a given level of dataquality. Often they source new data from a variety of relational databases,object stores, log entries, message streams or product endpoints. In a nutshell,a data engineer focuses on:* Creating and updating data models &amp; data schemas.* Building and managing ETLs.* Identifying, designing, and implementing internal process improvements such as  automating manual processes, optimizing data delivery for greater scalability.* Building and managing streaming data pipelines.* Building data integrations between various 3rd party systems such as Salesforce  and Workday.Data engineers are customers to platform administrators and the servicesthey provide. Data consumers are the direct customers of data engineers.{{site.terms.sb}} lets data engineers [decouple compute fromstorage]({{site.baseurl}}./data-engineer) and simplifies delivering the data thatyour users need. Our [data engineer userguide]({{site.baseurl}}./starburst-enterprise/data-engineer) will get you started with detailedinformation and training on topics such as:* Creating catalogs to connect data sources.* Developing custom connectors.* Diagnosing and fixing query performance issues.## Platform administrator{{site.terms.sb}} platform administrators care about the scalability,performance and reliability of the {{site.terms.sb}} cluster. They balance SLAsfor both data landing times and availability; implement access and datagovernance policies; and support audit requirements. In a nutshell, platformadministrators focus on:* Building and maintaining scalable data platform architectures to support the  ingest, storage and querying of large heterogenous datasets.* Creating and monitoring cluster health metrics to ensure optimal performance  and reduce any downtime.* Implementing and working with cloud providers like AWS, Azure, and Google  Cloud.{{site.terms.sb}} runs on [COTS](./glossary.html#cots) hardware, and uses memoryinstead of disk, making it fast and more cost-effective. We&#39;ve put together muchin-depth information and training in our [platform administrator userguide](./starburst-enterprise/platform-administrator/index.html) for you,covering:* Security* Performance tuning* Cluster setup and configuration"
 },
 {
  "title": "Using SQL in Starburst",
  "url": "/data-consumer/starburst-sql.html",
  "content": "# {{ page.title }}{{site.terms.oss}}&#39;s open source distributed SQL engine runs fast analyticqueries against various data sources ranging in size from gigabytes topetabytes. {{site.terms.sb}} brings that SQL engine to even more data sources,with more robust features. Because {{site.terms.sb}}&#39;s SQL is ANSI-compliant andsupports most of the SQL language features you depend on, you can hit the groundrunning.Business intelligence users and data scientists can continue to use theirfavorite [client tools](./clients/index.html) such as Tableau, Qlik and ApacheSuperset to access and analyze virtually any data source, or multiple datasources in a single query.## The basicsWe know you want to jump right in, and we know you already have awesomeanalytics skills. It&#39;s just a matter of harnessing the power of{{site.terms.sb}} products to take your analytics even further. With that inmind, you can browse our latest reference materials to learn just how familiar{{site.terms.sb}} SQL is:* [SQL language]({{site.sep_url}}language.html)* [SQL statement syntax]({{site.sep_url}}sql.html)* [Functions and operators]({{site.sep_url}}functions.html)### CatalogsOne key difference worth highlighting is the concept of *catalogs*. Each of yourdata sources is defined as a catalog in {{site.terms.sb}}, and that catalog inturn contains schemas. Using a SQL client such as our CLI, you can discover whatcatalogs are available:```sqlpresto&gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows)```From there, you can use the familiar `SHOW SCHEMAS` command to drill furtherdown.### Fully-qualified table namesTable names are fully qualified when they include the catalog name:```..```This becomes critical when creating [federated queries](./query-federation.html).### General SQL featuresJust in case you&#39;d like a review, here&#39;s a walkthrough of some basic SQLfeatures in {{site.terms.sb}} from one of our founders, David Phillips:             {% include youtubeSnippet.html id=page.youtubeId1 start=585 end=1060 %}              Topics:              Formatting        CASE and searched CASE expressions        IF expressions        TRY expressions        Lambda expressions            Click on the links to read more on that topic in our reference manual.      Running time: ~8 min.      ## Advanced SQLReady to move past the basics? For your convenience, we&#39;ve divided the AdvancedSQL for {{site.terms.sb}} video training course up into topic sections, andprovided links to the relevant parts of our documentation below.### Advanced aggregation techniques             {% include youtubeSnippet.html id=page.youtubeId1 start=1970 end=3654 %}              Topics:              count() with DISTINCT        Approximations, including counting and percentiles        max_by() values        Pivoting with count_if() and FILTER        Complex aggregations        Checksums        ROLLUP        CUBE        GROUPING SETS            Running time: ~28 min.      ### Window functions             {% include youtubeSnippet.html id=page.youtubeId1 start=5280 end=6768 %}              Topics:              Row numbering        Ranking        Ranking and numbering without ordering        Bucketing and percentage ranking        Partitioning        Accessing leading and training rows with lead() and lag()         Window frames        Accessing first, last and Nth values        ROWS vs RANGE using array_agg()        Using aggregations in window functions            Running time: ~25 min.      ### Array and map functionsMany data stores allow to to create arrays, but it isn&#39;t always easy.{{site.terms.sb}} allows you to easily create arrays and maps with your data.Creating arrays with your data is easy:```sqlSELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]```SQL array indexes are 1-based. Learn more about how to use and manipulate themin this in-depth video.             {% include youtubeSnippet.html id=page.youtubeId1 start=4067 end=5214 %}              Topics:              Accessing array and map elements with element_at()        Sorting arrays with array_sort()        matching elements with any_match(), all_match() and none_match()        Filtering elements        Transforming elements        Converting arrays to strings        Computing array products        Unnesting arrays and maps        Creating maps from keys and values and an array of entry rows            Running time: ~19 min.      ### Using JSON             {% include youtubeSnippet.html id=page.youtubeId1 start=1116 end=1969 %}              Topics:              The JSON data type        Extraction using json_extract() and json_extract_scalar()        Casting and partial casting from JSON        Formatting as JSON            Running time: ~14 min.      "
 },

 {
  "title": "Apache Superset",
  "url": "/data-consumer/clients/superset.html",
  "content": "# {{ page.title }}You can use [Apache Superset™](https://superset.apache.org/) with{{site.terms.sep_first}} to explore and visualize your data. Superset is a dataexploration and visualization web application. It is typically run on a serverand used by many users accessing the application. It enables users to write SQLqueries, create new tables, visualizations and dashboards and download data intofiles.  {% include video.html content=&quot;Check out the Trino CommunityBroadcast interview with the Apache Superset team to learn more and see ademo.&quot; %}## RequirementsApache Superset has built in support for {{site.terms.sep}}.{{site.terms.sep}} Versions 350-e and higher can use the preferred mechanism ofa [trino connection](https://superset.apache.org/docs/databases/trino). It ispowered by the [SQLAlchemy driver](https://pypi.org/project/sqlalchemy-trino/)and allows full usage of SQL as supported by {{site.terms.oss}}, and therefore{{site.terms.sep}}.Older versions need to use the legacy support with [prestoconnection](https://superset.apache.org/docs/databases/presto). It uses the[PyHive driver](https://pypi.org/project/PyHive/) and therefore does not supportall supported SQL syntax.{% include caution.html content=&quot;Users are stronly advised to upgrade toSEP 350-e or higher to take advantage of the superior Trino support.&quot; %}## ConnectionGet the necessary [connection information](./index.html#connection-information)for your cluster:* Username* Password* Host* PortSuperset understands the {{site.terms.sep}} concept of catalogs. You can createa connection with or without a specified catalog:* trino://{username}:{password}@{hostname}:{port}/{catalog}* trino://{username}:{password}@{hostname}:{port}Once you have gathered this information, you can [create a newconnection](https://superset.apache.org/docs/creating-charts-dashboards/first-dashboard#connecting-to-a-new-database)in Superset:* Select **Data &gt; Databases**, and click the **+ Database** button.* Enter the connection string formed from your connection information.* Test the connection, if desired.* Click **Add**.## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is required to useTLS/HTTPS. If the best practice of using a globally trusted certificate isimplemented, you can simply use the HTTPS URL of the cluster in the connectionstring.If the certificate is not globally trusted, you need to ensure [configure the{{site.terms.sep}} certificate for TLS/SSL access from Superset on the Supersetserver](https://superset.apache.org/docs/databases/extra-settings)## AuthenticationYou can configure the same authentication provider for Superset and{{site.terms.sep}}. As a result the same credentials can be used to login toeither system.Authentication providers supported by both systems include LDAP and OAuth2:* [Apache Superset](https://superset.apache.org/docs/security)* [{{site.terms.sep_full}}]({{site.sep_url}}security.html)## Resources* [Apache Superset website](https://superset.apache.org/)* [Apache Superset documentation](https://superset.apache.org/docs/intro)* [Preset](https://preset.io/), Powerful, easy to use data exploration and  visualization platform, powered by Apache Superset* [Trino Community Broadcast interview with the Apache Superset team, including  live demo](https://trino.io/episodes/12.html)"
 },
 {
  "title": "Starburst Support",
  "url": "/support.html",
  "content": "                    {{site.terms.support}}        Don&#39;t panic. Help is just a click away!                                      We love our customers!      At {{site.terms.sb}}, we offer our customers dedicated resources      for your organization with up to a 30 minute response time and 24×7      support from our team of experts.      Not a customer just yet? When you are ready to engage, Starburst is      here to help you chart a course, launch successfully, check in      regularly while you get started, and when you are in production.      Check it out!                                Get help with {{site.terms.sep_full}}                                                                              Your support cases              Our intuitive support portal offers customers fast ticket              submission and tracking, user-friendly forums, a knowledge              base, and easy downloads.                                                                                                    Our support policies              Learn more about support with fast response times for problems              and critical issues. Our expert engineers are ready to help you              whenever needed.                                                                                                    No login, no problem!              If you do not have a support portal login, contact us! And take              advantage of all the useful resources available in the              documentation.                                                              {{site.terms.sep_full}} releases and downloads                                                                              Supported releases              Learn about what versions we support, our release cycles,              and important support dates for specific versions.                                                                                                    Latest downloads              Get the latest long-term support (LTS) and short-term support              (STS) versions, and their clients.                                                              Get help with our marketplace offerings                                                              Amazon AWS                                                                        Google Cloud                                                                        Microsoft Azure                                                                        Red Hat OpenShift                              "
 },
 {
  "title": "Tableau",
  "url": "/data-consumer/clients/tableau.html",
  "content": "# {{ page.title }}[Tableau](https://www.tableau.com) is a popular analytics tool with powerfuldata visualization capabilities. There are three ways to connect recent releasesof Tableau products to recent releases of a {{site.terms.sep_first}} cluster:  * **[Tableau data connector](#tableau-connector-with-jdbc-driver)**: The  recommended connection method uses a Tableau data connector file paired with  the {{site.terms.sb}} JDBC driver.* **[ODBC connection](#odbc-driver-connection)**: {{site.terms.sb}} also  supports connecting to {{site.terms.sep}} clusters with the {{site.terms.sb}}  ODBC driver.* **[Legacy JDBC connection](#legacy-jdbc-driver-connection)**: For sites using  a plain JDBC connection using the legacy &quot;Other Databases (JDBC)&quot; method,  {{site.terms.sb}} strongly recommends migrating to the **[Tableau  connector](#tableau-connector-with-jdbc-driver)** method, but  {{site.terms.sb}} also supports continued use of the legacy method.## Before you begin1. Determine the [connection information](./index.html#connection-information)   for your {{site.terms.sep}} cluster, including its URL and login   credentials.2. Instructions to connect Tableau products to {{site.terms.sep}} clusters vary   slightly, depending on the {{site.terms.sep}} version. See [Determine cluster   version](./index.html#determine-cluster-version).## Tableau data connector with JDBC driverYou can connect recent releases of Tableau Desktop to recent {{site.terms.sep}}clusters with a combination of a Tableau data connector file and JDBC driver.Note that Tableau data connectors are not the same as a {{site.terms.sep}}Connector. The Tableau data connector is a bridge between Tableau and thestandard {{site.terms.sep}} JDBC driver that together enable read-only accessbetween Tableau and one or more {{site.terms.sep}} clusters.For Tableau 2021.1, the Tableau data connector for Starburst does not yet appearin the integrated Extension Gallery within Tableau. Prepare your Tableauinstallation as described here before attempting to connect to a{{site.terms.sep}} cluster.Follow these steps to configure the Tableau data connector for Starburst:1. Download the Tableau data connector file with ``.taco`` extension from the   [Starburst Enterprise   page](https://extensiongallery.tableau.com/connectors/274) of Tableau&#39;s   web-based Extension Gallery, using the **Download** button on the upper right   of the page. This requires logging into Tableau&#39;s site with a free login   name.2. Move the data connector file to:        Windows     C:UsersusernameDocumentsMy Tableau RepositoryConnectors     macOS     /Users/username/Documents/My Tableau Repository/Connectors   3. Download the {{site.terms.sb}} JDBC driver according to your   {{site.terms.sep}} version.   * For {{site.terms.sep}} 350-e or older, download     [``presto-jdbc-350.jar``](../../350-e/installation/jdbc.html).   * For {{site.terms.sep}} 354-e and newer, download the [latest JDBC driver     version]({{site.sep_url}}/installation/jdbc.html), or search for a specific     driver to match your cluster version.4. Place the JDBC driver file in the Tableau drivers directory:        Windows     C:Program FilesTableauDrivers     macOS     ~/Library/Tableau/Drivers      Do not store more than one JDBC driver in this directory. Delete any older   drivers when you update to a newer version. Connections to all   {{site.terms.sep}}-connected data sources are made through the   {{site.terms.sb}} JDBC driver.5. Start Tableau Desktop. In the left column, under **To a Server**, click   **More**.6. In the list of server types, select **Starburst Enterprise by Starburst**.   This opens a connection dialog:   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_initial-login-screen.png&#39;   img-id=&#39;tableau-initial-login&#39;   alt-text=&#39;Tableau Connector login dialog&#39;   descr-text=&#39;Image depicting Tableau Connector login dialog&#39;   pxwidth=&#39;501&#39;   screenshot=&#39;true&#39;   %}7. Fill in the connection dialog with the following parameters:   | Field       | Value |   | ----------- | ----------- |   | Server      | IP address of your {{site.terms.sep}} cluster |   | Port        | Port used by your cluster (8080 by default)   |   | Catalog     | Name of the {{site.terms.sep}} catalog to connect to |   | Authentication | Leave blank |   | Username    | Your {{site.terms.sep}} username |   | Schema      | Name of the schema in the specified catalog to connect to |8. Once the new report opens, you must reselect the schema you specified in the   connection dialog.### Customized JDBC connectionsIf you need to set additional connection properties that are not included inTableau&#39;s connection dialog, customize the connection using a properties file.For more information, see [Customize JDBC Connections Using a PropertiesFile](https://community.tableau.com/s/question/0D54T00000F339uSAB/customize-jdbc-connections-using-a-properties-file?_ga=2.13850551.1453931082.1617841062-354148074.1617841061)in the Tableau Community and the [list of available parameters for the JDBCdriver]({{site.sep_url}}/installation/jdbc.html#parameter-reference).## {{site.terms.sb}} advantageRemember that {{site.terms.sep_full}} is not a database, it&#39;s a SQL queryengine that can connect to multiple data sources at the same time. Each{{site.terms.sep}} cluster can query multiple catalogs in a wide range ofdifferent data sources.You can query all these data sources with one Tableau connection. Create itconfigured for a specific catalog and schema. To query multiple catalogs, select**Custom SQL Query** in Tableau, and then reference the fully qualified name ofany table using a `catalog.schema.table` identifier.The follow example query accesses the three catalogs `s3`, `mysql`, and`bigquery`:{% include image.html  url=&#39;../../assets/img/general/Tableau_example-sql-join.png&#39;  img-id=&#39;tableau-example-sql-joins&#39;  alt-text=&#39;Tableau, let SEP do the JOINs&#39;  descr-text=&#39;Image depicting Tableau with SEP performing JOINs&#39;  pxwidth=&#39;615&#39;  screenshot=&#39;true&#39;%}This approach is faster because all data access is managed by{{site.terms.sep}}, and is executed on the cluster. Tableau can also pull datafrom multiple catalogs, but Tableau performs its joins and other operations aslocal analysis.## ODBC driver connectionContact [Starburst Support](../../support.html) to obtain access to the[Starburst ODBC driver](../../latest/installation/starburst-odbc.html). Thisdriver is supported for clusters running any supported {{site.terms.sep}}release. The Presto ODBC driver provided by Tableau is not supported.{% include note.html content=&quot;The preferred method to connect is using theTableau data connector for Starburst file and the JDBC driver described in thepreceding section.&quot; %}Open Tableau and begin the ODBC configuration. On Tableau&#39;s startup page, select**Other Databases (ODBC)**, and configure as follows:* Driver: Starburst Presto ODBC Driver* Username: ``* String Extras: `Driver=Starburst Presto ODBC    Driver;Catalog=;Host=;Port=;`The `String Extras` field supports any of the ODBC connection properties fromthe Starburst ODBC driver.Select **Sign In** to establish the connection. If you are prompted for apassword, the server has authentication enabled.The [Tableau ODBC documentation](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases.htm)contains further information.## Legacy JDBC driver connectionTableau provides a generic connection method titled **Other Databases (JDBC)**.Although {{site.terms.sb}} strongly recommends using a [Tableau dataconnector](#tableau-connector-with jdbc-driver), this generic JDBC connectionmethod is still available. Follow these steps:1. Download the {{site.terms.sb}} JDBC driver according to your   {{site.terms.sep}} version.   * For {{site.terms.sep}} 350-e or older, download     [``presto-jdbc-350.jar``](../../350-e/installation/jdbc.html).   * For {{site.terms.sep}} 354-e and later, download     [``trino-jdbc-354.jar``](../../354-e/installation/jdbc.html). You can also     download the [latest JDBC driver     version](../../latest/installation/jdbc.html) or a specific version to     match your cluster version.2. Place the JDBC driver file in the Tableau drivers directory:        Windows     C:Program FilesTableauDrivers     macOS     ~/Library/Tableau/Drivers      Do not store more than one JDBC driver in this directory. Delete any older   drivers when you update to a newer version. Connections to all   {{site.terms.sep}}-connected data sources are made through the   {{site.terms.sb}} JDBC driver.3. Start Tableau Desktop and select **Connect to a server** using the   **Other Databases (JDBC)** connector in Tableau.6. Fill in the connection dialog as shown in the following table. For the URL   field, if you downloaded the Trino JDBC driver file to connect to newer   {{site.terms.sep}} versions, use a JDBC connection string in this format:   ```shell   jdbc:trino://cluster.example.com:8080/catalog   ```   If you downloaded the PrestoSQL JDBC driver file to connect to older   {{site.terms.sep}} versions, use a JDBC connection string in this format:   ```shell   jdbc:presto://cluster.example.com:8080/catalog   ```   For either driver, the JDBC connection string must include the initial   catalog to connect to. Once connected, you can select schemas and tables   within that catalog.   | Field             | Value |   | --------------- | ----------- |   | URL             | Full JDBC connection string for your cluster. Must include catalog. |   | Dialect         | Must be &#39;SQL92&#39;   |   | Username        | Your {{site.terms.sep}} username |   | Password        | Your {{site.terms.sep}} password |   | Properties File | Specify or browse to the path of a JDBC properties file containing further specifications for this connection. See [Customized JDBC Connections](#customized-jdbc-connections). |&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_other-databases-dialog-catalog.png&#39;   img-id=&#39;tableau-other-databases&#39;   alt-text=&#39;Tableau Other Databases login dialog&#39;   descr-text=&#39;Image depicting Tableau Other Databases login dialog&#39;   pxwidth=&#39;422&#39;   screenshot=&#39;true&#39;   %}"
 },
 {
  "title": "Metadata tags for page front matter",
  "url": "/internal/tag-list.html",
  "content": "# {{page.titles}}The tags in this document are adopted for use at this time. They are used asmetadata to collect and list pages.## searchindex tagIf set to `false`, the page is not included in the simple search, see`search.json`.## persona tagValues:* platform-administrator* data-engineer* data-consumer* data-leader## product tagValues:* sep* galaxy* marketplace* amazon-marketplace* azure-marketplace* google-marketplace* redhat-marketplace## media-type tagValues:* text* video* graphics* screenshot"
 },
 {
  "title": "Tags overview",
  "url": "/starburst-galaxy/metadata/tag-resources.html",
  "content": "# {{ page.title }}Tags are a label, or key-value pair, that you assign to organize and applymetadata to a [cluster](/starburst-galaxy/clusters.html).Create a tag by defining a key and assigning value to it. For example, define akey as a cost center and assign an associated region: `cost-center =boston`The {{site.terms.galaxy_first}} allows you to use tags from your AWS resources.Similar to AWS, you must manually assign a tag to a resource, it won&#39;tautomatically assign itself. Learn about [resource tagging on AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html).## Add tags to {{site.terms.galaxy}}There are two places where you can add tags:1. The **Saved items** page2. In the **Configure additional parameters** section of the    [cluster creation process](/starburst-galaxy/clusters.html)   under the **Add tags** section#### Add tags from the Saved items pageAdd tags from the **Saved items** page.1. On the **Saved items** page, click **+ New**.3. In the **Key** field, type to add a new key or select an existing key.4. Assign a **value** to your key.5. Click **Save**.Navigate to **Saved items** to edit, delete, orview the tag usage at any time.#### Add a tag during clusters creationWhen you create a cluster, you have the option to add existing tags to thecluster or create a new tag.1. On the **Clusters** page, select **+ New**.2. Open **Configure additional parameters (optional)** and    select **Create a new tag**.3. In the **Key** field, type to add a new key or select an existing key.4. Assign a **value** to your key.5. Click **Save**.Navigate to **Saved items** to edit, delete, orview the tag usage at any time.## Edit a tag or view tag usageNavigate to **Saved items** to edit, delete, orview the tag usage at any time.#### Edit a tagAny updates you make to a tag apply to all of the clusters that use that tag.1. Click **Menu** next to the tag you want to update and select **Edit tag**.2. Change the **Key** or **Value**.3. Click **Save tag** to input your updates.Your tag is updated and will apply the new tag info to the clusters that use it.#### View tag usageSee which clusters are associated with a tag. 1. Click **Menu** next to the tag whose usage you want to view    and select **View usage**.2. The tag usage displays and shows how many items use the tag.While you&#39;re viewing a tag, you have the option to remove items that use it."
 },
 {
  "title": "Universe Sitemap",
  "url": "/universe-sitemap.xml",
  "content": "            https://docs.starburst.io        {% for post in site.posts %}                    https://docs.starburst.io{{ post.url }}            {% endfor %}    {% for page in site.pages %}        {% if page.url == &#39;/search.json&#39;            or page.url == &#39;/sitemap.xml&#39;            or page.url == &#39;/universe-sitemap.xml&#39;            or page.url == &#39;/404.html&#39;            or page.url == &#39;/assets/css/style.css&#39;            or page.sitemap_exclude == &#39;y&#39;        %}             {% continue %}         {% endif %}                    https://docs.starburst.io{{ page.url | replace:&#39;/index.html&#39;,&#39;/&#39; }}            {% endfor %}"
 },
 {
  "title": "Verify server with Web UI",
  "url": "/starburst-enterprise/try/verify.html",
  "content": "# {{page.title}}To verify that your locally run {{site.terms.sep_full}} server is running, usethe Web UI.Open the Web UI---------------With any modern browser, go to [http://localhost:8080](http://localhost:8080).At the login screen, enter your current OS login name (or any string).{% include image.html  url=&#39;../../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}The default Web UI screen shows the version number, environment, and uptime ofthe server. The statistics fields show zeros until a query is run against theserver.{% include image.html  url=&#39;../../assets/img/general/web-ui-empty.png&#39;  img-id=&#39;webuiempty&#39;  alt-text=&#39;Empty Web UI screen&#39;  descr-text=&#39;Empty Web UI screen&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}Run queries-----------To run queries against the local server with the {{site.terms.oss}} CLI, see[CLI](../../data-consumer/clients/cli)."
 },

 {
  "title": "",
  "url": "",
  "date": "",
  "content": ""
 }
]