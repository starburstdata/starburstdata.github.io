[
 {
  "title": "Advanced SQL in Starburst",
  "url": "/videos/2020-07-29-advanced-presto-sql.html",
  "content": "# {{ page.title }}Hosts: {{ page.participants }}Video date: {{ page.date }} Running time: {{ page.length}}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping users understand how to run morecomplex and comprehensive SQL queries with {{site.terms.sb}}. Delivered by DavidPhillips, this session covers the following topics:* Using JSON and other complex data types* Advanced aggregation techniques* Window functions* Array and map functions* Lambda expressions* Many other SQL functions and features## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.* Welcome - 0:00* General SQL Features - [9:45](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=585s)   * Format function - [9:57](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=597s)   * Case expressions - [12:48](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=768s)   * Searched case expression - [13:34](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=814s)   * IF expression - [14:24](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=864s)   * TRY expression - [15:26](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=926s)   * Lambda expression overview[](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1061s)* Using JSON - [18:36](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1116s)   * JSON data type - [19:13](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1153s)   * Extraction using JSONPath - [23:11](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1391s)   * Casting from JSON - [26:01](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1561s)   * Parting casting from JSON - [27:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1627s)   * Formatting as JSON - [29:40](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1780s)* Advanced Aggregation Techniques - [32:50](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1970s)   * Counting distinct items - [33:06](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=1986s)   * Approximate percentiles - [36:02](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2162s)   * Associated max value - [37:59](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2279s)   * Associated max value using a row type - [38:39](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2319s)   * Pivoting with conditional counting - [40:58](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2458s)   * Pivoting with filtering - [42:08](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2528s)   * Pivoting averages - [42:57](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2577s)   * Aggregating a complex expression - [43:55](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2635s)   * Aggregating into an array - [45:36](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2736s)   * Aggregating into a lambda - [46:28](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=2788s)   * Order-insensitive checksums - [50:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3007s)   * ROLLUP with single - [52:32](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3152s)   * ROLLUP with multiple - [53:24](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3204s)   * CUBE - [55:52](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3352s)   * GROUPING SETS - [59:28](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=3568s)* Array and Map Functions - [1:07:47](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4067s)   * Creating arrays - [1:08:40](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4120s)   * Accessing array elements - [1:09:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4147s)   * Sorting arrays - [1:11:25](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4285s)   * Matching elements - [1:13:26](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4406s)   * Filtering elements - [1:14:57](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4497s)   * Transforming elements - [1:16:25](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4585s)   * Converting arrays to strings - [1:17:44](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4664s)   * Computing array product - [1:19:45](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4785s)   * Unnesting an array - [1:22:16](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4936s)   * Unnesting an array with ordinality - [1:23:06](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=4986s)   * Creating maps - [1:23:44](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5024s)   * Accessing map elements - [1:25:20](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5120s)   * Unnesting a map - [1:26:16](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5176s)* Window Functions - [1:28:00](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5280s)   * Window function overview - [1:28:58](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5338s)   * Row numbering - [1:30:04](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5404s)   * Row numbering order - [1:30:49](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5449s)   * Row numbering with limit - [1:31:39](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5499s)   * Rank - [1:32:30](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5550s)   * Rank with ties - [1:33:11](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5591s)   * Dense rank with ties - [1:33:54](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5634s)   * Ranking without ordering - [1:34:20](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5660s)   * Row numbering without ordering - [1:34:40](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5680s)   * Assigning rows to buckets - [1:36:07](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5767s)   * Percentage ranking - [1:37:12](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5832s)   * Partitioning - [1:37:53](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5873s)   * Partitioning on the same value - [1:39:26](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=5966s)   * Accessing leading and trailing rows - [1:40:12](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6012s)   * Accessing leading and trailing rows with nulls - [1:42:56](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6176s)   * Accessing leading and trailing rows without nulls - [1:43:56](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6236s)   * Window frames - [1:45:13](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6313s)   * Accessing the first value - [1:47:05](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6425s)   * Accessing the last value - [1:47:35](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6455s)   * Accessing the Nth value - [1:47:49](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6469s)   * Window frame ROWS vs RANGE - [1:48:15](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6495s)   * Rolling and total sum - [1:50:39](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6639s)   * Partition sum - [1:52:00](https://www.youtube.com/watch?v=HN_95ObHAiw&amp;t=6720s)"
 },
 {
  "title": "Understanding and tuning Starburst query processing",
  "url": "/videos/2020-08-12-query-performance.html",
  "content": "# {{ page.title }}Hosts: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping users understand how{{site.terms.sb}} executes queries. That knowledge can help you improve queryperformance. For instance, the explain plan is a powerful tool. We explore howto access the explain plan and how to read it. We look at the work thecost-based optimizer performs and how you can potentially help run yourqueries even faster. Delivered by Martin Traverso, this session covers thefollowing topics:* Explain the EXPLAIN* Learn how queries are analyzed and executed* Understand what the optimizer does, including some of its limitations* Showcase the cost-based optimizer## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can skip tothat timestamp in the video player above.* Welcome - 0:00* Query lifecycle - [8:11](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=491s)    * Parsing - [9:44](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=584s)    * Analysis - [11:29](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=689s)    * Planning - [15:25](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=925s)    * Optimization - [16:44](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1004s)    * Scheduling and execution - [18:50](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1130s)* Explain the EXPLAIN - [20:18](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1218s)    * EXPLAIN command - [20:55](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1255s)    * EXPLAIN vs EXPLAIN ANALYZE - [23:07](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1387s)    * Fragment Structure - [24:06](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1446s)    * Distribution - [26:55](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1615s)    * Row layout - [29:37](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1777s)    * Estimates - [31:54](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=1914s)    * Performance stats - [33:49](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2029s)    * Exchanges - [39:33](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2373s)* Optimization - [41:10](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2470s)    * Constant folding - [43:23](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=2603s)    * Predicate pushdown - [51:07](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=3067s)    * Predicate pushdown into connectors - [55:43](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=3343s)    * Predicate pushdown into the Hive connector - [1:08:24](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4104s)    * Hive partition pruning - [1:10:18](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4218s)    * Hive bucket pruning - [1:17:12](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4632s)    * Row group skipping for ORC and Parquet - [1:19:17](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4757s)    * Limit pushdown - [1:22:31](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=4951s)    * Partial limit pushdown - [1:26:11](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5171s)    * Aggregation pushdown - [1:28:26](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5306s)    * Skew - [1:33:41](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5621s)* Cost-based Optimizations - [1:39:01](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=5941s)    * Partitioned join - [1:41:41](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6101s)    * Broadcast join - [1:44:01](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6241s)    * Join type selection - Partitioned - [1:46:02](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6362s)    * Join type selection - Broadcast - [1:47:07](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6427s)    * Disabling cost-based optimizations - [1:48:10](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6490s)    * Join reordering - [1:48:44](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6524s)    * Table statistics - [1:50:21](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6621s)    * Computing statistics - [1:51:19](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6679s)* Resources - [1:52:09](https://www.youtube.com/watch?v=GcS02yTNwC0&amp;t=6729s)"
 },
 {
  "title": "Securing Starburst Enterprise",
  "url": "/videos/2020-08-26-securely-deploy-presto.html",
  "content": "# {{ page.title }}Presenters: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping {{site.terms.sep_first}} userssecurely deploy {{site.terms.sep}} at scale. We cover how to secure{{site.terms.sep}} as well as access to your underlying data. Delivered by DainSundstrom, this session covers the following topics:* Authentication, including password &amp; LDAP Authentication* Authorization to access your data sources* Encryption including client-to-coordinator communication* Secure communication in the cluster* Secrets usage for configuration files including catalogs## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.* Welcome - 0:00* Tips and Notes - [5:06](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=306s)   * Process for securing {{site.terms.sep}} - [7:34](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=454s)   * What to secure - [11:02](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=662s)   * Verify HTTP with the Web UI - [13:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=803s)   * Verify HTTP with the CLI - [14:48](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=888s)      * CLI failures - [15:14](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=914s)   * Client to Server Encryption - [15:44](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=944s)      * Approaches for HTTPS - [15:58](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=958s)      * HTTPS proxy or load balancer - [17:33](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1053s)      * Add the SSL/TLS certificate to the coordinator - [20:28](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1228s)         * Inspect the PEM file - [22:40](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1360s)         * Verify the PEM file certificate - [23:45](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1425s)         * Verify the PEM private key - [26:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1568s)         * Verify the JKS file - [26:38](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1598s)         * Configure {{site.terms.sep}} - [27:59](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1679s)      * Verify HTTPS with the Web UI - [28:51](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1731s)      * Verify HTTPS with the CLI - [29:36](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1776s)         * CLI failures - [29:46](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=1786s)         * CLI --insecure - [33:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2003s)   * Authentication - [34:57](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2097s)      * Password file authentication - [36:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2168s)      * LDAP Authentication - [41:19](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=2479s)      * Kerberos Authentication - [50:24](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3024s)      * Client certificate authentication - [53:53](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3233s)      * JSON Web Token authentication - [55:03](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3303s)      * Multiple authenticators - [56:01](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3361s)      * User mapping - [58:14](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3494s)   * Authorization - [1:00:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3608s)      * File-based system access control - [1:02:54](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=3774s)   * Client to server summary - [1:07:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4043s)   * Internal security and connector security - [1:18:14](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4694s)      * Securing the cluster itself - [1:18:30](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4710s)      * Shared secret - [1:20:29](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=4829s)   * Internal HTTPS - [1:23:58](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5038s)   * Secrets Management - [1:27:53](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5273s)   * Management Endpoints - [1:30:23](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5423s)   * Hive Catalog Security - [1:33:29](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5609s)      * Hive Catalog Authorization - [1:34:45](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5685s)      * Hive Metastore Authentication - [1:38:08](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=5888s)      * HDFS Authentication - [1:42:24](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6144s)      * Hive Kerberos Debugging - [1:43:31](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6211s)      * S3 Authentication - [1:45:53](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6353s)      * Google Cloud Authentication - [1:49:31](https://www.youtube.com/watch?v=KiMyRc3PSh0&amp;t=6571s)"
 },
 {
  "title": "Starburst cluster sizing, and performance tuning",
  "url": "/videos/2020-09-09-cluster-sizing-and-performance-video.html",
  "content": "# {{ page.title }}Presenters: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include youtubePlayer.html id=page.youtubeId %}This training session is geared towards helping users tune and size theirdeployment for optimal performance. Delivered by Dain Sundstrom, this sessioncovers the following topics:* Cluster configuration and node sizing* Memory configuration and management* Improving task concurrency and worker scheduling* Tuning your JVM configuration* Investigating queries for join order and other criteria* Tuning the cost-based optimizer## Detailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.* Welcome - 0:00* General Strategy - [5:45](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=345s)* Baseline Advice - [11:53](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=713s)* Cluster Sizing / CPU and Memory - [14:57](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=897s)* Machine Sizing - [34:46](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2086s)    * Memory - [34:58](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2098s)    * Memory Allocations - [39:55](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2395s)    * Shared Join Hash - [46:59](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2819s)    * Distributed Join - [49:11](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=2951s)    * Skew - [50:58](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3058s)    * Use bigger machines - [55:03](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3303s)    * Machine Types - [58:40](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3520s)* Additional Thoughts - [1:03:20](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3800s)    * Hash join vs (sort) merge join - [1:03:25](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3805s)    * Spilling - [1:04:43](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=3883s)    * Small Clusters - [1:07:54](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=4074s)* Tuning the Workload - [1:23:45](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5025s)    * Query Plan - [1:24:52](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5092s)    * Precomputing - [1:30:00](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5400s)    * Connectors - [1:34:53](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5693s)* Hive Data Organization - [1:39:27](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5967s)    * Organize the data for the Hive connector - [1:39:41](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=5981s)    * Hive Partitioning - [1:42:13](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6133s)    * Hive bucketing - [1:43:59](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6239s)    * Orc and Parquet - [1:46:26](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6386s)    * File Size - [1:51:02](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6662s)    * Bad Parquet Files - [1:53:13](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6793s)    * Rewrite table with ORC writer - [1:54:29](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6869s)* Making Queries Faster - [1:55:37](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=6937s)    * What to look for in a query - [1:57:09](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7029s)    * More hardware - [2:01:18](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7278s)    * Under-utilization - [2:02:32](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7352s)    * Hive Caching - [2:05:29](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7529s)* Sharing Resources / Resource Groups - [2:08:34](https://www.youtube.com/watch?v=Pu80FkBRP-k&amp;t=7714s)"
 },
 {
  "title": "What Data Mesh Means for Data Analysts",
  "url": "/videos/2021-05-19-data-mesh-for-analysts.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;vhc0oa8rb2&#39; %}This video focuses on the increasingly popular data mesh approach to enterprisearchitecture which calls for distributed, domain-oriented ownership of data. Butwhat does data mesh actually mean for data analysts? Is the concept of datawarehouse at odds with the data mesh approach? Is there a paradigm shifthappening in the data architecture landscape?Our panelists answer these pressing questions in a discussion moderated by SeanZinsmeister, VP Product Marketing at ThoughtSpot. Panelists:* Zhamak Dehghani, Founder of Data Mesh* Daniel Abadi, Darnell-Kanal Professor of Computer Science at University of Maryland, College Park* Gareth Stevenson, Director, Senior Quantitative Analyst, Bank of America This fun and informative discussion is worth watching in its entirety."
 },
 {
  "title": "Mars - Enabling Profitable Growth by Leveraging Data and Analytics",
  "url": "/videos/2021-05-19-mars-data-analytics.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;kj2xv60u3y&#39; %}Mars, best known for its M&amp;M&#39;s and Snickers brands, is in the midst of a majordigital transformation with a very specific focus. In this session, Deepak Jose,Head of Business Strategy and Advanced Analytics at Mars, talks about puttingtheir data and analytics approach into practice, including designing the MarsDigital Engine, an effort broadly focused on leveraging data and analytics todrive growth. The talk provides a great inside look at how a large, storied brand isapproaching our data-driven future, and where data analysts fit. Deepak has somecool advice for those of you encountering colleagues with a stubborn traditionalmindset and why today’s data analysts need to stand their ground and support newtools if they want to deliver truly transformational results.  "
 },
 {
  "title": "SQL and Trino - The Golden Language Prevails",
  "url": "/videos/2021-05-19-sql-trino.html",
  "content": "# {{ page.title }}Presenters: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;3lc8nexqkz&#39; %}SQL remains the dominant language for analytics. SQL-based solutions like Trinohelp you gain fast access to data to make informed business decisions. If you’rerelatively new to this space and the value of distributed query engines, thissession has everything you need to get started.{{site.terms.sb}}’s Brian Luisi delivers an overview of why enterprises needsuch platforms in the first place and discusses how they work within largeorganizations. Tom Nats shows you a SQL and Trino demo. "
 },
 {
  "title": "The State of Data Analysts",
  "url": "/videos/2021-05-19-state-of-data-analysts.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;t97r90bswp&#39; %}Moderated by Andy Cotgreave, Technical Evangelist at Tableau and Co-Author of‘The Big Book of Dashboards,’ this session reviews how the role of the dataanalyst has evolved over time. The demands of modern analytics, the advent of AIand ML, and the emergence of SQL-based MPP query engines like Starburst havealtered the way analysts have traditionally operated.The panel features Charles Wilson, Senior Manager of Customer Insights andAnalytics at New Balance, Manav Gupta, Head of Analytics and Insights at AdobeStock, and Parker Dillon, a senior consultant at Slalom.The speakers make bold predictions about dashboard usage, examine whether AI isoverhyped, share advice for budding analysts, and even get into a somewhatphilosophical debate about whether data analysis is an art or science. Or is ita scientific art? The highlight of this discussion is the mix of participantsand perspectives, and it’s a must-watch for every data analyst."
 },
 {
  "title": "Lightning demo - Tableau and Starburst",
  "url": "/videos/2021-05-19-tableau-staburst-demo.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;yvgxn8b3fz&#39; %}Lightning demo of Tableau and {{site.terms.sb}} delivered by Ben Lumbert covershow to connect Tableau and {{site.terms.sb}}."
 },
 {
  "title": "Lightning demo - ThoughtSpot and Starburst",
  "url": "/videos/2021-05-19-thoughtspot-staburst-demo.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;3atxe3r3mo&#39; %}Lightning demo of ThoughtSpot and {{site.terms.sb}} delivered by ToddBeauchene covers how to connect ThoughtSpot and {{site.terms.sb}}."
 },
 {
  "title": "Accelerating data science with Trino",
  "url": "/videos/2021-07-14-accelerating-data-science-with-trino.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;keywshcupz&#39; %}This panel features Starburst Regional Manager, Brian Luisi, and Starburst CTOs,Dain Sundstrom and David Philips. They cover how Starburst and Trino assist inthe responsibilities of data scientists, including collecting data, interactingwith data, and making predictive models. They also discuss what data accessmeans for data scientists, Trino use cases, and best practices for datascientists. Data scientists need a data engine that can interact with all of their data andis able to quickly sift through it, which allows for easier profiling andexploration. Trino and Starburst allows data scientists to do this by providinga single point of access to large amounts of data. David describes Trino as“fast and distributed, so if you need to process data, you can process it wayfaster than you could on a Python script.” Trino’s SQL-based MPP query engineprovides great value to scientists looking to expedite their processing throughits ability to analyze large volumes of data, fast."
 },
 {
  "title": "Data lakehouse - a new architectural horizon",
  "url": "/videos/2021-07-14-data-lakehouse-new-horizon.html",
  "content": "# {{ page.title }}Moderator and panelists: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;vm18cdjbew&#39; %}A hot topic in the big data field, this discussion led by Paco Nathan, exploredthe pros and cons of the data lakehouse. Although the lakehouse combines a lotof the best aspects of a data warehouse and the best aspects of a data lake,there are some complications that come along with the adoption of the datalakehouse.The panelists, Adri Purkayasth from BNP Paribas, Anjali Samani from Salesforce,and Tom Nats of Starburst, debate what it means for organizations to implementthis data architecture including regulatory considerations, standardizations,and the struggles with blending old architecture with the new architecture.Anjali says, “To realize a lot of value out of your data science investments,you need access to alternative data sources and that’s where data lakehousearchitecture is very attractive.”"
 },
 {
  "title": "Demo - Assurance and Starburst",
  "url": "/videos/2021-07-14-demo-assurance-and-starburst.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;jv1k60ewlf&#39; %}Mitchell Poslums, Senior Data Scientist at Assurance, an online insurancedistribution platform, presents on how the implementation of Trino and Starburst“have really enabled [Assurance] to accelerate time to insights, improve ourconversion rates, and enable robust modeling all coalescing to achieve betterbusiness outcomes.”In order to provide the best service to their customer, Assurance needs the bestout of their data. For all technology businesses, insights can’t be achievedwithout accurate, timely data. With Starburst and Trino, Assurance was able tosolve their blockers to insights and effectively provided the best solutions totheir customers."
 },
 {
  "title": "Demo - Red Hat OpenShift data science",
  "url": "/videos/2021-07-14-demo-redhat-openshift-data-science.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;s0xu9bh93m&#39; %}Red Hat launched a new product, Red Hat OpenShift Data Science, which givesdata scientists the ability to produce production-ready models as quickly aspossible. It also easily supports a data mesh paradigm.Through a video demonstration, Karl Eklund, Principal Architect at Red Hat,shows how this tool provides a central location to explore data and buildmodels. There is also no vendor lock-in, so users can use other tools, likeStarburst, to better leverage this new technology. With these technologies,“data scientists have everything they need to be successful in consuming data,building models, and then deploying and monitoring them.”"
 },
 {
  "title": "The intelligent edge",
  "url": "/videos/2021-07-14-intelligent-edge.html",
  "content": "# {{ page.title }}Presenter: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;ex099yj2tt&#39; %}An insightful session on the ‘Intelligent Edge’ by leading data scientist, KirkBorne. With metaphors from Formula 1 racing and quotes from Yogi Berra, heexplains how the ability to use the intelligent edge and sensory data can allowdata scientists to make better analytical decisions.Data is at the forefront of everything: “Innovations are inspired by data,informed by data, enabled by data, and create value from data.” New intelligentedge technologies, like model monitoring, robotics, and drones, are powered byAI and ML. This allows data users to “see around the corner” and monitorimportant data, detect early signs of risk events, and discover the rightquestions to ask your data. In order to make the most accurate predictions, datascientists need the most accurate data."
 },
 {
  "title": "What data mesh means for data scientists",
  "url": "/videos/2021-07-14-what-data-mesh-means.html",
  "content": "# {{ page.title }}Participants: {{ page.participants }}Video date: {{ page.date }}Running time: {{ page.length }}{% include wistiaPlayer.html id=&#39;ko7g02j84c&#39; %}Data mesh is both a technical and organizational approach to managing andaccessing data. We were lucky enough to have Zhamak Dehghani, who coined theterm data mesh, join us for this discussion on how the data mesh impacts datascientists.By having this decentralized approach and treating data as a product, data isable to be exposed and shared with those that need it the most, including thedata scientists. Zhamak said, “To get value from data, particularly foranalytical purposes, when we want to make predictions or we want to discovertrends, we have to aggregate and centralize data in one place, under the controlof one set of technologies and specifically under the control of a centralizedteam.”The question of data ownership is a huge pain point for data scientists.However, with the data mesh architecture, scientists are able to access datawhere it lives and better contribute to the overall business goals. In aconversation moderated by Sophie Watkins of Red Hat, the panel comprising,Daniel Abadi, professor of Computer Science at University of Maryland, MaxSchultze of Zalando, and Zhamak Dehghani of Thoughtworks, discuss pain points,best practices, and the overall principles of data mesh."
 },
 {
  "title": "404",
  "url": "/404.html",
  "content": "                                  Oops ... 404        Don&#39;t panic. We brought a towel.        Try going home.        Or maybe you are looking for some specific documentation and resources:                  {{site.terms.sep_full}}          Starburst personas overview          Platform administrator          Data engineer          Data consumer                            &amp;nbsp;                                          Confused where you landed?          Go          straight to the {{site.terms.sep_full}} reference documentation!          A lot of new useful documentation is available here, but fear not,          all the comprehensive reference documentation for          {{site.terms.sep_full}} is ready and constantly improving as always.                      Go there now!                                                    "
 },
 {
  "title": "Add a MySQL data source",
  "url": "/starburst-galaxy/data-sources/mysql/add-mysql.html",
  "content": "# {{ page.title }}1. From the **Data sources** page, click **Add a new data source** or **+ New**.2. Select **MySQL** to begin adding your [data source](../../../glossary.html#data-source).## Basic informationEnter the name and description you want for your data source. A unique catalogname allows you to tell it apart from other data sources and easily createqueries.1. Enter a unique **Catalog name** for your data source. The name supports   lowercase letters, numbers, and underscores.2. Add a **Description** to share more details about the data source than   just the name. {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-name-description.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Add the catalog name and description&#39;   descr-text=&#39;Image depicting the catalog name and description fields&#39;   screenshot=&#39;true&#39;   %}## Connection detailsEnter the information needed to connect with your MySQL database.1. Add your **Host** and **Port number**.2. The **Connection URL** auto-populates,   you can choose to manually edit it. {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-mysql-connection-details.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Enter the host and port number to auto-populate the connection url&#39;   descr-text=&#39;Image depicting the host, port number, and connection url fields&#39;   pxwidth=&#39;450&#39;   screenshot=&#39;true&#39; %}## AuthenticationTo authenticate, create your [external secrets](../secrets.html) in yourAWS Secrets Manager. To ensure access, use the prefix `starburst-galaxy-`.1. Enter your **User** information.2. Add our **AWS secret key name** from your **AWS Secrets Manager**. {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-user-secret-key.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Add the catalog name and description&#39;   descr-text=&#39;Image depicting the catalog name and description fields&#39;   pxwidth=&#39;450&#39;   screenshot=&#39;true&#39;   %}3. Click **Connect data source**.  {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-connect-data-source.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Add the catalog name and description&#39;   descr-text=&#39;Image depicting the catalog name and description fields&#39;   pxwidth=&#39;250&#39;   screenshot=&#39;true&#39;   %}Now that you&#39;ve created a data source, you can [createclusters](../../clusters/index.html).Navigate to the **Data sources** page to edit, delete, or add new data sourcesat any time."
 },
 {
  "title": "Add a PostgreSQL data source",
  "url": "/starburst-galaxy/data-sources/postgresql/add-postgresql.html",
  "content": "# {{ page.title }}1. On the **Data sources** page, click **Add a new data source** or **+ New**.2. Select **PostgreSQL** to begin adding your [data source](../../../glossary.html#data-source).## Name and descriptionThe name and description of your data source allows you to tell it apart fromother data sources at a glance. Unique names are useful for SQL queries.1. Enter a unique **Catalog name** for your data source. The name   supports lowercase letters, numbers, and underscores.2. Add a **Description** to share more details about the data source than   just the name.  {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-name-description.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Add the catalog name and description&#39;   descr-text=&#39;Image depicting the catalog name and description fields&#39;   screenshot=&#39;true&#39;   %}## Connection detailsEnter the information needed to connect with your PostgreSQL database.1. Add your **Host** and **Port number**.2. Enter the **Database**.3. The **Connection URL** auto-populates,   you can choose to edit it manually. {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-postgresql-connection-details.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Enter the host and port number to auto-populate the connection URL&#39;   descr-text=&#39;Image depicting the host, port number, and connection URL fields&#39;   screenshot=&#39;true&#39; %}## AuthenticationTo authenticate, create your [external secrets](../secrets.html) inyour AWS Secrets Manager. To ensure access, use the prefix `starburst-galaxy-`.1. Enter your **User** information.  {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-user-secret-key.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Add the catalog name and description&#39;   descr-text=&#39;Image depicting the user and AWS access key name fields&#39;   screenshot=&#39;true&#39;   %}3. Click **Connect data source**.  {% include image.html   url=&#39;../../../assets/img/galaxy/data-sources/data-source-connect-data-source.png&#39;   img-id=&#39;namedescription&#39;   alt-text=&#39;Add the catalog name and description&#39;   descr-text=&#39;Image depicting the catalog name and description fields&#39;   screenshot=&#39;true&#39;   %}Now that you&#39;ve created a data source, you can [createclusters](../../clusters/index.html).Navigate to the **Data sources** page to edit, delete, or add new data sourcesat any time."
 },
 {
  "title": "Add a Redshift data source",
  "url": "/starburst-galaxy/data-sources/redshift/add-redshift.html",
  "content": "# {{ page.title }}1. On the **Data sources** page, click **Add a new data source** or **+ New**.2. Select **Redshift** to begin adding your [data source](../../../glossary.html#data-source).## Name and descriptionThe name and description of your data source allows you to tell it apart fromother data sources at a glance. Unique names are useful for SQL queries.1. Enter a unique **Catalog name** for your data source. The name   supports lowercase letters, numbers, and underscores.2. Add a **Description** to share more details about the data source than   just the name.## Connection detailsEnter the information needed to connect with your PostgreSQL database.1. Add your **Host** and **Port number**.2. Enter the **Database**.3. The **Connection URL** auto-populates,   you can choose to edit it manually.## AuthenticationTo authenticate, create your [external secrets](../secrets.html) inyour AWS Secrets Manager. To ensure access, use the prefix `starburst-galaxy-`.1. Enter your **User** information.3. Click **Connect data source**.Now that you&#39;ve created a data source, you can [createclusters](../../clusters/index.html).Navigate to the **Data sources** page to edit, delete, or add new data sourcesat any time."
 },
 {
  "title": "Add an Amazon S3 data source with AWS Glue catalog",
  "url": "/starburst-galaxy/data-sources/s3/add-s3.html",
  "content": "# {{ page.title }}1. From the **Data sources** page, click **Add a new data source** or **+ New**.2. Select **Amazon S3 + AWS Glue data catalog** to begin adding your [data source](../../../glossary.html#data-source).## Name and descriptionA unique catalog name is useful for SQL queries and a description allows you totell it apart from other catalogs.3. Enter a unique **Name** for your data source. The name supports   lowercase letters, numbers, and underscores.5. Add a **Description** to share more details about the data source than just   the name.For example: `__`![Add name and description]({{site.baseurl}}/assets/img/galaxy/data-sources/ds-s3-name-description.png)## Authentication to AWS GlueAllow {{site.terms.sb}} access to metadata and mapping information about theobjects stored in S3.[External secrets](../secrets.html) must be created in your AWS SecretsManager and start with the prefix `starburst-galaxy-`.6. Enter your **AWS Access key ID**.7. Select the name of your **secret key** from your **AWS Secrets Manager**.![Add name and description]({{site.baseurl}}/assets/img/galaxy/data-sources/ds-s3-aws-access-key.png)## Authentication to S3Choose and configure the authentication mechanism to connect to S3.{{site.terms.galaxy_full}} defaults to using the same authentication propertiesas AWS. The specific path to the storage is part of the Glue metadata andprovided during table creation.![Add name and description]({{site.baseurl}}/assets/img/galaxy/data-sources/ds-s3-auth-s3.png)If you&#39;re happy with the default S3 authentication mechanism:8. Click **Connect data source**.![Add name and description]({{site.baseurl}}/assets/img/galaxy/data-sources/ds-s3-connect-data-source.png)Alternatively, you can add a different S3 authentication:8. Enter your **AWS Access key ID**.9. Select the name of your **secret key** from your **AWS Secrets Manager**.10. Click **Connect data source**.![Add name and description]({{site.baseurl}}/assets/img/galaxy/data-sources/ds-s3-aws-access-key.png)Now that you&#39;ve created a data source, you can [createclusters](../../clusters).Navigate to the **Data sources** page to edit,delete, or add new data sources at any time."
 },
 {
  "title": "Add an Amazon S3 data source with Hive metastore service",
  "url": "/starburst-galaxy/data-sources/s3-hms/add.html",
  "content": "# {{ page.title }}1. From the **Data sources** page, click **Add a new data source** or **+ New**.1. Select **Amazon S3 + Hive** to begin adding your [data   source](../../../glossary.html#data-source).## Name and descriptionA unique catalog name is useful for SQL queries and a description allows you totell it apart from other catalogs.1. Enter a unique **Name** for your data source. The name supports   lowercase letters, numbers, and underscores.1. Add a **Description** to share more details about the data source than just   the name.For example: `__`![Add name and description]({{site.baseurl}}/assets/img/galaxy/data-sources/ds-s3-name-description.png)## Hive metastore serviceAdd the Thrift URI of your HMS. The URL needs to use the ``thrift://`` protocoland point to the fully qualified domain name and port of the HMS.The address needs to be [accessible from {{site.terms.galaxy}} in your AWSaccount](../index.html#requirements).## Authentication to S3Authentication uses the AWS Access key:1. Enter your **AWS Access key ID**.1. Select the name of your **secret key** from your **AWS Secrets Manager**.1. Click **Connect data source**.![Add name and description]({{site.baseurl}}/assets/img/galaxy/data-sources/ds-s3-aws-access-key.png)Now that you&#39;ve created a data source, you can a [createcluster](../../clusters/index.html).Navigate to the **Data sources** page to edit, delete, or add new data sourcesat any time."
 },
 {
  "title": "Add a Snowflake data source",
  "url": "/starburst-galaxy/data-sources/snowflake/add.html",
  "content": "# {{ page.title }}You can add a Snowflake data source to query a database in your Snowflakeaccount in {{site.terms.galaxy}}:1. On the **Data sources** page, click **Add a new data source** or **+ New**.2. Select **Snowflake** to begin adding your [data source](../../../glossary.html#data-source).## Name and descriptionThe name and description of your data source allows you to tell it apart fromother data sources at a glance. The name is used as the catalog name, availablein [client tools](../../../data-consumer/clients/index.html) and commands suchas `SHOW CATALOGS` or `USE`.1. Enter a unique **Catalog name** for your data source. The name supports   lowercase letters, numbers, and underscores.2. Add a **Description** to share more details about the data source than   just the name.## Connection detailsEnter the information needed to connect with your Snowflake database.1. Enter the [full name of your Snowflake account](https://docs.snowflake.com/en/user-guide/connecting.html#your-snowflake-account-name)   in **Full account name**. The name is used to assemble the URL to your   Snowflake account. It is displayed automatically in **Connection URL**.2. Add the name of the Snowflake warehouse in **Warehouse**.3. Provide the name of the Snowflake database in **Database**.## Authentication1. Create an [external secret](../secrets.html) that contains the   password to Snowflake in your AWS Secrets Manager . Use the prefix   `starburst-galaxy-`.2. Enter the name of the user to connect to Snowflake in **Username**.3. Complete the secret name in **AWS secret key name**.3. Click **Connect data source**.Now that you&#39;ve created a data source, you can [createclusters](../../clusters/index.html).Navigate to the **Data sources** page to edit, delete, or add new data sourcesat any time."
 },
 {
  "title": "Try Starburst Enterprise on any Linux",
  "url": "/starburst-enterprise/try/any-linux.html",
  "content": "# {{page.title}}You can install {{site.terms.sep_first}} on any 64-bit Linux distribution withthe following steps:1. Obtain the most recent {{site.terms.sep}} `tar.gz` archive1. Unpack the archive as root1. Add configuration files1. Start the {{site.terms.sep}} server1. Obtain the {{site.terms.oss}} CLI client and run tests## Prerequisites{{site.terms.sep_full}} requires a Linux distribution that:* Is no more than a few years old* Runs on 64-bit Intel hardware* Has Python 2.7 or later, needed to run the `launcher` utility.* Has Java 11.0.11 or a later Java 11 LTS release from Azul, OpenJDK, or Oracle  Java distributions. Newer Java releases may work but are not tested  or supported.({{site.terms.sep}} also installs *for evaluation purposes only* on a macOSrelease with the same prerequisites.)## Download an {{site.terms.sep}} archiveTo gain access to {{site.terms.sep}} archives, visit the [{{site.terms.sb}}website](https://www.starburst.io) and click either the**Get Started** or **Download Free** buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form *using a valid email address*, then click **Free Download**.A few moments later, you receive email from {{site.terms.sb}} with a link to thedownloads page.{% include note.html content=&quot;You can optionally reply to the email fromStarburst to request a trial license that unlocks the performance and securityenhancements of Starburst Enterprise. However, to get a server up and runningquickly, you can postpone using a license for now.&quot; %}The Downloads page is organized into a Long-Term Support section at the top withSteps 1 and 2, and a Short-Term Support section at the bottom. Use the LTSbuttons.From the **Step 1: Starburst Enterprise** section, click the **Tarball** button.This starts the download of a file named with the pattern`starburst-enterprise-*.tar.gz`. If prompted to open or save the file, save itto your `/home//Downloads` directory.Next, from the **Step 2: Client applications** section, click the **CLI**button. This starts the download of `trino-cli-*-.executable.jar`; if prompted,save it to your `/home//Downloads` directory. We use this file later.{% comment %}If you prefer to use the RPM installation type on a Red Hat family Linux, see[cross-ref-TBD].{% endcomment %}## Unpack the archiveThe contents of the `tar.gz` archive are by default owned by root and include atop-level container directory. The `--strip 1` option in the `tar` command shownhere strips that container directory off.First create an empty target directory, then extract the contents of the`tar.gz` file without its container directory into the target directory. Forexample:```shellcd /home//Downloadssudo mkdir -p /opt/starburstsudo tar xvzf ./starburst-enterprise-*.tar.gz --strip 1 -C /opt/starburst```You can replace the asterisk (*) in the file name with the version of StarburstEnterprise that you downloaded, such as `356-e.1`. But the command works asshown, without the version string.The directory `/opt/starburst` is called the *installation directory*. Inspectthe new directory to find that it contains two top-level files and fourdirectories:```textNOTICEREADME.txtbinlibpluginstarburst-insights```## Add configuration filesEven the simplest {{site.terms.sep_full}} server must have a minimum set ofconfiguration files before it can start. To add those files, create a directoryin `/opt/starburst` named `etc`, parallel to `bin` and `lib`. Populate `etc`with the following configuration files, using contents suggested in [Configuring{{site.terms.oss}}](../../latest/installation/deployment.html#configuring-trino).  node.properties  Follow the sample in      Node      properties. For the node.properties line, the suggested      value production has no special meaning; use any value, such as      test.  jvm.config  Follow the sample in JVM      config. Make sure you are viewing the      deployment page      that matches the {{site.terms.sep}} version you downloaded, then use the      suggested text verbatim. An exception is to adjust the -XmX      value as appropriate for your test environment.  config.properties  Follow the sample in            Config properties. Use the third example in that section to specify a      combined coordinator and worker machine.  catalog property files  Create a subdirectory etc/catalog. In the catalog      subdirectory, create the following files, each containing a single line of      text.      See       Catalog properties for guidance.                        File name          Contents                          blackhole.properties          connector.name=blackhole                          jmx.properties          connector.name=jmx                          memory.properties          connector.name=memory                          tcpds.properties          connector.name=tcpds                          tpch.properties          connector.name=tpch                Start on [Configure and define catalogs](../../data-engineer/catalogs.html) tolearn more about the relationship between data sources, catalogs, andconnectors.## Alternate configuration filesAnother way to get started quickly is to use the set of configuration filesprovided as examples for the O&#39;Reilly book [Trino: The DefinitiveGuide](https://www.starburst.io/info/oreilly-trino-guide/).To use these ready-to-use configuration files, download the samples from their[GitHub location](https://github.com/trinodb/trino-the-definitive-guide)either as a zip file or a Git clone. Let&#39;s say you place your clone or unzipdirectory in `/home//bookfiles`.From `bookfiles`, copy the entire `etc` directory from the `single-installation`sample folder to your `installation` directory. For example:```shellcd /home//bookfiles/single-installationsudo rsync -av etc /opt/starburst/```## Start the serverOnce configuration files are in place, start the server. From the `installation`directory, run the following command:```shellsudo bin/launcher start```For a successful server start, the launcher script returns a process ID. Checkthe server&#39;s status to make sure the server finished its startup process:```shellsudo bin/launcher status```As an alternative, look for the exact phrase &quot;SERVER STARTED&quot; in the`server.log` file, which by default is in the `var/log` subdirectory of theinstallation directory:```shellgrep &quot;SERVER STARTED&quot; /var/log/server.log```## Verify the serverTo verify that your locally-run server is operating as expected, invoke the{{site.terms.oss}} UI as described in [Verify theserver](./verify.html).## Run queriesTo run queries against your server, use the {{site.terms.oss}} CLI as describedin [CLI](../../data-consumer/clients/cli.html)."
 },
 {
  "title": "Configure your AWS account",
  "url": "/starburst-galaxy/get-started/aws-create-role.html",
  "content": "# {{ page.title }}You need to configure a cross-account role for your AWS account. This enablesthe {{site.terms.galaxy}} to deploy and manage {{site.terms.sb}} clusters inyour account.## Create a cross account roleCreate a cross-account role using a cloud formation template (CFT) in the[AWS management console](https://aws.amazon.com/console/).1. In your AWS management console, go to **IAM** services.   You may be prompted to sign in to AWS.2. Download the **Cloud formation template** from {{site.terms.galaxy_full}}.3. Upload the **Cloud formation template** in your AWS console.4. Copy the **External ID** from {{site.terms.galaxy_full}} and paste it in your   AWS console.5. Run the cloud formation template.&gt; **IMPORTANT**: Copy the AWS **Account ID** where you created the role. You&gt; need it for the next steps.## Enter the cross-account role informationAdd the cross-account information to {{site.terms.galaxy_full}}.1. Navigate back to {{site.terms.galaxy_full}}2. In the AWS **Account ID** field, enter the AWS **Account ID** from the role   you created.3. Select the default **AWS region** for your AWS resources.4. Click **Finish**.You can edit your AWS region from **Settings** or on individual clusters asneeded."
 },
 {
  "title": "Using the cache service",
  "url": "/data-engineer/cache-service.html",
  "content": "# {{ page.title }}[Starburst Enterprise platform](../starburst-enterprise/index.html) (SEP) offerscaching options through its [cache service](../latest/admin/cache-service.html)to help you reduce query time and costs.The cache service can automatically manage refreshes of materialized views [inHivecatalogs](../latest/connector/starburst-hive.html#automated-materialized-view-management).With [table scan redirections](../latest/admin/table-scan-redirection.html), youcan redirect data requests for a table in one catalog to a cached version of itin another catalog on a more performant system, reducing the query load on theoriginal data source. This redirection is transparent to the user, and thereforeprovides performance improvements without the need to modify user queries.This document provides an overview of the cache service, including links torelevant sections of our reference documentation.## Select the right cache strategyBoth materialized views and table scan redirections improve query performance.Find out which one is right for your organization in this guide.### Materialized viewsMaterialized views created in Hive catalogs are backed by the cache service andthe Hive Metastore and can be automatically refreshed. If you use{{site.terms.sep}} with Hive and the cache service, you already have thenecessary prerequisites to use automatically refreshed materialized views.Any query that runs successfully in {{site.terms.sep}} can be used to create amaterialized view. Users access the materialized view through the catalog it ismade available from, just as they would any other data source.Materialized views in Hive allow you to access the results of a query from anydata catalog you have defined in {{site.terms.sep}} in real time withoutre-processing the query, and with a refresh scheme that works best for yourorganization.### Table scan redirectionsTable scan redirections allow you to transparently redirect queries to a cachedversion in a location that incurs lower egress costs, is more performant, orboth. Your users get even faster performance without changing the vetted queriesthat they depend upon.Table scan redirection is available for nearly all {{site.terms.sb}} connectors.## Get startedThe information in this section helps you get {{site.terms.sb}} Cached Viewsup and running using the {{site.terms.sb}} cache service. It includes links tomore detailed information in our reference documentation. As with anyconfiguration change in {{site.terms.sep}}, you must restart the server toapply the changes.### Enable the cache serviceTo use table scan redirections or Hive materialized views, the cache servicemust first [be enabled](../latest/admin/cache-service.html#configuration). Ourreference documentation has[requirements](../latest/admin/cache-service.html#requirements) and instructionsfor doing so. Take the time to read about the [two types ofdeployments](../latest/admin/cache-service.html#installation), standalone andembedded, to see which is better suited to your cluster. If your organizationuses our Helm charts to deploy {{site.terms.sep}}, you can use the [Helm chartfor the cache service](../latest/k8s/cache-service-configuration.html) to deployit as standalone service.Before you begin, ensure that you have an [external databaseinstance](../latest/admin/cache-service.html#relational-database) available tostore information about materialized views and table scan redirections.When you are ready, there are a number of [configurationproperties](../latest/admin/cache-service.html#configuration) that must be set,shown in the following example:```textservice-database.user=aliceservice-database.password=test123service-database.jdbc-url=jdbc:mysql://mysql-server:3306/redirectionsstarburst.user=bobstarburst.jdbc-url=jdbc:trino://coordinator:8080rules.file=etc/rules.json```Where these get set depends on whether or not you are using a standalone orembedded cache service, and whether you are using Helm charts or not:* Standalone -  * via the cache service Helm chart in the ``config.properties`` node nested    under the top level ``config`` node.  * Other deployments in the ``etc/config.properties`` file on the cache service    server.* Embedded -  * via the SEP Helm-chart in the ``config.properties`` node nested under the    top level ``coordinator`` node in the ``config`` section node.  * Other deployments in the ``etc/cache.properties`` file on the coordinator    server.### Enable materialized viewsMaterialized views must be [enabled in the Hivecatalog](../latest/connector/starburst-hive.html#sb-hive-mv-catalog-config) thatthey are accessed from. In addition, you must specify a schema to containmaterialized view storage. We strongly recommend that you create a schemadedicated to materialized views for a given catalog. A namespace must also bespecified; it is used internally by the cache service to avoid inadvertent namecollisions. To create a new schema with a specified location, use a commandsimilar to the following, which creates the ``mymvstorage`` schema in the``myhive`` catalog:```sqlCREATE SCHEMA myhive.mymvstorage WITH (location = &#39;s3a://mymvstorage/&#39;);```Each Hive catalog must be configured to allow materialized views and use theschema you created for materialized views:```textmaterialized-views.enabled=truematerialized-views.namespace=mymvnamespacematerialized-views.storage-schema=myhive.mymvstoragecache-service-uri=http://:8180```Users with the necessary access privileges in a schema in the configured catalogcan [create materialized views in the usualmanner](../data-consumer/materialized-views.html).### Enable table scan redirectionsLike materialized views, table scan redirections are also managed by the cacheservice. The redirections are transparent to users, so no additional training,modifications of queries or new commands are needed to use them.Table scan redirection refreshes are configured in a [JSON-formatted rulesfile](../latest/admin/cache-service.html#cache-refresh-rules-for-redirections).There are numerous properties to govern both global defaults for all rules andrule- specific behaviors.Table scan redirections offer even finer-grained control for refreshes thando materialized views, including cleanup options."
 },
 {
  "title": "Configure a catalog",
  "url": "/starburst-enterprise/try/catalog.html",
  "content": "# {{ page.title }}Before you can connect to your data sources with {{site.terms.sep_first}} youmust define them in a catalog. Each catalog connects to a single data source,such as a specific instance of a PostgreSQL or Oracle database. Catalogs aredefined in your Helm chart under the `catalogs:` node, or if you are not usingour K8s deployment, in its own properties file the `etc/catalog` directory of{{site.terms.sep}}.The name of the catalog YAML entry or catalog file becomes the name used toaccess the data in a query. There is a small set of required properties forevery catalog. While properties can vary, they minimally define the connectionto the data source. Depending on your data source type, there are anumber of different configuration properties available.This document teaches you how to create a minimum catalog configuration.## Gather your connection informationYou will need the following minimum information to connect to a data source:* Connection URL* Username* Password## Create your catalog propertiesThe catalog properties file content consists of `key=value` pairs. Every catalogmust include the `connector.name` property, and the[connector](../../../latest/connector/starburst-connectors.html) name must matchthe data source.In the `values.yaml` file in your K8s deployment, you must create a catalogentry under the `catalogs:` top-level node. The following example assumes thatthe `salesdata` catalog resides in a PostgreSQL database:```yamlcatalogs:  salesdata: |-    connector.name=postgresql    connection-url=jdbc:postgresql://example.net:5432/database    connection-user=root    connection-password=secret```If you are not using K8s, you must create the minimum contentfor a PostgreSQL catalog file. In the following example, the contents arestored as `etc/catalog/salesdata.properties`:```propertiesconnector.name=postgresqlconnection-url=jdbc:postgresql://example.net:5432/databaseconnection-user=rootconnection-password=secret```## Apply changesTo use the a newly-defined catalog, you must restart {{site.terms.sep}} for thechanges to take effect. Refer to the documentation for your deployment type forrestart instructions.## Next stepsTo learn more about catalogs and connectors in {{site.terms.sep}}:* Read our data engineering [guide to creating  catalogs](../../data-engineer/catalogs.html).* Familiarize yourself with the available {{site.terms.sep}}  [connectors](../../../latest/connector/starburst-connectors.html)"
 },
 {
  "title": "Configure and define catalogs",
  "url": "/data-engineer/catalogs.html",
  "content": "# {{ page.title }}You need to understand data sources and how they are connected to{{site.terms.sep_first}}, to take advantage of the query performance availableto your data consumers. The following content provides an overview of datasources and catalogs and how they work with connectors in {{site.terms.sep}}.## DefinitionsBefore you begin, here are definitions and explanations the key concepts of for*data sources*, *catalogs*, and *connectors*.### Data sourcesA data source is a system where data is retrieved from. You can query systemssuch as distributed object storage, RBDMSs, NoSQL databases, document databases,and many others.In {{site.terms.sep}}, you must connect to a data source so you can query fromthat source. To query those sources, you need to create a *catalog propertiesfile* to define a catalog.### CatalogsCatalogs define and name the configuration to connect to and query a datasource. They are a [key concept](../../latest/overview/concepts.html) in{{site.terms.sep}}.Without catalogs there is nothing to query.The catalog name is defined by the name of the catalog properties file, locatedin ``etc/catalog``. You can have as many catalogs as you want, and there are norestrictions for naming outside of valid character types. For example, afilename of ``etc/catalog/mydatabase.properties`` results in the catalog name``mydatabase``.The catalog properties file content defines the access configuration to the datasource. Properties are ``key=value`` pairs.Each catalog defines one and only one connector using the required``connector.name`` property. You must select the correct connector for your datasource. For example, the PostgreSQL connector is defined by using``connector.name=postgresql``, and enables the catalog to access a PostgreSQLdatabase. Another example is the Hive connector defined by``connector.name=hive``. It can enable a catalog to access Hadoop,Amazon S3 and many other object storage systems. {% include note.html content=&quot;While a catalog can have only one connector, asingle connector may be used by multiple catalogs.&quot; %}Access a list of catalogs with the[``SHOW CATALOGS``](../latest/sql/show-catalogs.html) command.#### Configuration propertiesEach connector has a small set of required properties. While properties canvary, they minimally define the connection to the data source. Depending on yourconnector and data source, there are a number of different configurationproperties available.Optional properties enable further configuration of the catalog in areas such assecurity, performance, and query behavior. These connector-specific propertiesare defined in the documentation for each connector. For more information onconnector-specific configuration properties, start with the [list of allconnectors](../latest/connector.html).#### Catalog session propertiesYou can further customize the behavior of the connector in your catalog usingcatalog session properties. A session is defined by a specific user accessing{{site.terms.sep}} with a specific tool such as the CLI. Catalog sessionproperties can control resource usage, enable or disable features, and changequery processing.Most of the session properties are similarly named to their config propertiescounterparts in a catalog file, mostly differing by the use of underscores (_)in the name to be SQL compliant. Session properties override catalog propertiesin certain circumstances.You can view current session properties using the[``SHOW SESSION``](../latest/sql/show-session.html) command. Thenimplement your session properties using[``SET SESSION``](../latest/sql/set-session.html).### ConnectorsA connector is specific to the data source it supports. It transforms theunderlying data into the {{site.terms.sep}} concepts of schemas, tables,columns, rows, and data types.Connectors provide the following between a data source and {{site.terms.sep}}:- Secure communications link- Translation of data types- Handling of variances in the SQL implementation, adaption to a provided API,  or translation of data in raw files## Setup of connectorMost {{site.terms.sep}} connectors include their configurations and anythingelse you might need by default, and therefore no setup is required.If you&#39;re using a connector that requires additional set up, such as theaddition of a proprietary JDBC driver, you find that documented with the[specific connector](../latest/connector.html).### Create a catalog properties fileYou can create catalog to access a data source with a few simple steps:- Create the catalog properties file in ``etc/catalog/``, for example  ``etc/catalog/mydatabase.properties``- Specify the required connector with ``connector.name=`` in file, for example  ``connector.name=postgresql``- Add any other properties required by the connector- Add any optional properties as desired- Copy the file onto the coordinator and all worker nodes- Restart the coordinator and all workers- Confirm the catalog is available with ``SHOW CATALOGS;``- List the available schemas with ``SHOW SCHEMAS FROM mydatabase;``- Start writing and running the desired queriesMany connectors use the similar properties in catalog properties files. Forexample, most JDBC-based connectors require these minimum properties for theircatalog files:```propertiesconnector.name=[connectorname]connection-url=[connectorprotocol]//:;database=connection-user=rootconnection-password=secret```"
 },
 {
  "title": "Connection parameters in CFT",
  "url": "/ecosystems/amazon/cft-connection-info.html",
  "content": "# {{page.title}}To run queries in {{site.terms.sep_first}}, you need the following connectionparameters described in our [client overviewpage](../../data-consumer/clients/index.html#connection-information).## Locating the connection informationTo locate the IP address, protocol, and port of your coordinator, complete thefollowing steps:1. Navigate to the **CloudFormation console** from **Service &gt; Management Tools**.2. Select your stack name and click the associated tab labeled **Outputs**.3. Note the value for the ``StarburstCoordinatorURL``.## Accessing the coordinatorThe ``StarburstCoordinatorURL`` is only accessible from within the same VPC as{{site.terms.sep}}. You must run your client tools from within the VPC, orwith connectivity to the VPC. Your IT department can help you with connectivityissues.Any easy way to test connectivity from a network location is to access the [WebUI](../../latest/admin/web-interface.html) using the``StarburstCoordinatorURL`` in your browser.## Connecting clients to {{site.terms.sep}}Once you have gathered the connection information, you can use [anyclient](../../data-consumer/clients/index.html) supported by {{site.terms.sep}}to connect and query.For example, to connect and query with the Command Line Interface (CLI):1. Download and install the [CLI](../../data-consumer/clients/cli.html)2. Use the parameters determined in the preceding section.3. Run the CLI with the URL and username supplied and activate the password   dialog.Once connected you can check for available catalogs with ``SHOW CATALOGS``, orrun any other query."
 },
 {
  "title": "Choosing the right deployment",
  "url": "/get-started/choosing-the-right-deployment.html",
  "content": "# {{ page.title }}[Starburst Enterprise](../starburst-enterprise/index.html) is available foron-premise usage or in private clouds, and can be run on bare metal servers,virtual machines and containers, all managed by you.You can also run {{site.terms.sep_full}} on public cloud provider systems, andtheir virtual machine or container offerings, or for further simplicity andconvenience [use their marketplaces](../ecosystems/index.html).That’s a lot of choice! This guide helps you select the best solution for yourorganization.The deployment you choose depends on a couple of key factors:* **People** and available skills in your organization* Variety and location of **data sources*** **Location** of your computing resources* **Security** and governance requirements## PeopleAs you decide the best {{site.terms.sb}} product and deployment style for yourorganization, the skill sets available in your organization is the mostimportant factor. You need the expertise of [platformadministrators](./starburst-personas.html#platform-administrator) and [dataengineers](./starburst-personas.html#data-engineer) to create, configure andmaintain clusters capable of running {{site.terms.sep_full}}. Your organizationmust also have people who can connect the desired data sources and ensure thatdata security and governance requirements are met. Typically, people with theseskill sets sit within your IT organization.[Starburst Enterprise](../starburst-enterprise/index.html) offers the mostcontrol of your deployments at the cost of being the most complex to install andmaintain.With limited availability using a public cloud provider or even [marketplaceoffering](../ecosystems/index.html) can help reduce the workload.## Data sourcesThe data sources you plan to query have a large impact on your choice. You needto understand what databases, object storage or others systems your users needto query. In addition, you need to know where these system are deployed toensure that {{site.terms.sep_full}} or {{site.terms.galaxy_full}} can accessthem with sufficient network performance and capacity.For example, if all your data is stored in your private network and data center,you should run {{site.terms.sep_full}} there as well. However, if all your datais hosted on a public cloud provider, you can choose to run{{site.terms.sep_full}} on the same cloud provider yourself, use a marketplaceoffering or even use {{site.terms.galaxy_full}}.{{site.terms.sep_full}} includes a very large variety of[connectors](../latest/connector.html) to support the most common datasources. They include many RDBMSs, Hadoop/Hive and other object storage systems,and commercial platforms such as Snowflake or Teradata. In addition,{{site.terms.sep_full}} can be operated anywhere. If your organization needs toquery large collection of data sources, {{site.terms.sep_full}} is the rightchoice for you. You can choose various options on how to run it, and ensure itis located closely to your data sources.{{site.terms.galaxy_full}} supports a [limited set of datasources](../starburst-galaxy/data-sources/index.html), and can only run in yourAWS account.## LocationIf you run exclusively on-prem, then your choice is easy -{{site.terms.sep_full}}. You can run it on bare metal servers, on virtualmachines, in private clouds, or even in your Kubernetes clusters on supportedcloud platforms. Your choice depends entirely on your requirements, datasources, people and skills.If you are using a single cloud provider, you can run {{site.terms.sep_full}},again with the options to manage the virtual machines yourself and use the`tar.gz` or RPM archive.You can also use a Kubernetes offering from cloud providers, and combine it withthe our [Kubernetes support with Helm charts](../latest/k8s.html).Alternatively, you can use cloud-specific offerings such our [AmazonCloudformation support](../latest/aws.html).{{site.terms.sb}} also makes {{site.terms.sep_full}} available in all major[marketplaces](../ecosystems/index.html).If you use a multi-cloud or hybrid cloud strategy, the locality of your datashould be your first consideration to reduce data transfer costs. You can evenrun multiple {{site.terms.sep_full}} clusters, and potentially connect them with[Starburst Stargate](../latest/connector/starburst-stargate.html).Here are some things to consider in choosing a location for your{{site.terms.sep_full}} cluster, keeping minimizing intra-cloud data transfer aslow as possible front-of-mind:* Can you place your cluster in a cloud that contains multiple data sources?* What intra-cloud data sources are most likely to be federated? Where is the  largest of them?* Is one of your cloud provider pricing models more favorable than the others?The closer you can put {{site.terms.sep_full}} to the bulk of your data, themore you can reduce the amount of data being returned, and save on data transfercosts.## SecurityWhat does data security and governance look like at your organization?{{site.terms.sep_full}} supports a wide range of security features.* Different authentication platforms such as LDAP, Kerberos or OAuth* Data access control with user impersonation, credential passthrough and others* Authorization management with Ranger or Privacera platform* Event logger for auditing and tracking* and othersOur [securityguide](https://www.starburst.io/info/starburst-enterprise-security-guide/)provides a good introduction.The location and mode of running {{site.terms.sep_full}} specifically determineswhich security features can be used. You need to specifically verify if afeature can be used based on your determined location.For example, if your company relies on [ApacheRanger](../latest/security/ranger-overview.html) or [Privaceraplatform](../latest/security/global-privacera.html) to secure your data,then a self-managed {{site.terms.sep_full}} in a private cloud or on-prem ismost likely the right choice for you.Depending on your particular security needs, one or more marketplace offeringsmay also work. {{site.terms.sep_full}} includes [Helm charts for Apache Rangerinstallation](../latest/k8s/ranger-configuration.html) and usage onKubernetes.## Starburst GalaxyAs our most convenient solution, {{site.terms.sb}} now offers a hosted andmanaged solution - [Starburst Galaxy](../starburst-galaxy/index.html).{{site.terms.galaxy_full}} is the most hands-off approach as it provides asimple user interface and most complexity, including deployment and upgrade,is completely taken care of for you by {{site.terms.sb}}."
 },
 {
  "title": "CLI",
  "url": "/data-consumer/clients/cli.html",
  "content": "# {{page.title}}The {{site.terms.oss}} command line interface (CLI) provides a terminal-based,interactive shell for running queries and inspecting catalog structures in any{{site.terms.sep}} cluster.## Setup to use the CLIThe CLI is distributed as an executable JAR file that you download, rename, andplace in a directory in the PATH.## Connection informationGet the [connection information](./index.html#connection-information) and the[cluster version](./index.html#connection-information) for the cluster you wantto connect to and use to run queries.### RequirementsThe CLI requires a ``java`` command on the PATH from Java 8 or newer. It isusually easiest to use the same Java 11 required by {{site.terms.sep}} itself,described on [Java runtimeenvironment](../../latest//installation/deployment.html#java-runtime-environment).The CLI needs to be compatible with the [version of the cluster you areconnecting to](./index.html#cluster-version).### Download the CLIYou can request the CLI for a specific version from[{{site.terms.support}}](../../support.html).Alternatively, to gain direct access to {{site.terms.sep}} archives, visit the[{{site.terms.sb}} website](https://www.starburst.io) and click either the **GetStarted** or **Start Free** buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form *using a valid email address*, then click **Free Download**.Click on the link to the **Downloads** page in the resulting email. It isorganized into Long-Term and Short-Term Support sections.Download a specific version of the CLI, based on your cluster version:* Version 350, if your cluster version is 350 or older* Same or newer version than the cluster for versions 354 and newerYou can also download the CLI binary straight from the link in the referencedocumentation:* [Version 350](../../350-e/installation/download.html)* [Version 354](../../354-e/installation/download.html)* [Latest version](../../latest/installation/download.html)Use the version selector in the documentation for different releases, or contact{{site.terms.support}}.### Rename and place the CLIYou downloaded an executable JAR file that is usable as-is. To make it practicalwith having to call Java, copy it to a directory in the PATH, rename it, andmake it executable. For example:```shellcd /usr/local/bincp -p /home//Download/trino-cli-*-executable.jar .mv trino-cli-*-executable.jar trinochmod +x trino```On macOS, substitute the path `/Users//Download/...` in the secondline.{% include note.html content=&quot;Older versions of the CLI use the name `presto`instead of `trino` for the command as well as the prompt.&quot; %}### First stepsNow you are ready to verify that the CLI runs. Check if you get a similar outputto the following listing:```shell$ trino --versionTrino CLI 360```The CLI works. Now gather the [connection information](./index.html#connectioninformation) for your cluster, and connect to it. You can learn more about usingthe CLI from the following tutorial or [the Command line interfacesection](../../latest/installation/cli) in the {{site.terms.sep}} referencedocumentation.## CLI tutorialThe following sections provide a brief introduction to using the CLI.### Interactive CLIAt the shell prompt, enter `trino` with no arguments. By default, this connectsto the running {{site.terms.sep}} server at the default address and port,`localhost:8080`.```shelltrino```Your cluster might use one of the other [defaultports](./index.html#default-ports) for {{site.terms.sep}} clusters, or adifferent port assigned by an administrator. If your {{site.terms.sep}} serveruses a different port or is running elsewhere on your network, specify theserver&#39;s URL with `--server`. For example:```shelltrino --server=cluster.example.com:8082```This opens a {{site.terms.oss}} CLI shell, with `trino&gt;` prompt. All commandsentered here must be terminated with a semicolon.Exit the CLI interactive mode with `quit;` or `exit;` or `Ctrl+D`:```sqltrino&gt; quit;```### Usage helpTo see the available commands in interactive mode:```shelltrino&gt; help;```This returns:```shellSupported commands:QUITEXITCLEAREXPLAIN [ ( option [, ...] ) ]     options: FORMAT { TEXT | GRAPHVIZ | JSON }             TYPE { LOGICAL | DISTRIBUTED | VALIDATE | IO }DESCRIBE SHOW COLUMNS FROM SHOW FUNCTIONSSHOW CATALOGS [LIKE ]SHOW SCHEMAS [FROM ] [LIKE ]SHOW TABLES [FROM ] [LIKE ]USE [.]```### Show configured resourcesEvery {{site.terms.sep}} server is configured to connect to one or more datasources by means of a *catalog* that defines the connection type. Each cataloghas at least one *schema*; each schema has at least one *table*.To see the list of catalogs configured for the current server, run:```sqltrino&gt; SHOW CATALOGS;```For the {{site.terms.sb}}-provided[Docker]({{site.baseurl}}/starburst-enterprise/try/docker) image, this returns:```text  Catalog----------- jmx memory system tpcds tpch(5 rows)```(Each query also returns a set of performance metadata, which is not repeatedhere.)### Explore the tpch catalogStart with the `tpch` catalog, which allows you to test the capabilities andquery syntax of {{site.terms.sep}} without configuring access to an externaldata source. The TPCH connector is described on its [documentationpage](../../latest/connector/tpch.html).Use the `SHOW SCHEMAS` command to list schemas provided by the TPCH connector:```sqltrino&gt; SHOW SCHEMAS FROM tpch;```This returns:```text       Schema-------------------- information_schema sf1 sf100 sf1000 sf10000 sf100000 sf300 sf3000 sf30000 tiny(10 rows)```To see the tables in one of these schemas:```sqltrino&gt; SHOW TABLES FROM tpch.sf100;```### Save typing with USETo avoid typing the catalog and schema name every time:```sqltrino&gt; USE tpch.sf100;trino:sf100&gt;```To see the tables in `sf100`:```sqltrino:sf100&gt; SHOW TABLES;```which returns:```text  Table---------- customer lineitem nation orders part partsupp region supplier(8 rows)```To see the structure of the `customer` table:```sqltrino:sf100&gt; SHOW COLUMNS FROM customer;```which returns:```text   Column   |     Type     | Extra | Comment------------+--------------+-------+--------- custkey    | bigint       |       | name       | varchar(25)  |       | address    | varchar(40)  |       | nationkey  | bigint       |       | phone      | varchar(15)  |       | acctbal    | double       |       | mktsegment | varchar(10)  |       | comment    | varchar(117) |       |(8 rows)```### CLI with argumentsYou can submit a valid SQL script on the `trino` command line:```shelltrino --execute &#39;SELECT custkey, name, phone, acctbal FROM tpch.sf100.customer LIMIT 7&#39;```You can send a SQL script file to the CLI from the command line by specifyingits name as an argument to the `-f` or `--file` command line options.```shelltrino -f filename.sql```Two TPCH scripts are included with the sample files for the O&#39;Reilly book[Trino: The DefinitiveGuide](https://www.starburst.io/info/oreilly-trino-guide/).To use these scripts, download the book&#39;s samples from their [GitHublocation](https://github.com/trinodb/trino-the-definitive-guide) either as a zipfile or a Git clone. Let&#39;s say you place your clone or unzip directory in`/home/username/bookfiles` (Linux) or `/Users/username/bookfiles` (macOS). Thenuse the TPCH scripts as follows:```shellcd /home/username/bookfiles/tpchtrino -f nations.sql```The second sample script requires the [Black holeconnector](../../latest/connector/blackhole.html) to be configured for thecurrent server. This connector is designed to operate like `/dev/null` and`/dev/zero` for performance testing and similar use cases.You can configure a local Docker-hosted {{site.terms.sep}} server to use thisconnector by following the steps in [Map a local etcdirectory]({{site.baseurl}}/starburst-enterprise/try/docker.html#localetc).If the target {{site.terms.sep}} server has the Black Hole connector, then runthe second sample script:```shelltrino --file=tpch-queries.sql```To run this second script more than once, you must stop and restart the{site.terms.sep}} server.For a locally running `tar.gz` installation, in the `sep-root` directory foryour server, run:```shellsudo bin/launcher restart```For a locally running Docker server:```shelldocker restart sepdock```{% include note.html content=&quot;To see the Web UI in action, keep the Web UI openand visible in a browser window while the `tpch-queries` script runs.&quot; %}## Further studyMore information on the CLI is available in [the Command line interfacesection](../../latest/installation/cli) in the {{site.terms.sep}} referencedocumentation."
 },
 {
  "title": "Starburst Enterprise cluster basics",
  "url": "/platform-administrator/cluster.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is powerful and highly configurable. We have extensivedocumentation to help you ensure that {{site.terms.sep}} works as efficientlyand securely as possible in your environment.## ArchitectureA {{site.terms.sep}} cluster consists of a coordinator and many workers. Usersconnect to the coordinator with their SQL query tool. The coordinatorcollaborates with the workers. The coordinator as well as all the workers accessthe connected data sources. This access is configured in[catalogs](../../data-engineer/catalogs.html).You can learn more in the [conceptssection](../latest/overview/concepts.html) of the reference documentation.Processing each query is a stateful operation. The workload is orchestrated bythe coordinator and spread parallel across all workers in the cluster. Each noderuns {{site.terms.sep}} in one JVM instance, and processing is  parallelizedfurther using threads.## Memory and CPU resource considerationsTypical workloads for {{site.terms.sep}} require large amounts of memory and CPUfor processing. For optimal scheduling all workers need to have the same largeamount of memory allocated.## OS and software requirements{{site.terms.sep}} requires Linux, Java, and Python. Specific details vary foreach version and deployment platform.## Networking{{site.terms.sep}} has a few networking aspects you need to consider:* Access to the coordinator for users requires HTTP/HTTPS access* Access from the cluster to any external authentication system such as LDAP* Access from the all cluster nodes to any queried data sources## Securing{{site.terms.sb}} has an array of powerful, comprehensive security features toensure that your data governance and security are top-notch. We strongly suggestyou begin by watching the training video below. After that, our extensivesecurity documentation will help you get started securing your data with{{site.terms.sep}}.* [Securing Starburst training video](./security.html)* [Security documentation](../../latest/security.html)## Configuration basicsWhen you are ready to install, we&#39;ve got you covered with detailed referencedocumentation covering everything from deployment to setting up data sourceconnections:* [Coordinator and worker configuration](../../latest/installation/deployment.html#config-properties)* [Node properties](../../latest/installation/deployment.html#node-properties)* [JVM configuration](../../latest/installation/deployment.html#jvm-config)* [Catalog properties introduction](../../latest/installation/deployment.html#catalog-properties)## Deployment options{{site.terms.sep}} can be deployed in several ways, depending on yourorganization&#39;s infrastructure, skills, and existing tooling. Follow the linkmost appropriate to your environment to learn more about deploying{{site.terms.sep}}:We highly suggest using a cloud provider marketplace offering, and self-managedKubernetes deployments in the cloud for production clusters:* [Google Cloud Marketplace](../../ecosystems/google/index.html)* [Red Hat Marketplace](../../ecosystems/redhat/index.html)* [Microsoft Azure Marketplace](../../ecosystems/microsoft/index.html)* [Self-managed Kubernetes deployment](../latest/k8s.html) on any  cloud provider platform including AKS, EKS, GKE and OpenShift.The following options are available for baremetal servers or virtual machines inyour own data center or in a cloud environment:* [RPM](../../latest/installation/rpm.html) with your own custom tooling* [Tarball](../../latest/installation/tarball.html) with your own custom tooling* RPM or tarball deployments managed with  [Starburst Admin](../starburst-enterprise/starburst-admin/index.html)* AWS deployment with [AMI and CFT](../../latest/aws.html)"
 },
 {
  "title": "Connectors",
  "url": "/data-engineer/connectors.html",
  "content": "# {{ page.title }}{{site.terms.sep}} comes with [supported enterpriseconnectors](../latest/connector.html) that allow you to configure[catalogs](./catalogs.html) that provide access to all your data sources.{{site.terms.sep}}’s architecture fully abstracts the data sources it canconnect to, compute and storage are separated. You can scale your query engineand performance separately from your data storage.Exposing all this data in one place creates an accessible data mesh, ready foryour analysis.## Developing custom connectors{{site.terms.sep_first}}  comes with an array of built-in connectors for avariety of cloud-based and on-prem data sources.That separation means that you can use the {{site.terms.sep}} connector serviceprovider interface (SPI) to build plugins for file systems and object stores,NoSQL stores, relational database systems, and custom services without anoff-the-shelf connector. As long as you can map data into relational conceptssuch as tables, columns, and rows, it is possible to create your own{{site.terms.sep}} connector.To learn more, read our latest [developerdocumentation](../latest/develop.html)."
 },
 {
  "title": "Create a cluster",
  "url": "/starburst-galaxy/clusters/create.html",
  "content": "# {{ page.title }}The {{site.terms.galaxy_full}} allows you to create, edit, and delete clustersfrom the interface. Access your clusters at any time by clicking **Clusters** onthe left hand menu.Before you can create a cluster in the {{site.terms.galaxy_full}}, you need toadd one or more [data sources](../data-sources/index.html).1. On the **Clusters** page, select **+ New**. If this is your first cluster,   you can select **+ New cluster** from the **Dashboard** or **Clusters** pages.2. Enter a unique **Cluster name**. Names can use lowercase letters,   numbers, and hyphens.  {% include image.html   url=&#39;../../../assets/img/galaxy/clusters/cluster-name.png&#39;   img-id=&#39;clustername&#39;   alt-text=&#39;Add a unique cluster name&#39;   descr-text=&#39;Image depicting the cluster name field&#39;   screenshot=&#39;true&#39;   %}3. Select your cluster profile (click  next to   the cluster profile name for more details):   * **Cost Optimized**   * **Low Memory**   * **General Purpose**   * **High Memory**4. From the **Add data source(s)** drop down menu, select the data sources for   your cluster. If you don&#39;t have a data source, click **Connect a new data   source** to add one.  {% include image.html   url=&#39;../../../assets/img/galaxy/clusters/add-data-source.png&#39;   img-id=&#39;datasource&#39;   alt-text=&#39;Select one or more data sources&#39;   descr-text=&#39;Image depicting dropdown data source list&#39;   screenshot=&#39;true&#39;   %}5. Select your **Availability zone**.  {% include image.html   url=&#39;../../../assets/img/galaxy/clusters/availability-zone.png&#39;   img-id=&#39;availzone&#39;   alt-text=&#39;Select the availability zone from the dropdown menu&#39;   descr-text=&#39;Image depicting the availability zone from a dropdown menu&#39;   screenshot=&#39;true&#39;   %}6. Set your number of workers. {{site.terms.galaxy_full}} recommends a minimum   of two workers. Check the box next to **Use autoscaling** to provide and   minimum and maximum number of workers for your cluster.  {% include image.html   url=&#39;../../../assets/img/galaxy/clusters/number-of-workers.png&#39;   img-id=&#39;workers&#39;   alt-text=&#39;Set the minimum and maximum number of workers&#39;   descr-text=&#39;Image depicting autoscaling options including the minimum and maximum workers&#39;   screenshot=&#39;true&#39;   %}7. Select **Create cluster** to finish.  {% include image.html   url=&#39;../../../assets/img/galaxy/clusters/create-cluster.png&#39;   img-id=&#39;createcluster&#39;   alt-text=&#39;Click create cluster button&#39;   descr-text=&#39;Image depicting the create cluster button&#39;   screenshot=&#39;true&#39;   %}You can start, stop, and edit your cluster at any time from the **Clusters** page."
 },
 {
  "title": "Data mesh with Starburst",
  "url": "/data-consumer/data-mesh.html",
  "content": "# {{ page.title }}{{site.terms.sep_full}} is the world’s fastest distributed SQL query engine. Itlets data consumers query anything, anywhere, and get the data they need in asingle query, no matter where it lives. This idea of combining data fromdisparate sources is called a *data mesh*. Specifically {{site.terms.sep_full}}supports query federation where a query access many different datasources at thesame time. These data sources are [defined as catalogs and expose data inschemas and tables](../data-engineer/catalogs.html).It allows you to combine, for instance, historical data from HDFS orobjects stores with the most recent incoming data from Kafka one query.Federating data is simple. You just need to use the fully-qualified name of thetables in your `FROM` clause. Table names are fully-qualified when they includethe catalog and schema name:```..```A *catalog* defines the schemas in a data source such as Snowflake,Oracle and Hive.Here&#39;s an example of data from two different sources, Hive and MySQL, federatedinto a single query:```sqlSELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &gt; 300LIMIT 2;```This query uses data from the following sources:* The `orders` table in the `order_entries` schema defined in the `hive_sales` catalog* The `customer_master` table in the `sf_history` schema defined in the `mysql_crm` catalogTo help you learn how {{site.terms.sb}} uses federated queries in popularanalytics tools, here is a handy walk-through of federating queries for rapidvisualization in[Looker](https://www.starburstdata.com/presto-videos/query-federation-using-looker-and-starburst-presto/)with {{site.terms.sep_full}}."
 },
 {
  "title": "JetBrains DataGrip",
  "url": "/data-consumer/clients/datagrip.html",
  "content": "# {{ page.title }}[DataGrip](https://www.jetbrains.com/datagrip/) by JetBrains is an IDE for databases.It is designed to work with databases installed locally, on a server, or in thecloud. It is installed as a local application on your workstation.You can use the [JDBC driver](./jdbc.html) with DataGrip to access {{site.terms.sep_full}}.The setup steps are similar for versions of[IntelliJ IDEA](https://www.jetbrains.com/idea/) and other tools from JetBrains.  ## RequirementsThe Trino JDBC driver and Datagrip 2021.2 work with {{site.terms.sep}} 354-e or newer.## ConnectionUse the following steps to prepare DataGrip to access your cluster:1. Get the necessary [connection   information](./index.html#connection-information) for your cluster.2. Start DataGrip.3. In the **Welcome to DataGrip** window, click the plus sign icon to start a   new project.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img//general/datagrip-new-project.png&#39;   alt-text=&#39;DataGrip new project&#39;   descr-text=&#39;DataGrip new project&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}4. Name your project.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img//general/datagrip-name-project.png&#39;   alt-text=&#39;DataGrip name project&#39;   descr-text=&#39;DataGrip name project&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}## Connect to a data source5. In the **Database** tool window on the left side of the screen, click the plus   sign, and select **Data Source &gt; Trino**.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-add-data-source.png&#39;   alt-text=&#39;DataGrip add data source&#39;   descr-text=&#39;DataGrip add data source&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}6. In the **General** tab of the **Data Sources and Drivers** dialog:   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-data-sources-general.png&#39;   alt-text=&#39;DataGrip general tab&#39;   descr-text=&#39;DataGrip general tab&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}   a. Name your database.   b. Enter the URL of the Starburst cluster in the **Host** field, and its port   in the **Port** field.   c. Leave the **Authentication** dropdown as is with **User &amp; Password** already   selected.   d. Enter your username in the **User** field.   e. If your cluster is TLS-enabled, you must enter a password, and follow the   upcoming steps for the **Advanced** tab.   f. Do not write in the **URL** field. It is constructed automatically.   G. Click **Apply**7. In the **Advanced** tab of the **Data Sources and Drivers** dialog:   For TLS-enabled clusters, you must configure the JDBC driver property   `SSL=true`.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-data-sources-advanced.png&#39;   alt-text=&#39;DataGrip advanced tab fields&#39;   descr-text=&#39;DataGrip advanced tab fields&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}8. Test the connection by clicking **Test Connection** at the bottom of the   dialog.## Configure the driver9. In the **Drivers** tab, choose **Trino** from the menu.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-driver-general.png&#39;   alt-text=&#39;DataGrip advanced tab fields&#39;   descr-text=&#39;DataGrip advanced tab fields&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}10. In the **General** tab, click the plus sign in the **Driver Files** field.    Navigate to the location of the Trino JDBC driver JAR file, and select it.    &amp;nbsp; {% include image.html    url=&#39;../../assets/img/general/datagrip-select-driver.png&#39;    alt-text=&#39;DataGrip driver general tab fields&#39;    descr-text=&#39;DataGrip driver general tab fields&#39;    screenshot=&#39;true&#39;    pxwidth=&#39;421&#39;    screenshot=&#39;true&#39;    %}11. Click **Apply**, then **Okay**## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is also required touse [TLS/HTTPS](../../latest/security/tls.html). If you&#39;re usingglobally-trusted certificate best practices, use the cluster&#39;s HTTPS URL in theconnection string as shown in the steps above.If you are not using a globally-trusted certificate, you may have to configurethe trust store on your client machine. Consult your site&#39;s networkadministrators for guidance.To use TLS, specify the JDBC parameter setting `SSL=true` as shown in theproceeding section. Alternatively, you can make the same setting appended to theJDBC connection string in the form `?SSL=true`.## QueryingTo reveal the databases and schemas in your data source, expand the nodenext to your data source, then click the three dots.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-choose-databases-schemas.png&#39;   alt-text=&#39;DataGrip choose databases and schemas&#39;   descr-text=&#39;DataGrip choose databases and schemas&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}Run your queries in the query console.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/datagrip-run-query.png&#39;   alt-text=&#39;DataGrip run query&#39;   descr-text=&#39;DataGrip run query&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}"
 },
 {
  "title": "DBeaver",
  "url": "/data-consumer/clients/dbeaver.html",
  "content": "# {{ page.title }}The open source tool [DBeaver Community](https://dbeaver.io/) is a powerful SQLeditor and universal database tool. It is installed as a local application onyour workstation. You can use it to access {{site.terms.sb}}clusters, because  it supports the [JDBC driver](./jdbc.html).The same setup steps apply to the commercial versions of[DBeaver](https://dbeaver.com).  ## RequirementsUsers of {{site.terms.sep_full}} 354-e or newer must use DBeaver 21.0 ornewer.## ConnectionUse the following steps to prepare DBeaver to access your cluster:1. Get the necessary [connection   information](./index.html#connection-information) for your cluster.2. Start DBeaver.3. Right-click in the **Database Navigator** panel and select **Create &gt;   connection**.4. In the **Connect to a database** dialog, select **All** in the left column.5. Enter `trino` in the search field. Select the Trino logo and click **Next**.   (If you are connecting to a {{site.terms.sep}} cluster with release 350-e or   older, instead enter `prestos`. Select the PrestoSQL icon, then **Next**.)   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-connection.png&#39;   alt-text=&#39;DBeaver select driver dialog&#39;   descr-text=&#39;DBeaver select driver dialog&#39;   pxwidth=&#39;699&#39;   screenshot=&#39;true&#39;   %}6. In the **Main** tab of the **Connect to a database** dialog:   a. The initial configuration is set for a server running at `localhost:8080`      with no security settings.   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-connect-to-database.png&#39;   alt-text=&#39;DBeaver connect to database dialog&#39;   descr-text=&#39;DBeaver connect to database dialog&#39;   pxwidth=&#39;575&#39;   screenshot=&#39;true&#39;   %}   b. The **JDBC URL** field is constructed automatically. Do not write in this      field.   c. In the **Host** field, enter the FQDN or IP address of your cluster&#39;s      coordinator (or its load balancer or proxy). Do not include the protocol      at the start of the string or a database name on the end.   d. In the **Port** field, enter the port on which your cluster is listening      for connections. You must fill in a port number, which your network      administrator can provide, or enter one of the [default      ports](./index.html#default-ports) for {{site.terms.sep}} clusters.    e. For your initial connection test, do not use the **Database/Schema**       field. You can come back later to narrow this connection entry to open a       particular database among those managed by your cluster.    f. In the **Username** field, for a cluster without security, enter any name.       For a TLS-enabled cluster, enter a valid username for the authentication       type in use on your cluster, such as LDAP.    g. In the **Password** field, leave blank for a cluster without security.       For a TLS-enabled cluster, enter the valid password for the username       entered.7. For a TLS-enabled cluster, continue into the **Driver properties**   tab.   a. In the **User Properties** grid, right-click and select **Add new      property**.   b. Add a property named `SSL` with value `true`.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-ssl-true.png&#39;   alt-text=&#39;DBeaver driver properties&#39;   descr-text=&#39;DBeaver driver properties&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;486&#39;   screenshot=&#39;true&#39;   %}8. Click the **Test Connection** button in the bottom left of the dialog.   If the JDBC driver is not already installed, this opens the **Download driver   files** dialog showing the latest available JDBC driver. Select that line and   click **Download**.9. The connection test continues. Look for a success dialog like the following:   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/dbeaver-connection-test.png&#39;   alt-text=&#39;DBeaver connection test&#39;   descr-text=&#39;DBeaver connection test&#39;   screenshot=&#39;true&#39;   pxwidth=&#39;421&#39;   screenshot=&#39;true&#39;   %}   If you instead receive an error dialog, go back through your settings or try   different port numbers. Make sure you are using the exact connection   information provided by your network administrator.   {% include note.html      content=&quot;If you encounter an error stating &quot;Session properties cannot be      overridden once a transaction is active&quot;, enable **Auto-commit**. You can      find this setting on the **Edit connection** menu under **Connection      settings &gt; Initialization**.&quot;   %}12. After a successful test, click **Finish**. This places an entry for this    connection in the **Database Navigator** panel. Open this entry to connect    to the cluster and see a list of its catalogs.    {% include note.html content=&quot;You can select the new entry, right-click, and      select **Rename** to give this connection your preferred name.&quot; %}## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is also required touse [TLS/HTTPS](../../latest/security/tls.html). If you&#39;re using globallytrusted certificate best practices, use the cluster&#39;s HTTPS URL in theconnection string as shown in the steps above.If you&#39;re not using a globally trusted certificate, you may have to configurethe trust store on your client machine. Consult your site&#39;s networkadministrators for guidance.To use TLS, specify the JDBC parameter setting `SSL=true` as shown in the stepsabove. As an alternative, you can make the same setting appended to the JDBCconnection string in the form `?SSL=true`.## QueryingYou can click on the defined connection in the **Database Navigator** toconnect. The initial connection downloads all metadata about catalogs, schema,tables, columns and more. You can browse the information as it loads.{% include image.html  url=&#39;../../assets/img/general/dbeaver-navigator.png&#39;  alt-text=&#39;DBeaver database navigator&#39;  descr-text=&#39;DBeaver database navigator&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}Open **SQL Editor &gt; Open SQL script** (or press F3) to write and execute yourqueries and inspect the returned results.{% include image.html  url=&#39;../../assets/img/general/dbeaver-editor.png&#39;  alt-text=&#39;DBeaver SQL editor&#39;  descr-text=&#39;DBeaver SQL editor&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}"
 },
 {
  "title": "Deploying and updating Starburst Enterprise",
  "url": "/platform-administrator/deploy-update.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} can be deployed in several ways, depending on yourorganization&#39;s infrastructure. Most organizations choose to install, configureand deploy {{site.terms.sep}} on Kubernetes. Follow the link most appropriate toyour environment to learn more about deploying {{site.terms.sep}}:* [Directly](../latest/k8s/overview.html) into Google Cloud, or through  [Google Cloud]({{ site.baseurl }}/ecosystems/google/index.html) using Helm  charts* [Directly](../latest/k8s/overview.html) into OpenShift, or through  [Red Hat Marketplace](../ecosystems/redhat/index.html) using  our Kubernetes operator* On AWS [using our AMI](../latest/aws/overview.html) or [using  EKS](../latest/k8s/overview.html)* On [Microsoft Azure](../ecosystems/microsoft/index.html)See our [latest reference manual](../latest/index.html) forcomprehensive documentation on installing and configuring{{site.terms.sep_full}} in your environment."
 },
 {
  "title": "Disclaimers",
  "url": "/disclaimers.html",
  "content": "# {{ page.title }}The following information applies to this site hosted at[https://docs.starburst.io](https://docs.starburst.io).## Copyright© 2017 - 2021, Starburst Data, Inc. Starburst, and Starburst Data are registeredtrademarks of Starburst Data, Inc. All rights reserved.## PrivacyThe [privacy policy](https://www.starburstdata.com/privacy-policy/) of Starburstapplies to all content on and usage of the site.## TrademarksProduct names, other names, logos and other material used on this site areregistered trademarks of various entities including, but not limited to, thefollowing trademark owners and names:  Apache Software Foundation  Apache Hadoop, Apache Hive, Apache Kafka, and other names  Amazon  AWS, S3, Glue, EMR, and other names  Azul Systems Inc  Zulu  Docker Inc.  Docker  Google  Google Cloud, GKE, YouTube, and other names  Microsoft  Azure, AKS, and others  Oracle  Java, JVM, OpenJDK, and other terms  The Linux Foundation  Kubernetes, Helm, Presto, Linux, and other names  Starburst Data  Starburst, Starburst Data, Starburst Galaxy, and other names"
 },
 {
  "title": "Try Starburst Enterprise with Docker",
  "url": "/starburst-enterprise/try/docker.html",
  "content": "# {{ page.title}}You can install {{site.terms.sep_first}} in a Docker container and runit on any Docker host. This is especially useful as a test or demo environmentor to evaluate {{site.terms.sep}}.{% include note.html content=&quot;You can run SEP with Docker on Mac or Windows computers for evaluation purposes only. Understand that these are likely to be memory-limited devices that cannot run complex queries. Production implementation of a SEP cluster on Docker is supported only on Linux servers.&quot; %}This page presumes you have Docker installed and running, and presumes somefamiliarity with Docker commands. For Mac and Windows evaluations, make sure youhave Docker Desktop installed and running.With custom configuration of the containers for the {{site.terms.sep}}coordinator and worker nodes, you can run a set of Docker containers toimplement a Linux-hosted {{site.terms.sep}} cluster on your local network or ona cloud provider. However, this is **not recommended**. Instead, Starburstoffers a full [Kubernetes deployment option](./k8s.html), including Dockercontainers and Helm charts for {{site.terms.sep}}, Apache Ranger, and HiveMetastore Service.## Docker image featuresThe default {{site.terms.sep}} image has the following characteristics:* There is *no* {{site.terms.oss}} CLI command installed in the Docker image  itself. You can, of course, run the CLI on the host.* The `jvm.config` file is set to use 1G maximum.* The following catalogs are installed:  * jmx  * memory  * system  * tpcds  * tpch## Initial run of {{site.terms.sep}} on Docker{{site.terms.sb}} provides a Docker image on Docker Hub that contains a trialconfiguration of {{site.terms.sep}}. Use the following command to download andrun this image:```shelldocker run -d -p 8080:8080 --name sepdock starburstdata/starburst-enterprise:latest```If you know you need a particular release, substitute its number for `latest`.For example, `starburstdata/starburst-enterprise:354-e`. For release 350 andearlier, you must use the identifier `starburstdata/presto:350-e`.The `docker run` options have the following meanings:| Option   | Meaning| ---------|---------------------------------------|| `–d`     | Detach                                || `–p`     | Map ports as `hostport:containerport` || &amp;#8209;&amp;#8209;`name`&amp;nbsp;| Provide a name for the image to use in subsequent Docker commands |{% comment %}Preserve the non-breaking hyphens and forced space in the last rowabove to prevent the first column wrapping before &quot;name&quot;.{% endcomment %}To make sure the server starts in Docker, continually view the Docker logs untilyou see &quot;======== SERVER STARTED ========&quot;.```shelldocker logs sepdockdocker logs sepdock...```Use the `docker ps` command to verify that the your *sepdock* service isrunning. Run `docker ps -a` to locate a server that failed to start.### Verify the serverTo verify that your Docker-hosted {{site.terms.sep}} server is operating asexpected, run the Web UI as described in [Verify the server](./verify.html).### Run queriesTo run queries against your server, use the {{site.terms.oss}} CLI as describedin [CLI](../../data-consumer/clients/cli.html).## Add custom configurationThe {{site.terms.sb}}-provided Docker image gives you a certain set ofconfiguration files. But the whole point of running {{site.terms.sep_full}} isto query your own data sources. How do you provide your own configuration filesto the Docker-hosted server?{{site.terms.sb}} does *not* recommend going into the Docker image to changeconfiguration files there. Those changes would be lost the next time{{site.terms.sb}} updated the public Docker image to a new version, which yournext `docker run` command would automatically download and use.Instead, you can map the `etc` directory used by the {{site.terms.sep}} instancerunning in Docker to a local directory. Once configured, the Docker-hosted{{site.terms.sep}} uses the local `etc` directory as its primary source ofconfiguration files, and ignores the default settings in the Docker image.To do this requires one extra `docker run` option. If your Docker-hosted{{site.terms.sep}} is running now, first stop and remove it:```shelldocker stop sepdockdocker rm sepdock```### Populate the local etc directoryCreate a local directory to contain your custom {{site.terms.sep}} configurationfiles. For example:```shellmkdir -p /home (or /Users)//sepdock```Substitute your ** in this command.The simplest way to test your local `etc` directory is to use the set ofconfiguration files provided as examples for the O&#39;Reilly book [Trino: TheDefinitive Guide](https://www.starburst.io/info/oreilly-trino-guide/).Download these samples from their [GitHublocation](https://github.com/trinodb/trino-the-definitive-guide) either as azip file or a git clone.Unzip or clone the files into a local directory, such as `/home (or/Users)//bookfiles`. From there, copy the entire `etc` directory fromthe `single-installation` sample to your `/home (or /Users)//sepdock`directory. For example:```shellcd /home//bookfiles/single-installationrsync -av etc /home//sepdock/```or```shellcd /Users//bookfiles/single-installationrsync -av etc /Users//sepdock/```### Edit local configuration filesNavigate to your local `etc` directory. For example:```shellcd /home (or /Users)//sepdock/etc```Inspect the configuration files provided there for suitability with Docker. Forexample, the `jvm.config` file sets `-Xmx16G`, which might be too large forDocker running on Docker Desktop on Mac or Windows. Set that to a lower valuesuch as `-Xmx2G`.{% include note.html content=&quot;Make sure Docker is configured to reserve enoughmemory to handle the -Xmx setting you choose for SEP. For example, DockerDesktop&#39;s default RAM setting is 2G. Use Docker Desktop Preferences, Resourcestab to specify enough RAM to allow the Docker-hosted server to start and run.&quot;%}### Restart to use the local etc directoryThe `docker run` command has another useful option:| Option     | Meaning| -----------|---------|| `‑‑volume` | Specify a directory in the container to mount to a host directory |The syntax for the `‑‑volume` option is:`localPath:containerPath:options`Specify _localPath_ first, followed by _containerPath_, separated by a colon. No_options_ are needed. Options are described in [Dockerdocumentation](https://docs.docker.com/storage/volumes/#choose-the--v-or---mount-flag).If your Docker-hosted server is running, stop and remove it:```shelldocker stop sepdockdocker rm sepdock```Make sure you&#39;re still in the `/home (or /Users)//sepdock/etc`directory. Then rerun the Docker image, this time including a `‑‑volume` optionthat maps the current directory to the `etc` directory inside the Docker image,`/usr/lib/presto/etc`.```shellcd /home (or /Users/username/sepdock/etcdocker run -d -p 8080:8080 --volume $PWD:/usr/lib/presto/etc --name sepdock starburstdata/starburst-enterprise:latest```Continually view the Docker logs as before, waiting to see &quot;======== SERVERSTARTED ========&quot;:```shelldocker logs sepdock...docker logs sepdock...```Verify that the server is running by using the Web UI as described in [Verifythe server](./verify.html).To run queries, connect the {{site.terms.oss}}[CLI](../../data-consumer/clients/cli.html) to the Docker-hosted{{site.terms.sep}} instance. Run the `show catalogs;`, `show schemas;`, and`show tables;` commands to confirm the assets of the server.### Configure custom data sourcesThe local `etc` directory feature described in the previous section lets youwork locally to make changes to your Docker environment.Continue adding new catalogs and configuring features that customize connectionsto the data sources you need {{site.terms.sep}} to see.## Next stepsYou need a cluster of servers to use {{site.terms.sep}} in production or evenfor better testing. Do not attempt this with the Docker container only. Insteaduse Kubernetes and the [Starburst Kubernetes deploymentsupport]({../../latest/k8s.html), including Docker containers and Helm chartsfor {{site.terms.sep}}, Apache Ranger, Hive Metastore Service, and our [cacheservice](../../latest/admin/cache-service.html)."
 },
 {
  "title": "EKS cluster creation",
  "url": "/ecosystems/amazon/eks-cluster-creation.html",
  "content": "# {{page.title}}AWS requires EKS clusters to have a minimum of two availability zones(AZs), but {{site.terms.sep_first}} best practices requires that the coordinatorand workers reside in a single AZ, in a single subnet.This document covers how to configure your cluster to ensure that all{{site.terms.sep}} resources are co-located and follow best practices.## PrerequisitesThe following tools and policies are required to create an {{site.terms.sep}}cluster in EKS:* `kubectl`* `eksctl` version 0.54.0 or later* IAM Polices for Glue, S3, as desired## Create your `sep_eks_cluster.yaml` fileYour YAML file should start with the following two lines:```yamlapiVersion: eksctl.io/v1alpha5kind: ClusterConfig```Next, add the `metadata:` section to describe your cluster. The followingexample shows the minimum required fields, as well as suggested tags for astaging cluster running in `us-east-2`:```yamlmetadata:  name: my-sep-cluster  region: us-east-2  version: &quot;1.20&quot;  tags:    cloud: aws    environment: staging    info: &quot;EKS cluster for Starburst Enterprise staging environment&quot;    user: your.username```## Specify your networkingAdd the two required AZs, and the existing subnets associated with them:```yamlvpc:  subnets:     private:       us-east-2a:         id: subnet-0Subnet1ID8String2       us-east-2b:         id: subnet-0Subnet0ID2String3```For purposes of this example, we set up {{site.terms.sep}} in the `us-east-2a`AZ.{% include note.html content=&quot;Review the [EKS networkingguide](./eks-networking.html) to ensure you follow best practices forperformance and cost controls.&quot; %}## Create your EKS `managedNodeGroups:`{{site.terms.sb}} recommends using `managedNodeGroups:` to create the pools ofinstances available to {{site.terms.sep}} and its associated services.`managedNodeGroups:` in EKS have the additional benefit of automating SIGTERMdelivery to {{site.terms.sep}} workers and the coordinator when a Spot instanceis removed to enable graceful shutdown. With `nodeGroups:` additionaldevelopment must be done, outside of {{site.terms.sep}}, to allow for gracefulshutdown.{% include caution.html content=&quot;Sudden removal of an expected resource cancause SEP to fail. You must carefully consider [whether to useSpot instances](./eks-networking.html#spot-instances-and-single-azs).&quot; %}In this example, a managed node group called `sep_support_services` is createdalongside the `sep` managed node group. The support services managed node groupruns HMS and Ranger, if those services are required in your environment.The first three IAM policy ARNs, shown below for both groups, are required byEKS. The IAM policy ARN for Glue illustrates using an additional policy to allowaccess to EKS, S3 and Glue in the account used for the EKS clusterwithout supplying credentials. It is not required.{% include note.html content=&quot;You must repeat all tags specified in themetadata tags in both entries in `managedNodeGroups:`.&quot; %}```yamlmanagedNodeGroups:  - name: sep    tags:      cloud: aws      environment: staging      info: &quot;EKS cluster for Starburst Enterprise staging environment&quot;      user: your.username    availabilityZones: [us-east-2a]    labels:      allow-workers: workers    instanceTypes: [&quot;m5.xlarge&quot;, &quot;m5a.xlarge&quot;, &quot;m5ad.xlarge&quot;]    desiredCapacity: 2    minSize: 2    maxSize: 4    privateNetworking: true    ssh:      allow: true      publicKeyName: en-field-key    iam:      attachPolicyARNs:        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy        - arn:aws:iam::47ID9String78:policy/EKS-S3-Glue  - name: sep_support_services    tags:      cloud: aws      environment: staging      info: &quot;EKS cluster for Starburst Enterprise staging environment&quot;      user: your.username    availabilityZones: [us-east-2b]    spot: true    instanceTypes: [&quot;m5.large&quot;, &quot;m5a.large&quot;, &quot;m5ad.large&quot;]    desiredCapacity: 1    minSize: 1    maxSize: 1    privateNetworking: true    ssh:      allow: true      publicKeyName: en-field-key    iam:      attachPolicyARNs:        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy        - arn:aws:iam::47ID9String78:policy/EKS-S3-Glue```## Save your file and create your clusterWhen you are finished adding the required content to your `sep_eks_cluster.yaml`file, save it and use it to create your cluster with `eksctl`:```shell$ eksctl create cluster -f sep_eks_cluster.yaml```When the command completes successfully, the following message appears inyour terminal:```shell2021-07-14 14:28:56 [✓] EKS cluster &quot;my-sep-cluster&quot; in &quot;us-east-2&quot; region is ready```{% include note.html content=&quot;If you get an error about not being able to deployKubernetes 1.20, ensure you are running `eksctl` version 0.54.0 or later.&quot; %}"
 },
 {
  "title": "AWS EKS networking best practices",
  "url": "/ecosystems/amazon/eks-networking.html",
  "content": "# {{page.title}}When deploying {{site.terms.sep_first}} into an AWS EKS cluster, it is importantto ensure that your cluster resources are located with the following in mind:* cluster communications latency* data ingress/egress costs* IP address availability## Use a single availability zone inside your existing VPCUsing your existing VPC is a key cost control measure. It ensures that costsassociated with data ingress and egress are kept to a minimum. Every new VPCcomes with a NAT gateway. And, while NAT gateways are inexpensive, costs fordata transferred through that gateway add up very quickly. As a best practice,placing your {{site.terms.sep}} inside of your existing VPC, co-resident with asmany of your data sources as possible not only keeps costs down, it also greatlysimplify networking and security.Equally as important is performance. No matter if you use an existing or newVPC, {{site.terms.sep}} must run in a single availability zone (AZ) to ensurethe best performance possible. To accomplish this, use node groups. The nodegroups are then tied to a single AZ using affinity or node selection rules.For {{site.terms.sep}}, two [managed node groups](./eks-cluster-creation.html)are required. The {{site.terms.sep}} coordinator and workers are deployed toone group while support services, such as HMS, Ranger and nginx, are deployed tothe second node group.{% include warning.html content=&quot;You must use an existing VPC in order toenforce the use of a single AZ. Do not use eksctl to create a new VPC at clustercreation time.&quot; %}## Spot instances and single AZsAWS EC2 [Spotinstances](https://docs.aws.amazon.com/es_es/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html)are sometimes desireable to keep costs down. However, they are inherentlyunreliable, as they can be recalled by the EC2 platform at any time. Because ofhow {{site.terms.sep}} drives performance by dividing query processing for anysingle query, sudden removal of an expected resource can cause failures.Further, when using a single AZ, Spot instance rebalancing is diminished, aspools in only the single AZ are available to it. Using Spot instances may stillbe desirable in some cases. In this case, to use Spot instances for the{{site.terms.sep}} workers, create a third node group to contain them,separating the non-Spot coordinator node group from the Spot worker node group.The remaining node group contains support services as before.## IP address requirementsAn important consideration in using an existing VPC is IP address availability.As part of standing up your {{site.terms.sep}} cluster, you must ensure thatsufficient IP addresses are reliably available for use by your{{site.terms.sep}} instances.In EKS clusters, AWS creates one subnet per availability zone. Usually, theseare configured as /20 Classless Inter-Domain Routings (CIDR) with 4,091 IPaddresses available for use (an additional 5 are reserved by the clusteritself). {{site.terms.sep}} requires that all hosts, both workers andcoordinators, are sized identically. Each of these instances has a maximumnumber of IP addresses that can be assigned to it, and EKS reserves twice thatnumber of addresses for it.For purposes of this discussion, we assume an {{site.terms.sep}} deploymentinvolving six m5.xlarge workers and one m5.xlarge coordinator. Each of thisinstances can have a maximum of 15 IP addresses in use, one per each of the 15 interfaces it comes with. An additional 15 are reserved, for a total of 30 IP addresses needed per instance. Those seven instances together then require 210 available IP addresses:```shell  ( 7 m5.xlarge instances)x ( 15 interfaces per instance )x ( 2 IPs per interface )= 210 IP addresses needed```In this example, you must ensure that a minimum of 210 IP addresses arereliably available for use by your {{site.terms.sep}} instances at all times.{% include note.html content=&quot;If you are taking advantage of AWS EKSautoscaling, you must calculate the number of addresses needed on the maximumallowable number of workers.&quot; %}### Using subnetsYou can use an existing subnet or create a new one. It must be configured with aroute out to the Internet either via a NAT gateway or an IGW to allow your EKScluster to communicate with the AWS EKS management backplane. Costconsiderations for these communications are minimal.### Considerations if you must use VPC peeringIf you cannot place {{site.terms.sep}} within your current VPC because of ascarcity of IP addresses, as an alternative you can create a peering connectionwith the new EKS cluster’s VPC to avoid the often cost-prohibitive operation ofall data passing through the NAT gateway. VPC peering requires additional setup,and comes with potential downsides:* Does not scale well.* Transitive routing is not available.* Peering connections are a resource that must be managed.* Firewall rules must be carefully managed.Additionally, you must ensure that the CIDR you set for the new{{site.terms.sep}} VPC does not match or overlap with your existing VPC’s CIDR.VPCs with overlapping CIDRs cannot create peering connections."
 },
 {
  "title": "Google Kubernetes Engine deployments",
  "url": "/ecosystems/google/gke.html",
  "content": "# {{page.title}}The [Google KubernetesEngine](https://cloud.google.com/kubernetes-engine) (GKE), is certified to workwith {{site.terms.sep_first}}.## IntroductionThis user guide provides information on using {{site.terms.sb}} Helm charts todeploy {{site.terms.sep}}, and eventually a cluster in Google Cloud - KubernetesApplications. More information is available in our [Kubernetes referencedocumentation](../../latest/k8s/overview.html), which includes acustomization guide, examples and tips.Before you get started installing {{site.terms.sep}}, we suggest that you readour [customization guide](../../latest/k8s/sep-configuration.html) to learnhow to customize {{site.terms.sep}} clusters.We also have a helpful [installationchecklist](../../latest/k8s/installation.html) for an overview of the generalHelm-based installation and upgrade process.We recommend that you use Google Cloud&#39;s marketplace UI with **Click to Deployon GKE** to deploy. The command line **Deploy via command line** installation isnot supported.## Getting startedSetting up and deploying {{site.terms.sep}} to a Google Kubernetes Enginecluster using Google Cloud is pretty straightforward.You can get started by simply following the UI instructions in the[marketplace](https://console.cloud.google.com/marketplace/details/starburst-public/starburst-presto).### Cluster requirements{{site.terms.sep}} can be deployed to an existing GKE cluster, or to a newlycreated one. If using an existing GKE cluster, make sure it is configured withthe appropriate number and type of instances needed to deploy an{{site.terms.sep}} cluster. Review the {{site.terms.sep}}[requirements](../../latest/k8s/requirements.html) to ensure that you havesufficient resources. With insufficient resources available in the cluster, thedeployed pods may fail to be scheduled.You can also choose to deploy a new cluster with minimal resources and defaultconfiguration from the marketplace offering configuration page. After thedefault {{site.terms.sep}} chart deploys successfully in this cluster, you mustreview the cluster configuration before updating it and making any adjustmentsto ensure that the cluster is sufficiently large.### DeployingThe default values of the {{site.terms.sep}} configuration [in theUI](https://console.cloud.google.com/marketplace/details/starburst-public/starburst-presto)work for most of the cases.{% include warning.html content=&quot;If you are enabling global access control using Ranger, make sure you note downthe values of the admin and service user passwords you provide in the UI forlater use.&quot; %}Once done, you can click on **Deploy** to install {{site.terms.sep}} in thecluster. You can track the progress of the deployment in the [applicationspage](https://console.cloud.google.com/kubernetes/application).Once the application is successfully deployed, {{site.terms.sep}} is deployedusing the Helm charts. If you enable either or both of[Ranger](../../latest/k8s/ranger-configuration) and [Hive MetastoreService](../../latest/k8s/hms-configuration) these are also deployed. Thedeployment uses the values you provide in the UI and some default values whendeployed the first time. If you want to upgrade or change the deploymentconfiguration, you can either use the **Application info** from the [applicationpage](https://console.cloud.google.com/kubernetes/application) or the following{{site.terms.sep}} upgrade instructions.## Upgrading {{site.terms.sep}}You can upgrade {{site.terms.sep}} using the [cloudshell](https://cloud.google.com/kubernetes-engine/docs/quickstart#choosing_a_shell).You need to download the Helm charts and extract them to modify thedeployment, as shown in the following:```shellexport TAG=2.0.1export APP_INSTANCE_NAME=wget https://console.cloud.google.com/storage/browser/starburst-enterprise-presto/helmCharts/presto-gcp/starburst-enterprise-presto-charts-$TAG.tgz;tar -zxvf starburst-enterprise-presto-charts-$TAG.tgz;```Now update the ``values.yaml`` in the umbrella chart folder(``starburst-enterprise-presto-charts-$TAG``) before you apply the manifestfile. Alternatively, you can use ``--set`` parameter in ``helm template``command to update the chart configuration. The values set in ``--set`` takesprecedence over the ones set in ``values.yaml``.Once you have the manifest created, apply (``kubectl apply``) the file to updatethe deployment.More details about the {{site.terms.sep}} configuration are found in the [reference documentation](../../latest/k8s/sep-configuration).Make sure you have [usagemetrics](../../latest/k8s/sep-configuration.html?highlight=usage%20metrics#usage-metrics)enabled in your {{site.terms.sep}} configuration. Our Google Cloud offeringrelies on the metrics and if disabled, the deployment is terminated after 36hours.The minimum set of parameters required for the upgrade are shown below:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set [PUT OTHER CHART PARAMETERS YOU WANT TO UPDATE] &gt; presto_manifest.yaml```The following example shows how to update the number of {{site.terms.sep}} workers:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set worker.count &gt; presto_manifest.yaml```Configure ``kubectl`` to point at your cluster and then apply the configuration:```shellkubectl apply -f presto_manifest.yaml```## Enabling global access control with Apache RangerYou can deploy or upgrade Ranger for [global accesscontrol](../../latest/security/global-ranger) by creating a manifest file withRanger parameters set.You can either update the ``values.yaml`` in the umbrella chart folder``starburst-enterprise-presto-charts-$TAG`` for the configuration(``starburst-ranger``) prior to generating the manifest file or or use ``--set``parameter while creating the manifest file. The ``--set`` parameters must beprefixed by ``starburst-ranger``.The complete reference of the chart configuration can be found in the[Ranger chart documentation](../../latest/k8s/ranger-configuration.html).Example with Ranger enabled:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set starburst-ranger.enabled=true   --set [PUT OTHER CHART PARAMETERS YOU WANT TO UPDATE] &gt; presto_manifest.yaml```## Enabling the Hive Metastore ServiceSimilar to Ranger, to deploy or upgrade the Hive Metastore Service (HMS)provided by {{site.terms.sb}}, generate a manifest file with Hive chartparameters set. You can either use ``values.yaml`` (for ``starburst-hive``) oruse the ``--set`` parameters. For Hive charts, the ``--set`` parameters must beprefixed by ``starburst-hive``.The complete reference of the chart configuration for the metastore can be foundin the [HMS documentation](../../latest/k8s/hms-configuration.html).Example with HMS enabled:```shellhelm template starburst-enterprise-presto-charts-$TAG/   --name-template=&quot;$APP_INSTANCE_NAME&quot;   --set deployerHelm.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/deployer:$TAG&quot;   --set reportingSecret=&quot;$APP_INSTANCE_NAME-reporting-secret&quot;   --set image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto&quot;   --set image.tag=&quot;$TAG&quot;   --set initImage.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/presto-init&quot;   --set initImage.tag=&quot;$TAG&quot;   --set metricsReporter.image=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/metrics_reporter:$TAG&quot;   --set imageUbbagent=&quot;gcr.io/cloud-marketplace-tools/metering/ubbagent:latest&quot;   --set starburst-hive.image.repository=&quot;gcr.io/cloud-marketplace/starburst-public/starburst-presto/hive&quot;   --set starburst-hive.image.tag=&quot;$TAG&quot;   --set starburst-hive.enabled=true &gt; presto_manifest.yaml```Configure ``kubectl`` to point at your cluster and then apply the configuration:```shellkubectl apply -f presto_manifest.yaml```## Deleting a deploymentTo delete a GKE deployment, run the following command:```shellkubectl delete -f presto_manifest.yaml```"
 },
 {
  "title": "Glossary",
  "url": "/glossary.html",
  "content": "# {{ page.title }}##### Terms A-E###### Amazon AWS marketplaceA provider for all aspects of the required infrastructure. This includes usingAWS CloudFormation for provisioning, Amazon Simple Storage Service (S3) forstorage, Amazon Machine Images (AMI), and Amazon Elastic Compute Cloud (EC2) forcomputes, Amazon Glue as metadata catalog, and others. For more information, see[Amazon AWS Marketplace](./ecosystems/amazon/index.html).###### Bare-metal serverA physical computer server dedicated to a single tenant. See[bare-metal server](https://en.wikipedia.org/wiki/Bare-metal_server)###### CatalogCatalogs define and name the configuration to connect to, and query a datasource. For more information, see[Catalogs](./data-engineer/catalogs.html).###### ClusterA cluster provides the resources to run queries against numerous data sources.Clusters define the number of workers, the configuration for the JVM runtime,configured data sources, and others aspects. For more information, see [clusterbasics](./platform-administrator/cluster.html).###### ConnectorTransforms the underlying data into the {{site.terms.sep}} concepts of schemas,tables, columns, rows, and data types. A[connector](./data-engineer/connectors.html) is specific to the data source itsupports, and are named as properties in [catalogs](#catalog).###### ContainerA lightweight virtual package of software that contains libraries, binaries,code, configuration files, and other dependencies needed to deploy anapplication. A running container does not include an operating system. It usesthe operating system of the host machine, typically Linux. Learn more at[Container concept](https://kubernetes.io/docs/concepts/containers/) in theKubernetes documentation.###### COTSCommon off-the-shelf. Refers to commodity hardware components.###### Data consumer personaOwns data products such as reports, dashboards, models, and the quality ofanalysis. For more information, see [Starburstpersonas](./get-started/starburst-personas.html#data-consumer).###### Data engineer personaOwns schemas and is responsible for the source data quality and ETL SLA. Formore information, see [Starburst personas](./get-started/starburst-personas.html#data-engineer)###### Data sourceA data source is a system from which data is retrieved. In {{site.terms.sep}},you must connect to a data source so you can query that source by using a[catalog](#catalog). See [Configure and definecatalogs](./data-engineer/catalogs.html)###### External ID (AWS)An external ID is an identifier in AWS that is required for using{{site.terms.galaxy_full}}. It is used to ensure that only trusted AWS accountsare given permission to operate the {{site.terms.galaxy}} clusters based ontheir assigned role and trust policy. For more information on AWS Identity andAccess Management, see [How to use an external ID when granting access to yourAWS resources to a thirdparty](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html).##### Terms F-J###### Google Cloud marketplaceDeploy in the Google Cloud Marketplace or using the Starburst Kubernetessolution on the Google Kubernetes Engine (GKE). GKE is a secure, productionready, managed Kubernetes service in Google Cloud managing for containerizedapplications. For more information, see [Google Cloudmarketplace](./ecosystems/google/index.html).##### Terms K-O###### MarketplacePurchase a preconfigured set of machine images, containers, and other neededresources to run {{site.terms.sep}} on their cloud hosts under your control. See[Marketplace deployments](./ecosystems/index.html).###### Microsoft Azure marketplaceDeploy using in the Azure Marketplace or using the Starburst Kubernetes solutiononto the Azure Kubernetes Services (AKS). AKS is a secure, production-ready,managed Kubernetes service on Azure for managing for containerized applications.For more information, see [Microsoft AzureMarketplace](./ecosystems/microsoft/index.html).##### Terms P-T###### Platform administrator personaOwns platforms and services (ITIL-style). Has service SLA responsibility for theinfrastructure supporting the cluster. For more information, see [Starburstpersonas](./get-started/starburst-personas.html#platform-administrator).###### Presto and {{site.terms.fka}}Old name for Trino.###### Red Hat OpenShift marketplaceA container platform using Kubernetes operators that automates the provisioning,management and scaling of applications to any cloud platform or even on-prem.Starburst Enterprise is available on Red Hat marketplace as of OpenShift version 4.For more information, see [Red HatMarketplace](./ecosystems/redhat/index.html).###### Starburst Enterprise platformHelps companies harness the value of open source {{site.terms.oss}}, the fastestdistributed query engine available today. Starburst adds connectors, security,and support that meets the needs for fast data access at scale. For moreinformation, see [Starburst Enterprise](./starburst-enterprise/index.html).###### SEPAbbreviation of Starburst Enterprise platform. For more information, see[Starburst Enterprise](./starburst-enterprise/index.html).###### SQLStructured Query Language. The standard language used with relational databases.For more information, see [SQL](./data-consumer/starburst-sql.html).###### {{site.terms.oss}}Fast distributed SQL query engine for big data analytics, formerly{{site.terms.fka}}.###### Virtual machine (VM)An emulation of the hardware of a computer system on a physical host machine, soany operating system suitable for that hardware can run in the emulator. Atypical example is a Linux virtual machine running on a Windows-based hostmachine. See [virtual machine](https://en.wikipedia.org/wiki/Virtual_machine)."
 },
 {
  "title": "AWS Glue",
  "url": "/ecosystems/amazon/glue.html",
  "content": "# {{page.title}}[AWS Glue](https://aws.amazon.com/glue/) data catalogs are a supported metadatacatalog for {{site.terms.sep_first}}, and can be used as an alternative to theHive Metastore to query your [S3 data](./s3.html) with the following connectors:* [Hive connector](../../latest/connector/starburst-hive.html)* [Delta Lake](../../latest/connector/starburst-delta-lake.html)* [Iceberg](../../latest/connector/iceberg.html)Ensure the requirements for the connector are fulfilled.## RequirementsBefore you configure the Glue metastore, verify the following prerequisites:* Your {{site.terms.sep}} instance must have permissions to access both S3 and  Glue AWS services.* For CFT deployments, review the  [IAM role permissions requirements](../../latest/aws/requirements.html)  if you are providing your own IAM Instance.* When using the AMI and launching it manually, make sure you choose an IAM Role  that satisfies the requirements.## Configuration1. Configure to use Glue as metastore in the catalog properties file  ```text  connector.name=hive  hive.metastore=glue  ```2. Add [other desired Glue properties](../../latest/connector/hive.html#aws-glue-catalog-configuration-properties)   such as the AWS region or credentials to use.3. Restart the cluster to apply the changes.## AWS Glue with {{site.terms.sep}} AMIYou can use the {{site.terms.sep}} AMI from the AWS Marketplace, with the Hiveconnector to use Glue.After the configuration as described in the preceding section, you can restartthe AMI:```textsudo service starburst restart```## AWS Glue with CloudFormation templateWhen using the CloudFormation template in AWS, you can leverage Glue bynavigating to the **Stack Creation** form and choosing **AWS Glue Data Catalog**in the **MetastoreType** field in the **Starburst Enterprise Configuration**section.## {{site.terms.sep}} with AWS Glue usageWhen configured, the Glue data catalog is available via the catalog from withinthe CLI or any other {{site.terms.sep}} connection. You must specify thelocation of the data on S3 for either the entire schema or at the table level.For example, to create a schema ``myschema`` in the Glue data catalog, with theS3 base directory (root folder for per-table subdirectories) pointing to theroot of ``my-bucket`` S3 bucket, run the following SQL command:```sql    CREATE SCHEMA mycatalog.myschema    WITH (location = &#39;s3://my-bucket/&#39;)```You can also create and edit the schema and tables directly from Glue. InGlue terminology, a schema is referred to as a “database”.## Table and column statistics support{{site.terms.sep}} supports standard AWS Glue table and column statistics viathe [AWS GlueAPI](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-tables.html#aws-glue-api-catalog-tables-GetColumnStatisticsForTable).You can create and manage the statistics with the[``ANALYZE``](../../latest/sql/analyze.html) statement.### Legacy statistics{{site.terms.sep}} releases prior to 354-e use custom statistics. These{{site.terms.sep}}-based statistics are no longer supported in 354-e and laterreleases, and are replaced by the standard Glue statistics.{% include note.html content=&quot;The``hive.metastore.glue.column-statistics.enabled`` configuration property isdeprecated.&quot; %}You can enable read-only access to the {{site.terms.sep}}-based table statisticsfor transition purposes. Set the property``hive.metastore.glue.read-properties-based-column-statistics`` to ``true``during the migration time, until the standard statistics are available. Thelegacy statistics are only used if standard statistics are not present for atable or a partition.{{site.terms.sep}}-based statistics are still stored in JSON format as Gluetable and partition parameters. After the migration these can be deleted.## Known limitations of AWS Glue supportThe following {{site.terms.sep}} features are not supported with the Glue datacatalog:* Statistics are not preserved when a column is renamed. Tables with renamed  columns must be re-analyzed.* Renaming tables from within AWS Glue is not supported.* Partition values containing quotes and apostrophes are not supported (for  example, ``PARTITION (owner=&quot;Doe&#39;s&quot;``).* Using [Hive authorization](../../latest/connector/hive-security.html) is not  supported."
 },
 {
  "title": "Create an IAM user and attach a policy",
  "url": "/starburst-galaxy/data-sources/iam-policies-users.html",
  "content": "# {{ page.title }}Create an IAM user and attach a policy to allow it to access specificusers—identities in your AWS account. For example, your custom IAM policy couldinclude full or read only access to S3 resources.1. Navigate to your [IAM service](https://console.aws.amazon.com/iam/home). You   may need to sign in.2. Select **Users** from the left side menu. 3. Click **Add user**.4. Enter a `starburst-galaxy-` prefixed user name, select **Programmatic access**,    and click **Next: Permissions**5. Select **Attach existing policies directly**.6. Filter and select the name of the policy or policies using the check box.7. Click **Next: Tags** and, optionally, add tags.8. Click **Create user**.Copy your newly created **Access key ID** and **Secret access key**.* The **Secret access key** is the value in the plain text field when you create  a secret.* The **Access key ID** is used during {{site.terms.galaxy_full}} data source  creation## Next step* [Create a secret in AWS](../data-sources/secrets.html)"
 },
 {
  "title": "Image usage",
  "url": "/internal/images.html",
  "content": "## ImagesDisplay images with:* Our own [include image syntax](#include-image-syntax)* [HTML syntax](#html-syntax)* [Markdown syntax](#markdown-syntax)### include image syntaxThis is our in-house and preferred image syntax.This syntax provides a `screenshot` attribute that places a single pixel borderaround the image, which is best used for screenshots that have white space up totheir edges, so that the image is clearly delineated from the page&#39;s text.This syntax also allows you to specify valid CSS to tailor a particular image.If no `pxwidth` value is passed, the image defaults to the file&#39;s inherent widthup to a maximum of 90% of the current container for very large images.Another option of the `include image` syntax is to turn very large images intotwo-size modal images. The specified image is reduced for normal display on thepage, limited to `pxwidth` size, but the image is also now a button. Whenclicked, the image expands and is shown in a larger modal window. The`screenshot` and `modal` options are mutually exclusive; use one or the other.Use `include image.html` in a separate tag block with the parameters shown inthe following table:* Parameters may be used in any order.| Key         | Value | Required | Notes || ----------- | ----------- | ----------- | ----------- || url         | Relative path to the asset.  | **Required** || alt‑text    | String for alt tag content. | **Required** || descr       | String for description tag content for accessibility support. | *Optional* | Use if `alt‑text` does not convey enough information || pxwidth     | Width value in pixels, integer only. See the notes below | *Optional* || screenshot  | Set to &#39;true&#39; adds 1px solid border. No effect if modal. | *Optional* || modal       | Set to &#39;true&#39; adds modal ability. | *Optional* | Must assign `img‑id` if true || img‑id      | Adds ID string. | *Optional* | **Required for modal.** Must be unique per page. || style       | Pass any valid CSS to the image. | *Optional* |For the `pxwidth` key:* Don&#39;t include a `pxwidth` line, or leave its value empty to specify using the  native image resolution by default. ``pxwidth=&#39;&#39;``* Do not specify ``pxwidth=&#39;0&#39;``, which disables the image.* For modal, `pxwidth` specifies the non-modal display size on the page before  expansion.#### Using within listsWhen using the `include image` within a list, you **must** prefix the `includeimage` tag block with a `&amp;nbsp;` and the proper indentation in order to ensurecorrect alignment. If no `&amp;nbsp;` is used, the list items after the `includeimage` will start over at `1`.```{%raw%}1. The first item in the list  &amp;nbsp; {% include image.html ... %}2. The second item in the list  1. The first item in a nested list    &amp;nbsp; {% include image.html ... %}  2. The second item in a nested list3. The third item in a list{%endraw%}```#### Copy these imagesFeel free to copy the include tag blocks that display the following images.Paste these as templates into your working documents, then edit as appropriate:```{%raw%}{% include image.html  url=&#39;../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}{%endraw%}```{% include image.html  url=&#39;../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}```{%raw%}{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;Starburst cluster overview&#39;  descr-text=&#39;Image depicting Starburst architecture&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}{%endraw%}```{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;Starburst cluster overview&#39;  descr-text=&#39;Image depicting Starburst architecture&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}### HTML syntaxThis syntax allows you to add further tweaks such as sizing. If you copy thisexample, remember to surround `site.baseurl` with double open and close brace{ } characters.``````### Markdown syntaxOnly for very simple image cases, use Markdown syntax, which is:`![title](../assets/img/image.png)`. Use a relative path to the image from thecurrent location, or use the `site.baseurl` substitution token. (If you copythe following example, remember to surround `site.baseurl` with double open and closebrace { } characters.)```![Starburst](site.baseurl/assets/img/logo/starburst.png)```![Starburst]({{ site.baseurl}}/assets/img/logo/starburst.png)## LogosThe following conditions should be met when optimizing and saving brand logos touse on the site:| Property      | Value || ----------- | ----------- || Bounding | Trim/Cropped || Width      | 400px || Height | Auto (Constrained proportions) || Background   | Transparent        || Format | PNG || Example |  ![dbeaver](../assets/img/logo/dbeaver.png) |When adding a brand logo to the beginning of a page, for example the[Red Hat](../marketplace/redhat/index.html) marketplace page, the initialcontent section should contain the logo using the following HTML structure:```{% raw %}                {% endraw %}```Similarly, the `image.html` `include` may also be used by changing the HTMLsnippet above to:```{% raw %}              {% include image.html      url=&#39;../assets/img/logo/dbeaver.png&#39;      alt-text=&#39;DBeaver Logo&#39;    %}  {% endraw %}```This HTML structure ensures that the logo will be placed to the right of thecontent, creating a two column layout that allows the text to not produce astaggered wrapping effect. Once a certain breakpoint has been reached the logowill then move to above the text content.All content after the above HTML snippet will take up the full width of thecontainer.## IconsYou can and should use fontawsome icons.There are [thousands available](https://fontawesome.com/icons).Syntax is ``.You can change size, color and more.This is a user icon  in a paragraph.Also be careful, we use fontawesome 5 and therefore some icons are in adifferent name space.* `` for * `` for * `` for * `` for Coloring can be done with CSS or an embedded style.* `` for ## Persona based icons          Platform administrator            Data consumer            Data engineer  ## Topic based icons          Admin            Catalog            Clients            Clusters            Connectors            Data sources            Deploying            Migration            Performance tuning            Query performance  "
 },
 {
  "title": "Starburst Galaxy",
  "url": "/starburst-galaxy/index.html",
  "content": "                    {{page.title}}                              Learn more                                Request access                                          All the power of        {{site.terms.sep_full}}        and much more, running in the cloud, managed for you by {{site.terms.sb}}.                                                            Get started                                                                                Data sources                                                                                Clusters                                                                                Notebooks                                                  Learn about querying and using {{site.terms.galaxy_full}} with your BI,        reporting, or SQL tool of choice.                  Data consumer resources                    "
 },
 {
  "title": "Cluster overview",
  "url": "/starburst-galaxy/clusters/index.html",
  "content": "# {{ page.title }}A cluster in {{site.terms.galaxy_full}} provides the resources to run queriesagainst numerous [data sources](../data-sources/index.html). Clusters define thenumber of workers, the configuration for the JVM runtime, configured datasources, and other aspects.The {{site.terms.galaxy_full}} allows you to create, edit, and delete clustersfrom the interface. Access your clusters at any time by clicking **Clusters** onthe left hand menu.## Cluster profiles examples{{site.terms.galaxy_full}} provides four cluster options to allow you to [createa cluster](./create.html) that is right for your purposes. When you select a **Cluster profile** while creating a new cluster, you&#39;reselecting from a range of profile types that adjust the cluster to theanticipated workload. Review the cluster profile differences:* **Cost optimized**: Balance your workloads with total cost. Ideal for  performing the most work for the price.* **Low Memory**: Ideal for compute bound applications that benefit from high  performance processors.* **General purpose**: Balance your compute, memory, and networking resources.  Suitable for a variety of diverse workloads.* **High memory**: Deliver fast performance for workloads that process large  data sets in memory."
 },
 {
  "title": "Get started",
  "url": "/starburst-galaxy/get-started/index.html",
  "content": "# {{ page.title }}Sign up for {{site.terms.galaxy_full}} on [starburst.io](https://www.starburst.io/platform/starburst-galaxy/) orthrough [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-yntfb2ims6c4i?ref_=srh_res_product_title).## Free trialAll customers start with a 14 day free trial with unlimited functionality.If the free trial expires and you don&#39;t opt into a paid subscription, youraccount is moved to the free tier.## Free tierThe free tier allows only one running cluster with limited auto-scaling.Clusters created during your free trial or previous subscription are no longeravailable for you to start or edit.## Requirements{{site.terms.galaxy_full}} requires that you have and use an AWS account to runyour clusters. AWS bills separately from {{site.terms.galaxy_full}}.### AWSUsing {{site.terms.galaxy_full}} requires you to [configure your AWSaccount access for running the clusters](./aws-create-role.html)."
 },
 {
  "title": "Hitchhiker&#39;s Guide to editing Starburst content",
  "url": "/internal/index.html",
  "content": "# {{ page.title }}* [Markdown guide](./markdown.html) with all sorts of examples and usage* Our documentation [iconography](./icons.html) guide* [Personas we write for](./personas.html)* [Metadata tags for page front matter](./tag-list.html)## BreadcrumbsThe site currently uses a simple path based breadcrumb system implemented in`breadcrumbs-simple.html`.The [breadcrumbs system](https://www.seanh.cc/2020/01/01/jekyll-breadcrumbs/)from Sean Hammond is also included as inspiration for future improvements ormigration or a merge of the two systems. It does not work for plain pages andpaths but does work nicely for posts and collections."
 },
 {
  "title": "Notebook manager",
  "url": "/starburst-galaxy/notebook-manager/index.html",
  "content": "# {{ page.title }}The {{site.terms.galaxy_first}} notebook manager enables query management,visualizations, data source exploration, and querying your connected datasources.You can access a notebook manager for each cluster in the {{site.terms.galaxy}}user interface.SQL queries are stored in notes. One or more notes are managed as a collection -a notebook. Notebooks are stored in folders.Each note contains a SQL query statement. The notes can be used to actuallyexecute queries and inspect the results in table format, or createvisualizations.Here&#39;s a quick overview of the available feature helps you accomplish:* **Query management**: Create and organize your folders, notebooks and notes  for easy access and sharing.* **Visualizations**: Create diagrams and charts with the data from your data  sources, including bar charts, time series plots, and others.* **DB exploration**: Browse all connected data sources, schemas, tables, and  the contained data.* **Search**: Search the notes of all users.&gt; Important: The notebook manager displays in a new tab in your browser&gt; {{site.terms.galaxy_full}}There are three main sections of the notebook manager:* **My Notebooks**: A list of all of your notebooks.* **Favorites**: A list of your saved notebooks for quick access. Select the   icon to favorite a notebook.* **History**: The query logs. The history shows all of your queries whether you  saved them in a note or not."
 },
 {
  "title": "Manage users",
  "url": "/starburst-galaxy/admin/index.html",
  "content": "# {{ page.title }}Users gain access to {{site.terms.galaxy_full}} by invitation from{{site.terms.galaxy}} admins. Invitations specify the role of the new user:* **User**: Write SQL queries and run them in the notebook manager or other  data analytic tools. Users are often called  [data consumers](../../data-consumer/index.html).* **Admin**: Manages all clusters, data sources, notebooks, users, and settings. Admins provide users with connection details so they can run queries in notebooks or the clients of their choice.## Admin capabilitiesOn the **Users** page an admin can invite, edit, or delete users and otheradmins. Admins can also track user logins to {{site.terms.galaxy_full}}.### Invite usersInvite a user at any time from the **Users** page in the **Admin** section.1. Click **Admin** and select **Users**.2. Click **+ New**.3. Enter the **Email address** and **Username**.4. Change to the new role type, either **User** or **Admin**.5. Click **Save** to send the user an invitation email. To invite more than one user,   select **Add another** and repeat the process.### Edit usersChange the user role types for other users and admins.If you change the role type from user to admin, the former admin can no longeredit, add, or delete data sources and clusters.If you upgrade a user to an admin, the former user can now access the{{site.terms.galaxy_full}} dashboard and the **Data sources** and **Clusters**pages. They can now add, edit, and delete data sources and clusters.1. Click **Admin** and select **Users**.2. Click  options and select **Edit user** next   to the user whose role you want to edit.3. Select the new role type **User** or **Admin**.4. Click **Save** to update the user account.### Deactivate usersIf you deactivate a user, they can no longer access {{site.terms.galaxy_full}}.To deactivate a user:1. Click **Admin** and select **Users**.2. Click  options and select **Deactivate user**.3. Confirm and click **Save**.To deactivate multiple users:1. Click **Admin** and select **Users**.2. In the users list, select the users you wish to deactivate by checking the   box next to the email address.3. Click **Actions** and select  **Deactivate**. The   **Actions** button only displays when one or more users is selected.4. Confirm deactivating the users in the dialog.Deactivated users can be activated again with the **Activate user** action orthe **Activate** bulk editing action.### Delete usersIf you delete an admin, the resources that the admin added to{{site.terms.galaxy_full}} stay but no longer list an owner. This means that anyadmin can manage those resources.To delete a user:1. Click **Admin** and select **Users**.2. Click  options and select **Delete user**.3. Confirm and click **Save** to complete removing a user.To delete multiple users:1. Click **Admin** and select **Users**.2. In the users list, select the users you wish to delete by checking the box   next to the email address.3. Click **Actions** and select  **Delete**. The   **Actions** button only displays when one or more users is selected.4. Confirm to proceed with deleting the users in the dialog."
 },
 {
  "title": "Starburst for data engineers",
  "url": "/data-engineer/index.html",
  "content": "                                  Anytime you need help connecting to data sources, configuring or securing        catalogs, or understanding how the data sources are used as catalogs, start here.        Learn more about data engineers.                                                                                  Introduction              Create your data mesh              to access all your data with {{site.terms.sb}}                                                              Helpful resources                                                                              Catalogs              Define, configure, and secure data sources in Starburst                                                                                                    Connectors              Enable access to any data source in your data mesh                                                                                                    Query performance              Get the best out of your cluster resources                                                                                                    Starburst Insights              Easy access to cluster and query metrics                                                       Looking for a different guide? Go to {{site.terms.sb}} for      data consumers      or      platform administrators.            "
 },
 {
  "title": "Metadata overview",
  "url": "/starburst-galaxy/metadata/index.html",
  "content": "# {{ page.title }}The {{site.terms.galaxy_full}} supports metadata, including [metastores](/starburst-galaxy/metadata/metastores.html) and [tags](/starburst-galaxy/metadata/tag-resources.html). Add, edit, or remove:* [Metastores](/starburst-galaxy/metadata/metastores.html)* [Tags](/starburst-galaxy/metadata/tag-resources.html)"
 },
 {
  "title": "Client overview",
  "url": "/data-consumer/clients/index.html",
  "content": "# {{ page.title }}{{site.terms.sb}} lets you query all the connected data sources with the mostwidely used and best supported query language — standard SQL. You can even querymultiple data sources with the same query. Simple queries, and these morecomplex federated queries all access the data right in the source. There is noneed for ETL processes.As a data consumer, you can use any supported client tool to connect to your{{site.terms.sb}} cluster and execute your queries. Many tools allow you muchmore powerful usage than simply running custom written queries. You can createcomplex reports, charts, dashboards, and many other useful results.## Basic connection informationThe details you need to connect to {{site.terms.sb}} are independent of yourpreferred client tool. For a minimum connection without TLS, you need:* URL of the {{site.terms.sb}} cluster, including the port used.* Credentials, typically username and password.Ask your {{site.terms.sb}} platform administrator for this information.Let&#39;s look at some examples:A basic test installation on your local computer, using the default port and noTLS configuration:* http://localhost:8080* username can be a random string like your first name since no authorization is  configured* no passwordThe same basic test application running on a different server:* http://starburst.example.com:8080* random username string* no password### Default portsThe following table shows the default ports for a {{site.terms.sep}} cluster:| Port | Connection type ||------|-----------------|| 8080 | Direct to coordinator, no security enabled || 8443 | Direct to TLS-enabled coordinator ||   80 | Connect through load balancer or proxy server ||  443 | TLS-enabled load balancer or proxy server |## TLS connection informationIf you enable TLS for your coordinator, you typically use a load balancer orproxy. In this case, the default port is generally used, and the protocolchanges to ``https``:* https://starburst.example.com* random username string* passwordTLS is a requirement for authorization against a provider&#39;s data, such as yoursite&#39;s LDAP directory. In this case, you must use real credentials:* https://starburst.example.com* LDAP username* LDAP passwordIf your client tool uses JDBC to connect, you must enable TLS support with the``SSL=true`` parameter in the JDBC configuration for your client, as describedin [Enable JDBC TLS support](./jdbc.html#enable-jdbc-tls-support).Other authorization providers may require additional credentials. Support forother providers varies among client tools.## Determine cluster versionThe version of {{site.terms.sep}} running on the cluster determinescompatibility of suitable clients. Specifically important is whether the clusteris using {{site.terms.sep}} 354-e or newer, or an older release prior to therename of the open source project. Newer versions use the name Trino, whileolder releases up to 350-e use the Presto or PrestoSQL name.You can determine the version of the cluster by asking your platformadministrator, or with one of the following methods, depending on your access:* Connect a client or browser to the ``v1/info`` REST endpoint. For example,  connect to ``http://starburst.example.com:8080/v1/info``.* Connect a modern web browser to the cluster and log in to view the  {{site.terms.sep}} [Web UI](../../starburst-enterprise/try/verify.html). For  example, connect to ``http://starburst.example.com:8080/ui``. The version  number is shown on the right side of the top row.* Connect the {{site.terms.sep}} [CLI](./cli.html) and run the following query:  ``select * from system.runtime.nodes;``## General adviceGeneral advice for using clients is the following:* For clusters running {{site.terms.sep}} version 350 or earlier, use version  350 of the client.* For clusters running {{site.terms.sep}} version 350, you can use client  version 350, 354 or newer to help with migration.* For clusters running {{site.terms.sep}} versions 354 and later, use the same  version of the client as the cluster or a newer client.This applies for the CLI and JDBC driver. Some clients, such as ODBC driver,have separate versioning, and details are documented with the client. Otherclients, including open source tools like [DBeaver](./dbeaver.html), use theTrino name for versions 354 or newer and PrestoSQL or Presto for older versions.Examples:{{site.terms.sep_full}} version 360-e LTS recommended clients:* CLI 360* JDBC driver 360* DBeaver with **Trino** connection{{site.terms.sep_full}} version 345-e LTS recommended clients:* CLI 350* JDBC driver 350* DBeaver with **PrestoSQL** connection## {{site.terms.sb}} client tools and drivers{{site.terms.sb}} offers a number of supported clients and tools* [Command line interface (CLI)](./cli.html) for shell scripts and manual  execution using a terminal* [Java Database Connectivity (JDBC) driver](./jdbc.html), typically for  JVM-based applications and others with JDBC support* [Open Database Connectivity (ODBC) driver](./odbc.html), typically for for  Windows-based applications and others with OBDC supportThe JDBC and ODBC driver can be used for many other clients, and you can findinstructions and specifics tips for the following tools:* [Tableau](./tableau.html)* [Microsoft Power BI](./powerbi.html)* [DBeaver](./dbeaver.html)* [SQuirrel SQL](./squirrel-sql.html){% include note.html  content=&quot;If you encounter an error message on connection stating &quot;Session  properties cannot be overridden once a transaction is active&quot;, enable **Auto  commit** or the equivalent setting in your client. Refer to your client&#39;s  documentation for information on where to configure this setting.&quot;%}## Other client tools and driversIn addition, the {{site.terms.oss}} community provides the following clients:* [trino-python-client](https://github.com/trinodb/trino-python-client)* [trino-go-client](https://github.com/trinodb/trino-go-client)The wider open source community maintains [numerous other clients andtools](https://trino.io/resources.html) that can be used.## Other resourcesThe O&#39;Reilly book, [Trino, the DefinitiveGuide](https://www.starburst.io/info/oreilly-trino-guide/), was writtenhere at {{site.terms.sb}}, and contains some great information on gettingstarted with using different types of clients with {{site.terms.sb}}. It&#39;savailable for free!"
 },
 {
  "title": "Get started",
  "url": "/get-started/index.html",
  "content": "                    {{page.title}}                                              {{site.terms.sb}} makes it easy to set up trial configurations for the {{site.terms.sep_first}}.        Try {{site.terms.sep_full}}.                                                                                          Choose the right deployment              This guide helps you select the best solution for your organization.                                                              Welcome to the user guides. Select your role below to start your journey.                                                                              Data consumer              I use data from {{site.terms.sb}} with my BI and data science tools to create important business insights.                                                                                                    Data engineer              I provide the source data to data consumers and ensure its quality, availability, and performance.                                                                                                    Platform administrator              I install and manage the clusters that serve our data,                 and ensure everything is performing well.                                                Still not sure where to start? Head over to our user personas page to learn which fits you best.              Browse topics                                              Security                                                                    Video library                                                                    Glossary                                                                    Connectors                              "
 },
 {
  "title": "Starburst",
  "url": "/index.html",
  "content": "                    Everything about {{site.terms.sb}} products                                      What can you learn here?      No matter which {{site.terms.sb}} product you use, and what you are trying to achieve,        you can find help on these pages:              Installing and operating a {{site.terms.sb}} cluster on your own infrastructure        Using Starburst on public cloud provider infrastructure        Connecting data sources to query your relational database or your object storage        Using a BI tool to query data with {{site.terms.sb}}        Writing SQL statements to get more insights from your data            Explore the site to find all this and more information in articles,      the reference documentation, videos and other material.                          Which crew member best describes you?                                                                              Data consumer              You use data from {{site.terms.sb}} with your BI and data science tools to create important business insights                                                                                                    Data engineer              You provide the source data to data consumers using {{site.terms.sep}}, and ensure its quality, availability, and performance                                                                                                    Platform administrator              You install and manage the clusters that serve the data, and ensure everything is performing well                                                Are you a data leader? You can learn more    about how {{site.terms.sb}} products can impact your data insights, ops processes, budget, and efficiency.        Still not sure where to start? Head over to our user personas page to learn which fits you best!              Other resources                                                Video library                                                                    Reference docs                                                                    SQL                              "
 },
 {
  "title": "Data sources overview",
  "url": "/starburst-galaxy/data-sources/index.html",
  "content": "# {{ page.title }}You can create a catalog by selecting a data source and configuring theconnection to various external databases and other systems.Once the data source is defined and used in your {{site.terms.sb}} cluster, youcan query the data source by accessing the catalog and the nested schemas andtables.## RequirementsThe requirements to access the different data sources vary and are detailed witheach data source. Detailed steps are available in the following documentation:* [Create an IAM user and attach a policy](./iam-policies-users.html)* [Create a secret in AWS](./secrets.html)* [Create a private connection](./private-connections.html)## Initial data source setupSelect a data source below to see requirements and instructions for adding it to{{site.terms.galaxy_full}}:* [Amazon S3 + AWS Glue data catalog](./s3/index.html)* [Amazon S3 + Hive metastore service](./s3-hms/index.html)* MySQL* PostgreSQL* Redshift* Snowflake## Data source types and propertiesData source types define the connector that {{site.terms.galaxy_full}} uses forthe specific data source. The connector in turn, defines the configurationproperties to use.A number of properties are used for all data source configurations.            Property      Description                  Type      Defines the connection type of the external data source. Type is        related to the connector used to access the  data in the external data        source.              Name      The name of the data source that is also used as the name of the        catalog. This name is visible to when querying {{site.terms.fka}} and        the underlying data source.              Description      A short paragraph that provides more details about the data source        than the name alone.              Connection properties      Properties required by the connectors that enable connection to a        specified data source, such as access keys.      "
 },
 {
  "title": "PostgreSQL data source",
  "url": "/starburst-galaxy/data-sources/postgresql/index.html",
  "content": "# {{ page.title }}You can connect PostgreSQL databases in {{site.terms.galaxy}} as a data source.## Requirements* User with sufficient access to the data source must be configured with a  [secret](../secrets.html).* Established [private connection](../private-connections.html) between  {{site.terms.galaxy}} and PostgreSQL, , or inbound and outbound security  groups to allow `0.0.0.0/0` for all traffic.## Operation* [Add a PostgreSQL data source](./add-postgresql.html)"
 },
 {
  "title": "Amazon AWS",
  "url": "/ecosystems/amazon/index.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} can be used with many components of Amazon AWS. You candeploy there, access data sources in Amazon AWS and use some of the otherfeatures of Amazon AWS.Find all the relevant information for using {{site.terms.sb}} and Amazon AWS inthe following sections and guides.## DeploymentsYou can install {{site.terms.sb}} on Amazon Elastic Kubernetes Service (EKS),Amazon Elastic Compute Cloud (EC2) or through the AWS Marketplace, once you haveestablished your enterprise account.To get started {{site.terms.sb}} offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### Amazon EKSYou can use the Amazon Elastic Kubernetes Service (EKS) directly; EKS iscertified to work with {{site.terms.sep}}. More information applicable to bothscenarios in available in our [Kubernetes referencedocumentation](../../latest/k8s.html), including an installation checklist, acustomization guide, examples and more tips.### Amazon EC2As a user of Amazon EC2 instances you can manage your {{site.terms.sep}} clusterwith the same [AMI and CloudFormation Template](../../latest/aws.html) as AWSMarketplace users.Alternatively, you can manage a RPM or tarball-based installation with[Starburst Admin](../../starburst-enterprise/starburst-admin/index.html).### AWS Marketplace{{site.terms.sep}} is available on [AWSmarketplace](https://aws.amazon.com/marketplace/pp/B07DKV5659?qid=1605738632387&amp;sr=0-1&amp;ref_=srh_res_product_title) with the following fulfillment options:- 64-bit (x86) Amazon Machine Image (AMI)- [CloudFormation Template](../../latest/aws.html)        ## DocumentationYou are currently viewing the AWS section of our user guides. In the left-handnavigation are guides for AWS-specific topics. We also have user guides tailoredtoward the different types of {{site.terms.sb}} users: [data consumers, dataengineers and platform administrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### AWS marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:* Email-only support* Five [email issues to  awssupport@starburstdata.com](mailto:awssupport@starburstdata.com) per month* First response SLA of one business day* Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Starburst for platform administrators",
  "url": "/platform-administrator/index.html",
  "content": "                                  Anytime you need help with an {{site.terms.sep_first}} administration           task or concept, you should start here. This guide takes           you to the most up-to-date information on running your {{site.terms.sep}} cluster. Learn more about platform administrators.        And if you have not seen it already, read our          guide to choosing the right          deployment.                                                                                         Introduction                Learn how {{site.terms.sep}} works, and access resources for security and deployment options.                                                                            What you need to know                                                    Cluster basics          Everything you need to know to get started                                                            Tune your cluster          Advanced configuration for even better performance                                                            Deploy and update          From initial install to keeping {{site.terms.sep}} up-to-date                                                            Secure your cluster          Client access, data sources, and more                           Looking for a different guide? Go to {{site.terms.sb}} for    data consumers    or    platform administrators.      "
 },
 {
  "title": "Try Starburst Enterprise",
  "url": "/starburst-enterprise/try/index.html",
  "content": "# {{ page.title }}{{site.terms.sb}} makes it easy to set up trial configurations of{{site.terms.sep_first}}.For evaluation purposes, {{site.terms.sep}} can be installed without a license.To use more than the basic functionality you must [request a triallicense](https://www.starburst.io/contact/).Features that require a license include:* Starburst Insights Worksheets SQL editor and query history* Advanced authentication methods and other security features* Starburst Cached Views* Enhanced and exclusive [connectors](../../latest/connector.html)## Deployment optionsYour preferred platform for running applications influence how you deploy{{site.terms.sep}} to a production environment. {{site.terms.sb}} supports thefollowing deployments:* [Container](../../glossary.html#container)* [Bare metal](../../glossary.html#bare-metal)* [Virtual machine](../../glossary.html#virtual-machine)We recommend deploying {{site.terms.sep}} using containers and Kubernetes.Linux is the required operating system.## Container-based deployment on KubernetesYou can manage {{site.terms.sep_full}} and a number of other components, such asthe Hive metastore service or Apache Ranger, with the[available Kubernetes support](./k8s.html).You can use the Kubernetes support for trial, development, and productionclusters.For a very limited trial, you can use[Docker on a server or your local workstation](./docker.html).## Bare metal and virtual machine deploymentTo use bare metal hardware or virtual machines, deploy and manage{{site.terms.sep}} clusters with [StarburstAdmin](../starburst-admin/index.html). Use your own data center or cloudprovider to provide and manage these clusters.You can use {{site.terms.sbadmin}} for trial, development, andproduction clusters.For a very limited trial, you can manage the server manually or use your localworkstation. In this case, use the most recent {{site.terms.sep}}[tarball](./any-linux.html) or [RPM](../../latest/installation/rpm.html) packagedirectly."
 },
 {
  "title": "Amazon S3 data source with AWS Glue data catalog",
  "url": "/starburst-galaxy/data-sources/s3/index.html",
  "content": "# {{ page.title }}You can connect data from [Amazon S3](https://aws.amazon.com/s3/) with metadatafrom [AWS Glue](https://aws.amazon.com/glue) in {{site.terms.galaxy}} as a datasource.## Requirements* AWS Access key ID and [secret](../secrets.html) with access to S3* AWS Access key ID and [secret](../secrets.html) with access to AWS Glue## Operation* [Add an Amazon S3 data source with AWS Glue catalog](./add-s3.html)"
 },
 {
  "title": "Microsoft Azure",
  "url": "/ecosystems/microsoft/index.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} can be used with many components of Microsoft Azure.You can deploy there, access data sources in Microsoft Azure and use some of theother features of Microsoft Azure.Find all the relevant information for using {{site.terms.sb}} and MicrosoftAzure in the following sections and guides.## DeploymentsYou can install {{site.terms.sep}} on Microsoft Azure Kubernetes Service (AKS),with Azure Virtual Machines, or through the Azure Marketplace, once you haveestablished your enterprise account.To get started {{site.terms.sb}} offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### Azure AKSYou can use the Microsoft Azure Kubernetes Service (AKS) directly. AKS iscertified to work with {{site.terms.sep}}.More information is available in our [Kubernetes referencedocumentation](../../latest/k8s/overview.html), including a customizationguide, examples and tips.We also have a helpful [installationchecklist](../../latest/k8s/installation.html) for an overview of the generalHelm-based installation and upgrade process.### Azure VMs and App ServicesAs an alternative, you can use Azure App Services and linux-based Azure VMs foryour {{site.terms.sep}} cluster, managing an RPM or tarball-based installationwith [Starburst Admin](../../starburst-enterprise/starburst-admin/index.html).### Azure Marketplace{{site.terms.sep}} is available in the Microsoft Azure[marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/starburstdatainc1582306810515.starburst-enterprise-presto?tab=Overview).          ## DocumentationYou are currently viewing the Microsoft Azure section of our user guides. In theleft-hand navigation are guides for Azure-specific topics. We also have userguides tailored toward the different types of {{site.terms.sb}} users: [dataconsumers, data engineers, and platformadministrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### Azure Marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:- Email-only support- Five [email issues to azuresupport@starburstdata.com](mailto:azuresupport@starburstdata.com) per month- First response SLA of one business day- Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Starburst Admin - get started",
  "url": "/starburst-enterprise/starburst-admin/index.html",
  "content": "# {{ page.title }}{{site.terms.sbadmin}} is a[collection](https://docs.ansible.com/ansible/latest/user_guide/collections_using.html#collections)of [Ansible](https://www.ansible.com/) playbooks for installing and managing{{site.terms.sep_first}} or {{site.terms.oss}} clusters.It includes the following features:* Installation and upgrade of {{site.terms.sep_first}} or {{site.terms.oss}}  using the RPM or `tar.gz` archives* Update the coordinator and worker nodes configuration files,  including catalog properties files for data source configuration* Service management of the cluster and on all nodes (start/stop/restart/status)* Collection of logs* Support for adding custom binary files, such as custom connectors or UDF* Support for adding custom configuration filesAll target machines must meet the [requirements](#requirements)outlined below prior to installing {{site.terms.sbadmin}}.{{site.terms.sbadmin}} does not manage the creation of the servers, theoperating system installation and configuration, and the Python and Javainstallation. It is also not designed to manage other related tools such asApache Ranger, a Hive Metastore Service or any data source.It is most suitable for managing clusters installed on bare metal servers orvirtual machines. Use the [Kubernetes with Helmsupport](../../latest/k8s.html) instead of {{site.terms.sbadmin}} if you usecontainers and Kubernetes.{% include note.html content=&quot;The legacy Presto Admin is deprecated, and nolonger supported for Starburst Enterprise version 354-e and higher.&quot; %}## RequirementsDeep knowledge of Ansible is not expected for usage, but familiarity withAnsible is required. It is assumed that you have Ansible installed, and arefamiliar with running Ansible playbooks.### Requirements for the control nodeThe [controlnode](https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html)is used to run {{site.terms.sbadmin}}, and therefore Ansible playbooks.[Standard Ansiblerequirements](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#prerequisites)apply:* Ansible 2.10 or higher* Linux/Unix operating system* Python 2.7 and higher or Python 3.5 and higherIn addition, the following resources are needed:* [SSH connectivity to the cluster nodes](https://docs.ansible.com/ansible/latest/user_guide/connection_details.html)* Downloaded {{site.terms.sep_first}} or {{site.terms.oss}} `tar.gz` or RPM  archive files on the control node, or alternatively URL to the files that is  accessible on all cluster nodesThe controller node can be any machine that is configured to fulfill theserequirements. For initial testing you can use your workstation or even a node inthe cluster directly. Production usage should follow Ansible best practices, anduse dedicated workflow or Ansible orchestration and automation tools such as[Ansible Tower](https://www.ansible.com/products/tower) or[Concord](https://concord.walmartlabs.com/).### Requirements for managed cluster nodes{{site.terms.sbadmin}} does not manage the cluster hardware, operating systemand package installation. It relies on the existence of all the nodes inthe cluster and the fact they fulfill the requirements detailed in this section.Typically provisioning systems such as Puppet, Chef, Terraform and others areused to prepare the cluster nodes.All cluster nodes need to fulfill the normal {{site.terms.sep_first}} or{{site.terms.oss}} requirements:* Linux operating system* Java runtime environment* PythonMemory and hardware resource requirements depend on the planned capacity of thecluster. Following are a few high level guidelines:* Use identical hardware configurations for all workers.* Start with at least two workers, scale up as needed.* Prefer fewer, more powerful worker nodes over many smaller ones.* For performance reasons, nodes are ideally located on the same subnet and  within the same data center. All nodes communicate using TCP/IP.Review the [requirements for the specific version ofSEP](../../latest/installation/deployment.html#requirements) for additionaldetails.Specific testing is performed with the distributions CentOS versions 7 andCentOS 8 and Red Hat Enterprise Linux (RHEL) versions 7 and 8.Additional requirements:* [Enabled SSH access and connectivity from the control  node](https://docs.ansible.com/ansible/latest/user_guide/connection_details.html),  the configured user must have root or sudo access.* `rsync`, often an optional package that needs to be installed.* `bash`, typically installed by default.When using {{site.terms.sbadmin}} with an RPM archive:* RPM-based Linux distribution* `rpm` command, `yum`, `dnf`, or others are not requiredWhen {{site.terms.sbadmin}} with an `tar.gz` archive:* GNU `tar` command* `unzip` command## Install {{site.terms.sbadmin}} on the control node{{site.terms.sbadmin}} is a collection of Ansible playbooks that you install onthe control node:* Contact {{site.terms.support}} for the {{site.terms.sbadmin}} `tar.gz` binary  package.* Download it onto the control node into any directory, such as `~/tmp`.* Access the directory in a command line interface.* Install the collection with the following command:  ```shell  ansible-galaxy collection install starburst-admin-*.tar.gz  ```* Confirm the command finishes successfully:  ```shell  Starting galaxy collection install process  Process install dependency map  Starting collection install process  Installing &#39;starburst.admin:1.0.0&#39; to &#39;....&#39;  starburst.admin:1.0.0 was installed successfully  ```The collection is installed into `/home//.ansible/collections` bydefault. The installation path `/home//.ansible/collections/ansible_collections/starburst/admin/files`is used for the binaries and all the configuration files for a cluster. Makesure you manage the files in this directory with a version control system.You can override the installation path with the option `-p `.### Install on multiple control nodesIf you need to install the collection into numerous control nodes, you can makethe binary available on a remote URL:* Make the binary available on a server via HTTP, for example,  `https://repo.example.com/files/starburst-admin-1.0.0.tar.gz`.* Create a file `requirements.yml` that includes a link to the binary.  ```yaml  ---  collections:      # Example link to tar.gz package      - https://repo.example.com/files/starburst-admin-1.0.0.tar.gz  ```* Use the YAML file for the installation  ```sh  ansible-galaxy collection install -r requirements.yml  ```## Next stepsNow that you have set up the control nodes and the managed cluster nodes, youcan proceed with the [initial installation on thecluster](./installation-guide.html)."
 },
 {
  "title": "Google Cloud",
  "url": "/ecosystems/google/index.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} can be used with many components of Google Cloud. Youcan deploy there, access data sources in Google Cloud and use some of the otherfeatures of Google Cloud.Find all the relevant information for using {{site.terms.sep}} and Google Cloudin the following sections and guides.## DeploymentsYou can install {{site.terms.sep}} on Google Kubernetes Engine (GKE), GoogleCloud Compute Engine, or through the Google Cloud Marketplace, once you haveestablished your enterprise account.To get started {{site.terms.sb}} offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### Google Kubernetes Engine (GKE)GKE is a fully supported deployment service for {{site.terms.sep}} with[more information available in the dedicated documentation](./gke.html).### Google Cloud Compute EngineAs an alternative to using GKE, you can use virtual machines in Google CloudCompute Engine for your {{site.terms.sb}} cluster, managing an RPM ortarball-based installation with [StarburstAdmin](../../starburst-enterprise/starburst-admin/index.html).### Google Cloud Marketplace{{site.terms.sep}} is certified on [GoogleCloud](https://console.cloud.google.com/marketplace/product/starburst-public/starburst-enterprise) and is available straight from the Cloud Console, lettingyou deploy {{site.terms.sep}} now, and scale later as your data needs grow.        ## DocumentationYou are currently viewing the Google Cloud section of our user guides. In theleft-hand navigation are guides for Google Cloud-specific topics. We also haveuser guides tailored toward the different types of {{site.terms.sb}} users:[data consumers, data engineers, and platformadministrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### Google Cloud Marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:- Email-only support- Five [email issues to gcpsupport@starburstdata.com](mailto:gcpsupport@starburstdata.com) per month- First response SLA of one business day- Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Red Hat OpenShift",
  "url": "/ecosystems/redhat/index.html",
  "content": "# {{page.title}}OpenShift from Red Hat Marketplace (RHM) is a container platform usingKubernetes operators that automate the provisioning, management, and scaling ofapplications to any cloud platform or even on-prem. Whether you need informationabout deploying {{site.terms.sb}} on OpenShift, or working with data sourcesthere, we have the information you need right here.{{site.terms.sb}} offers a stable, secure, efficient, and cost-effective way toquery all your enterprise data, wherever it resides with{{site.terms.sep_first}}. Our Kubernetes deployments ease the burden andcomplexity of configuring, deploying, and managing {{site.terms.sep}}.{% include video.html content=&quot;Check out this video demonstration by Karl Eklund, Principal Architect atRed Hat, showing OpenShift as a central location to explore data and buildmodels.&quot; %}## Deployments{{site.terms.sep}} is available to install through RHM, or[directly](https://harbor.starburstdata.net/) from {{site.terms.sb}} once youhave established your enterprise account.{{site.terms.sb}} also offers Starburst also offers a [triallicense](https://www.starburst.io/platform/starburst-enterprise/download/releases/)and help with your proof of concept.### OpenShift deploymentsYou can use {{site.terms.sb}}&#39;s Kubernetes deployment directly on OpenShift.More information is available in our [Kubernetes referencedocumentation](../../latest/k8s.html), including our [customizationguide](../../latest/k8s/sep-configuration.html) for {{site.terms.sep}}.We also have a helpful [installationchecklist](../../latest/k8s/installation.html) for an overview of the generalHelm-based installation and upgrade process.### Red Hat Marketplace{{site.terms.sep}} is available as an operator on[RHM](https://marketplace.redhat.com/en-us/products/starburst-enterprise)as of OpenShift version 4. Our [Red Hat OpenShift deployment guide](./openshift-deployment.html) provides instructions for using {{site.terms.sb}}&#39;s Helm-based installation and upgrade process.        ## DocumentationYou are currently viewing the Red Hat OpenShift section of our user guides. Inthe left-hand navigation are guides for OpenShift-specific topics. We also haveuser guides tailored toward the different types of {{site.terms.sb}} users:[data consumers, data engineers, and platformadministrators](../../get-started/starburst-personas.html).Our comprehensive [reference documentation](../../latest/index.html) isavailable for all supported releases.## SupportNo matter where you deploy from, {{site.terms.support}} [has yourback](../../support.html).### Enterprise accounts{{site.terms.sb}} enterprise customers enjoy:* Dedicated resources for your organization* 24/7 expert support* Rapid-response SLAs* Access to knowledge base articles and an intuitive support portal### AWS marketplace customers{{site.terms.sb}} offers the following support for our marketplace subscriberswithout an enterprise contract:- Support through the OpenShift Support portal- Five issues per month- First response SLA of one business day- Support hours of 9 AM - 6 PM US Eastern Time"
 },
 {
  "title": "Starburst for data leaders",
  "url": "/data-leader/index.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a fast, interactive distributed SQL query enginethat decouples compute from data storage. {{site.terms.sep}} lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single {{site.terms.sep}} query can combine data from all these datasources and more. {{site.terms.sep}} can run on-prem as well as in many cloudenvironments, and comes with [30+ supported enterpriseconnectors](https://www.starburst.io/learn-presto/connectors), providinghigh performance SQL-based access to most of the data platforms in yourorganization such as Snowflake, Postgres, and Hive.Maybe your organization relies on a single variant of SQL, or maybe they use afew. With {{site.terms.sep}}, you only need to know {{site.terms.sb}} SQL, whichis ANSI-compliant and should feel comfortable and familiar. {{site.terms.sep}}takes care of translating your queries to the correct SQL syntax for your datasource.{{site.terms.sep}} can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast, and requires a minimal disk investment. It can pullyour landing times forward, and help you meet or beat your SLAs. And, Starbursthas robust access control options for your organization, from integrating withLDAP to using your existing Ranger-managed policies.Your analysts and data scientists can connect to most of their favorite toolsusing only the {{site.terms.sep}}-specific JDBC or ODBC driver, much like theones they already use, making the transition as frictionless as possible.Want to know more? No problem! Here are some resources to answer commonquestions about {{site.terms.sep}}&#39;s capabilities and value:* [What problems does Starburst solve?](https://www.starburst.io/presto-videos/video-series-intro-to-starburst-and-presto/)* [Will this help me future-proof my data ops?](https://www.starburst.io/presto-videos/modern-data-architecture/)* [How does Starburst deliver value?](https://www.starburst.io/download-starburst-presto-technical-solution-brief/)* [Is there a comprehensive product review available?](https://www.starburst.io/starburst-overview-bloor/)"
 },
 {
  "title": "Deploying Starburst in the cloud",
  "url": "/ecosystems/index.html",
  "content": "                    {{page.title}}                                       If you need information on using {{site.terms.sep_first}} with a      specific ecosystem, you&#39;ve come to the right place! If you already have      Starburst, click on your ecosystem below for the guides and how-tos you      need.            Getting ready to deploy {{site.terms.sep}}? Along with our      install-anywhere Kubernetes      deployment, {{site.terms.sep}} is available directly      in many of your favorite ecosystems as a part of their marketplace      offerings. You can purchase preconfigured machine images, containers and      other needed resources to run {{site.terms.sep}} on their cloud hosts,      under your control. A free trial configuration is available from most      providers. For all cloud provider offerings, you need the following:              Access credentials from your cloud provider        The URLs and access credentials for the data sources you want your            cloud-hosted SEP cluster to access        All appropriate ACLs must be in place            Click on your preferred ecosystem below to learn more.                                {{site.terms.sb}} is available in the following ecosystems:                                                              Amazon AWS                                                                        Google Cloud                                                                        Microsoft Azure                                                                        Red Hat OpenShift                              "
 },
 {
  "title": "Video library",
  "url": "/videos/index.html",
  "content": "                    {{ page.title }}                                      Explore the growing list of videos to learn about {{site.terms.sb}}        products, installation, tuning, management, SQL, and more.        Use timestamps to drill down and learn about a specific topic.      {% include list-videos.html dir=&quot;/videos/&quot; %}"
 },
 {
  "title": "Redshift data source",
  "url": "/starburst-galaxy/data-sources/redshift/index.html",
  "content": "# {{ page.title }}You can connect Redshift databases in {{site.terms.galaxy}} as a data source.## Requirements* User with sufficient access to the data source must be configured with a  [secret](../secrets.html).* Established [private connection](../private-connections.html) between  {{site.terms.galaxy}} and Redshift, or inbound and outbound security  groups to allow `0.0.0.0/0` for all traffic.## Operation* [Add a Redshift data source](./add-redshift.html)"
 },
 {
  "title": "Amazon S3 data source with Hive metastore service",
  "url": "/starburst-galaxy/data-sources/s3-hms/index.html",
  "content": "# {{ page.title }}You can connect data from [Amazon S3](https://aws.amazon.com/s3/) with metadatafrom an external Hive metastore service (HMS) in {{site.terms.galaxy}} as a datasource.## Requirements* AWS Access key ID and [secret](../secrets.html) with access to S3* Established [private connection](../private-connections.html) between  {{site.terms.galaxy}} and the HMS, or inbound and outbound security groups to  allow `0.0.0.0/0` for all traffic. Default port for HMS access with the Thrift  protocol is 9083.* HMS access without authentication.## Operation* [Add an Amazon S3 data source with Hive metastore service](./add.html)"
 },
 {
  "title": "User guide",
  "url": "/starburst-enterprise/index.html",
  "content": "                    {{site.terms.sep_full}}                              Try {{site.terms.sep_full}}                      Check the reference documentation                                        Welcome to the {{site.terms.sep_first}}.                                                                              Try {{site.terms.sep_full}}                                                                                                    Reference docs                                                                                                    {{site.terms.sb}} Admin                                                Still not sure where to start? Head over to our user personas page to learn which fits you best.              Browse topics      We&#39;ve also gathered some resources on popular topics for you.                                                Glossary                                                                    Videos                                                                    Clusters                                                                    SQL                              "
 },
 {
  "title": "Starburst for data consumers",
  "url": "/data-consumer/index.html",
  "content": "                                  Start here anytime you need help with using {{site.terms.sb}}          products to drive your day-to-day analytics, update your SQL skills,          or improve your data science or business intelligence insights.          Learn more about data consumers.                                                                                                  Introduction                Learn about working with                {{site.terms.sb}} as data consumer.                                                                                                                        Clients                Use {{site.terms.sb}}                products with your favorite tools.                                                                                Helpful resources                                                                                SQL              SQL syntax, data types, and functions and operators                                                                                                    Your data mesh              Use query federation with  {{site.terms.sb}}              to access your data anywhere                                                                                                    {{site.terms.sb}} {{site.terms.insights}}              Simple Worksheet SQL query editor included in {{site.terms.sep_full}}                                                                                                    Materialized views              Get your data even faster with              auto-refreshing materialized views                                                       Looking for a different guide? Go to {{site.terms.sb}} for      data engineers      or      platform administrators.            "
 },
 {
  "title": "Snowflake data source",
  "url": "/starburst-galaxy/data-sources/snowflake/index.html",
  "content": "# {{ page.title }}You can connect to any database in your Snowflake account in{{site.terms.galaxy}} as a data source.## Requirements* Password for a user with sufficient access to the data source must be  configured with a [secret](../secrets.html).## Operation* [Add a Snowflake data source](./add.html)"
 },
 {
  "title": "MySQL data source",
  "url": "/starburst-galaxy/data-sources/mysql/index.html",
  "content": "# {{ page.title }}You can connect MySQL databases in {{site.terms.galaxy}} as a data source,## Requirements* User with sufficient access to the data source must be configured with a  [secret](../secrets.html).* Established [private connection](../private-connections.html) between  {{site.terms.galaxy}} and MySQL, or inbound and outbound security  groups to allow `0.0.0.0/0` for all traffic.## Operations* [Add a MySQL data source](./add-mysql.html)"
 },
 {
  "title": "Starburst Insights and Worksheet query editor",
  "url": "/data-consumer/clients/insights.html",
  "content": "# {{ page.title }}{{site.terms.insights}} is a web-based interface providing an intuitive queryeditor and data browser, along with a visual overview of important querystatistics. It is available as part of {{site.terms.sep_full}}, and needs to be[activated and configured](../../latest/insights.html) by your platformadministrator.## AccessTo access {{site.terms.insights}}, get the {{site.terms.sep_full}} cluster URLfrom your platform administrator. For example:```shellhttps://my.internal.url/```The {{site.terms.insights}} URL is that URL with `/ui/insights` appended:```shellhttps://my.internal.url/ui/insights```Login with the same credentials you use to access {{site.terms.sep_full}} withthe CLI or any other client.## Easy data exploration with Worksheet{{site.terms.insights}} includes an ad hoc querying tool, Worksheet. Worksheetis a powerful, web-based tabbed query editor and data source explorer. Itallows you to run multiple ad hoc queries and, with a click, drill down into theinformation you need to optimize it.{% include image.html  url=&#39;../../assets/img/general/insights-worksheet.png&#39;  img-id=&#39;insight-worksheet&#39;  alt-text=&#39;Insights Worksheet&#39;  descr-text=&#39;Image depicting Insights Worksheet&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;  screenshot=&#39;true&#39;%}Worksheet is meant for quick exploration and query validation. If your resultset is very wide, or is greater than 1000 rows, it is truncated. Queries andresults are only available until a new query is run in a tab, or your page ortab is closed or refreshed, whichever comes first.## Detailed views of query statisticsYou can also drill down into query details, including step-by-step executionstatistics to help you optimize query performance.{% include image.html  url=&#39;../../assets/img/general/insights-query-details.png&#39;  img-id=&#39;insights-query-details&#39;  alt-text=&#39;Insights query details&#39;  descr-text=&#39;Image depicting Insights query details&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;  screenshot=&#39;true&#39;%}## Other features and details{{site.terms.insights}} includes [numerous otherfeatures](../../latest/insights.html) that allow your platform administrator tounderstand the performance and usage of your {{site.terms.sep_full}} cluster."
 },
 {
  "title": "Starburst Admin installation guide",
  "url": "/starburst-enterprise/starburst-admin/installation-guide.html",
  "content": "# {{ page.title }}You can proceed with the installation using the following steps, after[preparing the control node and the managed clusternodes](./index.html#requirements).## Define the inventoryBefore using the playbooks, you need to edit the [Ansible inventoryfile](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html),`hosts`, to define where to install the software:1. Copy the `playbooks/hosts.example` inventory file to `playbooks`, name it   `hosts` and set the hostname for your coordinator and your worker(s).2. Set the environment variable `ANSIBLE_INVENTORY` to the absolute path   of the hosts file, for example:   ```   export ANSIBLE_INVENTORY=~/.ansible/collections/ansible_collections/starburst/admin/playbooks/hosts   ```3. Specify the IP address of the single coordinator host and one or more worker   hosts.4. Set the username for Ansible to connect to the host with SSH with   `ansible_user` for each host.5. Set the password  value with `ansible_password` or use a path to a private   key using `ansible_ssh_private_key_file`.The following snippet shows a simple example with one coordinator and twoworkers:```[coordinator]172.28.0.2 ansible_user=root ansible_password=changeme[worker]172.28.0.3 ansible_user=root ansible_password=changeme172.28.0.4 ansible_user=root ansible_password=changeme```Run the Ansible `ping` command to validate connectivity to the hosts:```$ ansible all -m ping -i playbooks/hosts172.28.0.4 | SUCCESS =&gt; {    &quot;ansible_facts&quot;: {        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    },    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}172.28.0.2 | SUCCESS =&gt; {    &quot;ansible_facts&quot;: {        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    },    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}172.28.0.3 | SUCCESS =&gt; {    &quot;ansible_facts&quot;: {        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;    },    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}```Alternatively to using `ANSIBLE_INVENTORY` for the location of the hosts file,you can create an `ansible.cfg` file in the root directory of `starburst-admin`(typically `~/.ansible/collections/ansible_collections/starburst/admin/`) withthe setting `inventory=playbooks/hosts` in the `[defaults]` section and run thecommands from that folder.```[defaults]inventory=playbooks/hosts```You can also specify the hosts file when running the `ansible-playbook` commandwith the `-i ` parameter.Maintaining multiple host files and potentially multiple set of configurationfiles enables you to manage multiple clusters.## Provide {{site.terms.sep_full}} packageDecide if you want to use the RPM or tarball installation. RPM requires `sudo`access and installs files in specific folders. When using the tarball, customfolders can be specified, and they don&#39;t have to be owned by root.Download the binary archive and place it in the `files` directory.Alternatively, configure the `installer_url` in `files/vars.yml` to point to aURL that is available from all your cluster nodes to download the packagedirectly.Reference the [documentation about the installplaybook](./playbook-reference.html#install) to learn more about all therelevant configuration steps and options.## Create minimal configurationThe default configuration is managed in separate configuration files in `files`as well as variables defined in in `playbooks/vars.yml`. It is suitable to getstarted after the following adjustments:* Update the `version` value in `vars.yml` to the version of downloaded binary  file.* Set the `environement` value in `vars.yml` to the desired cluster name.* Update the Java heap space memory configuration `-Xmx16G` in  `files/coordinator/jvm.config.j2` and `files/worker/jvm.config.j2` to  about 80% of the total available memory on each host.* Remove the `query.*` properties in  `files/coordinator/config.properties.j2` and  `files/worker/config.properties.j2` if you are using {{site.terms.sbadmin}}  1.0.0. Newer version do not include these properties. {{site.terms.sbadmin}}  calculates optimal values automatically.The default setup includes a `tpch` catalog, that you can use to test queriesand ensure the cluster is operational.If you are using the tarball, you can edit the target installation paths.Proceed with this simple configuration until you have verified that the clustercan be started and used, and then follow up with more configuration and changes.Reference the [documentation about the push-configsplaybook](./playbook-reference.html#push-configs) to learn more about all therelevant configuration steps and options.The [operation guides](./operation-guides.html) include numerous tips and tricksfor these next steps for your deployment.## Run installation and start the clusterYou are finally ready to get {{site.terms.sep_full}} installed on all clusternodes. The following steps copy the package to all cluster nodes and install it.Ansible creates the necessary configuration files from `files` and the definedvariables to the nodes, and starts the application on all of them:```shellansible-playbook playbooks/install.ymlansible-playbook playbooks/push-configs.ymlansible-playbook playbooks/start.yml```* [install details](./playbook-reference.html#install)* [push-configs details](./playbook-reference.html#push-configs)* [start details](./playbook-reference.html#start)## Accessing the Web UIA first step to validate the cluster is to log in to the [WebUI](../../latest/admin/web-interface.html):* Navigate to port 8080 on the coordinator host with the IP address or the  configured fully qualified domain name.* Login with any username.* Verify the displayed number of workers equals the number of workers in the  `hosts` file.## Connecting with clientsYou can use the [CLI or any otherclient](../../data-consumer/clients/index.html) to connect to the cluster andverify that the `tpch` catalog is available.## Managing configurationFor production usage you need to manage a separate set of all configurationfiles for each cluster you operate. It is also important to keep track of anychanges. Typically a version control system such as Git is used.You can also expand your processes to automatic management with a GitOpsworkflow and tools such as [AnsibleTower](https://www.ansible.com/products/tower) or[Concord](https://concord.walmartlabs.com/).## Next stepsAfter the successful installation and first initial tests, you can proceed toread about all the [available playbooks](./playbook-reference.html) and [learnabout updating and operating the cluster](./operation-guides.html)."
 },
 {
  "title": "Starburst for data engineers",
  "url": "/data-engineer/introduction.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a fast, interactive distributed SQL query enginethat decouples compute from data storage. {{site.terms.sep}} lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single {{site.terms.sep}} query can combine data from all these datasources and more forming a data mesh.{{site.terms.sb}} can greatly reduce reliance expensive, complex and oftenbrittle ETL frameworks and their pipelines. Because it uses data instead of diskto execute queries across the cluster, it’s also fast. {{site.terms.sep}} canpull your landing times forward, and help you meet or beat your SLAs.## How does this work?{{site.terms.sep}} comes with [supported enterpriseconnectors](../latest/connector.html) providing high performanceSQL-based access to most of the data platforms in your organization - such asTeradata, Oracle, PostgreSQL, and Hive. Each data platform is defined as a[catalog](./catalogs.html). Catalogs, in turn, define schemas and their tables.Catalogs also, at a minimum, define the [connector](./connectors.html) that{{site.terms.sep}} uses to connect to that data source:```connector.name=sqlserverconnection-url=jdbc:sqlserver://:;database=connection-user=rootconnection-password=secret```Once you have your connection established, many connectors have configurationproperties that help you tune the connector’s performance, such as for timeouts,retries and connection limits.{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;SEP architecture&#39;  descr-text=&#39;Image depicting SEP components and architecture&#39;  pxwidth=&#39;500&#39;  modal=&#39;true&#39;%}{{site.terms.sep}} uses an ANSI-compliant SQL that should feel comfortable andfamiliar. {{site.terms.sep}} takes care of translating your queries to thecorrect SQL syntax for your data source. If you are migrating from Hive, we havea [migration guide](../../latest/appendix/from-hive.html) in ourdocumentation.## How do I get started?As a first step you should read our [guide to choosing the right deployment](../get-started/choosing-the-right-deployment.html).If your organization does not already have {{site.terms.sep}}, you can request a[trial instance](https://www.starburst.io/#!), following the steps in [Trying{{site.terms.sb}} products](../starburst-enterprise/try/index.html)."
 },
 {
  "title": "Starburst for data consumers",
  "url": "/data-consumer/introduction.html",
  "content": "# {{ page.title }}If you champion data-driven decisions in your org, {{site.terms.sb}} hasthe tools to connect you to the data you need. {{site.terms.sb}} brings allyour data together in a single, federated environment. No more waiting for dataengineering to develop complicated ETL. The data universe is in your hands!{{site.terms.sep_full}} is a distributed SQL query engine. Maybe you know asingle variant of SQL, or maybe you know a few. {{site.terms.sb}}&#39;s SQL isANSI-compliant and should feel comfortable and familiar. It takes care oftranslating your queries to the correct SQL syntax for your data source. All youneed to access all your data from a myriad of sources is a single JDBC or ODBCclient in most cases, depending on your toolkit.Whether you are a data scientist or analyst delivering critical insights to thebusiness, or a developer building data-driven applications, you’ll find you caneasily query across multiple data sources, in a single query. Fast.## How does this work?Data platforms in your organization such as Snowflake, Postgres, and Hive aredefined by data engineers as *catalogs*. Catalogs, in turn, define schemas andtheir tables.  Depending on the data access controls in place, discovering whatdata catalogs are available to you across all of your data platforms can beeasy! Even through a [CLI](../latest/installation/cli.html), it’s asingle, simple query to get you started with your federated data:{% highlight sql %}presto&gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows){% endhighlight %}After that, you can easily explore schemas in a catalog with the familiar `SHOWSCHEMAS` command:{% highlight sql %}presto&gt; SHOW SCHEMAS FROM hive_sales LIKE `%rder%`; Schema--------- order_entries customer_orders(2 rows){% endhighlight %}From there, you can of course see the tables you might want to query:{% highlight sql %}presto&gt; SHOW TABLES FROM order_entries; Table------- orders order_items(2 rows){% endhighlight %}You might notice that even though you know from experience that some of yourdata is in MySQL and others in Hive, they all show up in the unified `SHOWCATALOGS` results. From here, you can simply join the data sources fromdifferent platforms as if they were from different tables. You just need to usetheir fully qualified names:{% highlight sql %}SELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &gt; 300LIMIT 2;{% endhighlight %}## How do I get started?The first order of business is to get the latest {{site.terms.sb}}[JDBC](../latest/installation/jdbc.html) or[ODBC](./clients/odbc.html) driver and get itinstalled. Note that even though you very likely already have a JDBC or ODBCdriver installed for your work, you do need the {{site.terms.sb}}-specificdriver. Be careful not to install either in the same directory with other JDBCor ODBC drivers!If your data ops group has not already given you the required connectioninformation, reach out to them for the following:* the JDBC URL - `jdbc:presto://example.net:8080`* whether your org is using SSL to connect* the type of authentication your org is using - username or LDAPWhen you have that info and your driver is installed, you are ready to connect.## What kind of tools can I use?More than likely, you can use all your current favorite client tools, and evenones on your wishlist with the [help of our tips andinstructions](./clients/index.html).{% include image.html  url=&#39;../assets/img/general/data-consumption-overview.png&#39;  img-id=&#39;dco&#39;  alt-text=&#39;Data Tools and SEP&#39;  descr-text=&#39;Image depicting how SEP connects data sources to analytics tools&#39;  pxwidth=&#39;400&#39;  modal=&#39;true&#39;%}## How do I migrate my data sources to {{site.terms.sb}}?In some cases, this is as easy as changing the sources in your `FROM` clauses.For some queries there could be slight differences between your data sources’native SQL and SQL, so some minor query editing is required. Rather thanchanging these production queries on the fly, we suggest using your favorite SQLclient or our own[CLI](../../latest/installation/cli.html) to test yourexisting queries before making changes to production.If you are migrating from Hive, we have a [migrationguide](../../latest/appendix/from-hive.html) in ourdocumentation. To help you learn how others have made the switch, here is ahandy walk-through of using[Looker](https://www.starburstdata.com/presto-videos/query-federation-using-looker-and-starburst-presto/)and {{site.terms.sep_full}} together.## Where can I learn more about {{site.terms.sb}}?From our documentation, of course! Visit our [data consumer&#39;s user guide]({{site.baseurl }}/data-consumer/index.html)."
 },
 {
  "title": "Starburst for data platform administrators",
  "url": "/platform-administrator/introduction.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a fast, interactive distributed SQL query enginethat decouples compute from data storage. {{site.terms.sep}} lets you query datawhere it lives, including Hive, Snowflake, MySQL and even proprietary datastores. A single {{site.terms.sep}} query can combine data from all these datasources and more. {{site.terms.sep}} can run on-prem as well as in many cloudenvironments.{{site.terms.sep}} can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast. It can pull your landing times forward, and help youmeet or beat your SLAs. And, {{site.terms.sep}} has robust access controloptions for your organization, from integrating with LDAP to using yourRanger-managed policies.## How does this work?[SEP](../latest/overview/concepts.html) is a distributed system that runson COTS hardware. The coordinator parses, analyzes and plans query execution,and then distributes the query plan for processing among worker machines in thecluster. Workers use connectors specific to your data sources, such asSnowflake, Postgres, and Hive to transform queries and return data.{% include image.html  url=&#39;../assets/img/general/data-engg-schematic-overview.png&#39;  img-id=&#39;des&#39;  alt-text=&#39;Starburst cluster overview&#39;  descr-text=&#39;Image depicting Starburst components and architecture&#39;  pxwidth=&#39;500&#39;  modal=&#39;true&#39;%}{{site.terms.sep}} uses ANSI-compliant SQL, and takes care of translating yourqueries to the correct SQL syntax for your data sources.{{site.terms.sep}}’s ability to federate data sources in a single query reducesyour organization&#39;s reliance on temporary tables and more complex ETL pipelines.Because {{site.terms.sep}} query processing works in memory, your diskinvestment is light.## How do I get started?As a first step you should read our [guide to choosing the right deployment](../get-started/choosing-the-right-deployment.html).You can spin up a [trial instance](https://www.starburst.io/#!) following thesteps in [Trying {{site.terms.sb}}products]({{site.baseurl}}/starburst-enterprise/try).When you are ready to create a production cluster and you&#39;ve chosen one of ourKubernetes-based deployments, read our handy [introduction to SEP withKubernetes](../latest/k8s/overview.html) guide. It explains{{site.terms.sep}} cluster design and sizing, as well as best practices forcustomization and configuration with Helm.We also have some great [training videos](../videos/index.html) to get youstarted, and some articles on topics you are likely to have hard questions on:* Data architecture [philosophy and  approach](https://www.starburst.io/presto-videos/modern-data-architecture/)* [Reference  architectures](https://www.starburst.io/learn/open-source-trino/reference-architectures/)* Security  [guide](https://www.starburst.io/starburst-enterprise-security-guide) and  [deep dive](https://trino.io/blog/2020/08/13/training-security.html)* [SEP and Helm](../latest/k8s.html)* SEP [administration](../latest/admin.html)"
 },
 {
  "title": "JDBC driver",
  "url": "/data-consumer/clients/jdbc.html",
  "content": "# {{ page.title }}The Java Database Connectivity (JDBC) driver enables any application supportinga JDBC driver to connect to {{site.terms.sb}} clusters. The application can thenissue SQL queries and receive results.Typically the applications are JVM-based, or support JDBC driver usage with adifferent mechanism. Using an applications with the JDBC driver generallyfollows the same steps.## Connection informationGet the [connection information](./index.html#connection-information) and the[cluster version](./index.html#connection-information) for the cluster you wantto connect to and use to run queries.## Add the driver to your clientThe way to add the JDBC driver to your [client application](./index.html) variesfor each application.Some applications, such as [DBeaver](./dbeaver.html), automatically download andconfigure the driver, or include it by default and no action is necessary.Many applications, however, require you to download the driver and place it in adirectory that varies for each application. Refer to your client application&#39;sdocumentation for the necessary details.The JDBC driver needs to be compatible with the [version of the cluster you areconnecting to](./index.html#cluster-version).### Download the JDBC driverYou can request the JDBC driver to match a specific {{site.terms.sb}} versionfrom [{{site.terms.support}}](../../support.html).Alternatively, to gain access to {{site.terms.sep}} archives, visit the[{{site.terms.sb}} website](https://www.starburst.io) and click either**Get Started** or **Download Free**.This opens a dialog that prompts for your name, email address, and location.Fill out the form *using a valid email address*, then click **Free Download**.Click the link to the **Downloads** page in the resulting email. The page isorganized into Long-Term Support (LTS) and Short-Term Support (STS) sections.The LTS section is split into **{{site.terms.sep_full}}** and **Clientapplications** steps.If your cluster version is a current release, use the **LTS Step 2** or **STS**sections to download the JDBC driver version that matches your cluster version.You can also download the JDBC driver binary directly from the link in thereference documentation:* For cluster versions older than 350, download JDBC driver [version  350](../../350-e/installation/download.html).* For cluster versions 350, 354, and newer, download the matching JDBC driver  version:  * [Version 354](../../354-e/installation/download.html)  * [Latest version](../../latest/installation/download.html)Use the reference documentation&#39;s version selector in the top right corner toselect the download page for different releases, or contact[{{site.terms.support}}](../../support.html).## Create a connection to the clusterWith an installed driver you can now configure a connection to a cluster.Typically, you only need the connection information. Other details such as theJDBC URL are automatically configured in many applications, or can be seen andupdated in a user interface dialog.The main parameters are the driver classname, which is typically preconfiguredby your client driver integration, the JDBC URL and the credentials from theneeded [connection information](./index.html#connection-information).Driver version 354 and newer:* Classname: `io.trino.jdbc.TrinoDriver`* JDBC URL: `jdbc:trino://host:port`Driver version 350 and older:* Classname: `io.prestosql.jdbc.PrestoDriver`* JDBC URL: `jdbc:presto://host:port`The `host` parameter is the network name or IP address of the coordinator ofyour {{site.terms.sep}} cluster. Enter the `port` provided by your networkadministrator, or enter one of the [default ports](./index.html#default-ports)for {{site.terms.sep}} clusters. See the [reference documentation for the JDBCdriver](../../latest/installation/jdbc.html) for other supported JDBCparameters.## Enable JDBC TLS supportIf your {{site.terms.sb}} cluster is configured to require a TLS connection fromclients, you must configure your client as follows:* Specify a valid username and password for the authentication system in use on  the coordinator, such as LDAP.* Specify the JDBC parameter `SSL=true` (where the older term `SSL` activates  `TLS` support).  * If the client UI provides an interface, such as a grid for specifying    parameters, use that.  * If not, you can append `?SSL=true` to the JDBC connection string. For    example:  ```url  jdbc:trino://host:port?SSL=true  ```## Start queryingThe configured connection can now be opened and you can start running queries.For example, if your application allows you to write and run queries, you cansee what catalogs are available:```sqlSHOW CATALOGS;```Many applications provide a user interface to see the list of catalogs, schema,tables and much more.## Next stepsNow you can take advantage of the features of your application, and potentiallylearn more about the [SQL support](../starburst-sql.html) in{{site.terms.sb}}."
 },
 {
  "title": "Try Starburst Enterprise with Kubernetes",
  "url": "/starburst-enterprise/try/k8s.html",
  "content": "# {{ page.title }}Kubernetes is the preferred platform for {{site.terms.sep_first}} deployments.You can manage the deployment directly with the available Helm charts orthrough your preferred cloud provider&#39;s[marketplace](../../ecosystems/index.html). The following platforms aresupported:* Amazon Elastic Kubernetes Service (EKS)* Google Kubernetes Engine (GKE)* Microsoft Azure Kubernetes Service (AKS)* Red Hat OpenShift 4.x or higherYou can use a K8s deployment as a proof-of-concept (PoC) and scale it up easilyto a fully-fledged deployment of {{site.terms.sep}} in your organization.{% include note.html content=&quot;The [marketplaceofferings](../../ecosystems/index.html) provide even more convenient, yet lessflexible alternatives for running Starburst Enterprise.&quot;%}This page assumes you have the following:* Familiarity with Kubernetes.* ``kubectl`` is installed and running.* You can create and administer a Kubernetes cluster.* You have access to our [Helm chart repository](../../latest/k8s/requirements.html#k8s-helm-repository) and  [Docker registry](../../latest/k8s/requirements.html#k8s-docker-registry),  provided by {{site.terms.support}}.Your {{site.terms.sep}} cluster must be located in a namespace dedicated to itand it alone. Our K8s overview discusses {{site.terms.sep}} [clusterdesign](../../latest/k8s/overview.html#kubernetes-cluster-design-for-sep-full),and our [K8s requirements](../../latest/k8s/requirements.html) covers minimumproduction cluster specs and versions for tools that you need.## {{site.terms.sb}} Helm chartsThe following Helm charts are available as part of the {{site.terms.sep}} K8soffering:* [Starburst Enterprise](../../latest/k8s/sep-configuration) for images, security,  coordinator and worker nodes, as well as catalogs, mounted volumes and other  properties.* [Apache Ranger](../../latest/k8s/ranger-configuration) including the  Starburst Ranger plugin, policy database and everything you need to integrate  Ranger with {{site.terms.sep}}.* Apache [Hive Metastore Service](../../latest/k8s/hms-configuration).* [Starburst cache service](../../latest/k8s/cache-service-configuration) to use  {{site.terms.sb}} {{site.terms.cached_views}}.## Prepare for deploymentA proof of concept for {{site.terms.sep}} can have as few as two nodes, oneeach for the coordinator and a worker, in a dedicated namespace.### Set up registry accessOnce you have the {{site.terms.sep}} Helm chart, create the``registry-access.yaml`` file to override the default, empty values as describedin our [referencedocumentation](../../latest/k8s/installation.html#create-this-one-file-before-you-begin).You use this one file for all {{site.terms.sb}} Helm charts.### Prepare cluster and resource configurationsNow it&#39;s time to create your [correctly-sized Kubernetescluster](../../latest/k8s/requirements.html#k8s-cluster-requirements), andensure that your Helm/``kubectl`` configuration points at the correct clusterusing ``kubectl cluster-info``.With that done, create a [minimal YAML configurationfile](../../latest/k8s/sep-config-examples.html#k8s-ex-sep-prod-setup) with thememory and CPU resource configurations for ``coordinator:`` and ``worker:`` thatreflect your cluster&#39;s available resources. We suggest a name such as``sep-PoC-setup.yaml``.{% include warning.html content=&quot;**Do not skip this step.** The default valuesfor memory and CPU resources likely vary significantly from your cluster&#39;savailable resources. If you attempt to run SEP with the defaults,it may not start.&quot; %}## Deploy {{site.terms.sep}}Run the ``helm`` command to install the default chart, as well as any overrideYAML files using the ``--values`` argument, as in the following example:```shell$ helm upgrade my-sep-poc-cluster starburstdata/starburst-enterprise     --install     --version 360.2.0     --values ./registry-access.yaml     --values ./sep-PoC-setup.yaml```We strongly suggest using the ``helm upgrade`` rather than ``helminstall``. It ignores any unchanged configuration files and only applies thosewith changes.### VerifyTo verify that your {{site.terms.sep}} cluster is operating, first determine theIP address or the DNS hostname of the coordinator by running the ``kubectl getpods`` command. Next, use the IP address or hostname to verify the coordinatoris running by using the Web UI as described in [Verify theserver](./verify.html). You can use the same information to connect with the CLIor the JDBC driver.## Configure your clusterFor a proof-of-concept, we suggest that you start with small, focused changesthat configure one concept, such as networking, security, or data sources.Depending on your organization&#39;s security requirements and the location of your{{site.terms.sep}} cluster relative to your data sources, you may find that youneed to configure [internalcommunication](../../latest/k8s/sep-configuration.html#internal-communication) and[networking](../../latest/k8s/sep-configuration.html#exposing-the-cluster) before youcan configure a data source.Once you have verified that your cluster has access to your data sources, it&#39;stime to [configure the datacatalogs](../../latest/k8s/sep-configuration.html#catalogs) that describe your datasources.### Restart to deploy configuration changesRun the ``helm upgrade`` command to apply any changes, adding any new YAMLfiles, for instance, in the following example, an ``sep-catalogs.yaml`` file hasbeen added:```shell$ helm upgrade my-sep-poc-cluster starburstdata/starburst-enterprise     --install     --version 360.2.0     --values ./registry-access.yaml     --values ./sep-PoC-setup.yaml     --values ./sep-catalogs.yaml```## Next stepsAs you build out your PoC and beyond, our reference documentation has anextensive [SEP configurationsection](../../latest/k8s/sep-configuration.html), along with[examples](../../latest/k8s/sep-config-examples.html). You may also wish to setup these optional items:* [Hive Metastore Service](../../latest/k8s/hms-configuration)* [Apache Ranger](../../latest/k8s/ranger-configuration)* [Starurst cache service](../../latest/k8s/cache-service-configuration)"
 },
 {
  "title": "LDAP authentication",
  "url": "/starburst-galaxy/ldap-auth.html",
  "content": "# {{ page.title }}The LDAP configuration enables users to authenticate to the{{site.terms.galaxy_first}}.It also allows you to use the same credentials to connect with client tools,such as the {{site.terms.fka}} CLI, or an application using the JDBC or ODBCdriver. Once connected you can use them to run queries against the data sourcesconfigured as catalogs.The following properties need to be configured for your LDAP authenticationssupport. Work with the administrator of your LDAP server to determine thecorrect values.      Property    Description        URL    The combination of the protocol, FQDN, and port used by your LDAP server.      Protocol can be ldaps:// or ldap://        User bind pattern    The pattern used to look up the relevant users in your LDAP directory.      An example is ${USER}@ldap.example.com        Group authorization query    Queries the LDAP user groups        Distinguished name    {{site.terms.galaxy_full}} service user identity        Bind password    Authorizes the user bind pattern          User distinguished name    Service user identity        Cache TTL    Time interval to cache credentials after first authentication, and      before authentication is required again  Consult the [LDAP documentation for{{site.terms.sep}}](../latest/security/ldap.html) for more details,and work with your LDAP server administrator.## Edit LDAP cluster settingsEdit your cluster&#39;s LDAP settings from the **Clusters** page.1. Locate the cluster whose LDAP settings you want to edit2. Select the **Options** icon (three stacked dots), click **Edit**3. Select the **LDAP configuration** tab4. In the **Configure LDAP properties** section, edit your properties5. When you complete your edits, click **Update** to save your new configuration6. To apply your changes, restart your cluster"
 },
 {
  "title": "Markdown usage",
  "url": "/internal/markdown.html",
  "content": "# {{ page.title }}This page shows the output of all the supported markdown syntax usage. The siteuses CommonMark.## Markdown source files- Standard extension is `.md`- 80 character hard wrap width for markdown code- Embedded HTML can be wider- Use 2 space indent for HTML (no tab, not wider)## Section titlesTitles are marked with one or more `#`.Before title..# Level 1after title and before next one, should only be used for page title## Level 2after title and before next one### Level 3after title and before next one#### Level 4after title and before next one##### Level 5after title and before next one, too deep, probably should never be used###### Level 6after title and before next one, too deep, probably should never be used## Text formattingNormal text in a paragraph. You can just write along. Spaces between word orline breaks don&#39;t matter. An empty line starts a new paragraph.Use `_` or `*` to highlight words in _italics text_. Use `__` or `**` tohighlight words in __bolded text__. Don&#39;t mix bolding and italics.* __bold with underscore__* **bold with double asterisk*** _italics with underscore_* *italics with asterisk*Standard usage at Starburst is to use asterisk `*bold*`.## Code blocks and other source codeInline usage of source code uses simple backticks to surround the variable orsimple command:Add `~/bin` to the `PATH` with `EXPORT PATH=~/bin:$PATH`.Separate code block with three backticks:```cd /opt/dev/myprojectmvn clean install```You can declare a language like `shell` after the initial three backticks toget syntax highlighting.Here is a `shell` fenced block:```shellcd /opt/dev/myprojectmvn clean install```A `java` fenced block:```javaString a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an int```A `yaml` fenced block:```yamlimage:  repository: &quot;harbor.starburstdata.net/starburstdata/hive&quot;  tag: &quot;338.2.2-rc.1-SNAPSHOT&quot;  pullPolicy: &quot;IfNotPresent&quot;expose:  type: &quot;clusterIp&quot;  clusterIp:    name: &quot;hive&quot;    ports:      http:        port: 9083  nodePort:    name: &quot;hive&quot;    ports:      http:        port: 9083        nodePort: 30083```A `sql` fenced block:```sqlSELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]```If for some reason you really must have line numbers, then you must use a liquidhighlight block. This `java` highlight block uses the `linenos` keyword todisplay line numbers:{% highlight java linenos %}String a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an int{% endhighlight %}However, if you find yourself needing to refer to line numbers, your codeblockis likely too long for your purpose. Consider instead a one- to two-line excerptto illustrate what you are writing about, even if you keep the longer codeblockintact.## Unordered listsUse `-` or `*` for the items.- Apple- Pear- BananaYou can also indent:- Pets    - Dog    - Cat- Farm animal    - Cow    - Sheep## Ordered listsThe actual numbers in your document&#39;s source code aren&#39;t used as the line numbervalues in static documents. The line numbers you see in the generated documentsare  computed. So you can use e.g. 1. all the time. This example uses 1. for alllines in the source document, but the generated document are numbered correctly:1. Item 11. Item 2    1. Nested 1 (nested items in lists require 4 spaces to indent properly)    1. Nested 21. Item 3## Definition listsUse [dl/dt/dd HTML](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dl):  Term 1  the description for term 1  Term 2  the description for term 2## Links- Markdown syntax with `[text](url)` is preferred- Relative links are preferred to avoid issues with the root context- Ideally use `({{ site.baseurl }}/file.html ` for absolute links## TablesDirect layout markup:| Tables        | Are           | Cool  || ------------- |:-------------:| -----:|| col 3 is      | right-aligned | $1600 || col 2 is      | centered      |   $12 || zebra stripes | are neat      |    $1 |Semantic markup:Markdown | Less | Pretty--- | --- | ---*Still* | `renders` | **nicely**1 | 2 | 3Raw HTML:            First name      Last name                  Alison      Loney              Manfred      Moser      ## VideosEmbedding YouTube videos is supported courtesyhttps://github.com/nathancy/jekyll-embed-video. video-embed.css is included tomake it responsive, but that is untested. Other sources can be supported, butYouTube is all we need for now.You must include the video ID you want to use in the front matter.```---youtubeId1: buqtdpuZxvk---```You can have multiple videos on a page, you just have to give them differentnames in the front matter.If you want to use an entire video, just use youtubePlayer.html in your include,with the :```include youtubePlayer.html```If instead you want to play just a portion, use youtubeSnippet.html in yourinclude, and provide start and end values in seconds after the video id:```id=page.youtubeId1 start=22 end=31```At this point, you might be wondering to yourself, &quot;But what if I want to startat a particular place, then play until the end?&quot; Great question! In that case,just set the value for end to -1:```id=page.youtubeId1 start=22 end=-1```You&#39;ll have to read the .md file for now to see the entire include, as it justrenders the included HTML as html in a comment:{% include youtubeSnippet.html id=page.youtubeId1 start=22 end=31 %}If you are looking to include a Wistia video, use the wistiaPlayer.html in your include.You can customize the start time by using the `start` property with thefollowing format `start=&#39;1m30s&#39;`.*You **must** use either single or double quotes when adding a property. Thevalue of the `id` parameter is the same as the value of the `wvideo=` argument inthe Wistia URL.```{%raw%}{% include wistiaPlayer.html id=&#39;j38ihh83m5&#39; %}{%endraw%}```{% include wistiaPlayer.html id=&#39;j38ihh83m5&#39; %}If you just want to add a admonition with a text to a video, use the``video.html``. Careful about the apostrophes used for the embedded URL.{% include video.html content=&quot;Check out the TrinoCommunity Broadcast interview with the Apache Superset teamto learn more and see a demo.&quot; %}## AdmonitionsThe simplest admonition is just using a blockquote.&gt; This is a simple blockquote.And after this insightful blockquote, you can have a look at a even biggerblockquote.&gt; And here is a longer one.&gt;&gt; Its has two sentences. And also&gt;&gt; * a list item&gt; * another itemBeyond that you can use specific warning, caution and note admonitions.As per [Google developer documentation(GDD)](https://developers.google.com/style/notices) a warning is: &quot;Stronger thana Caution; it means &quot;Don&#39;t do this.&quot;Note that multi-line, paragraph-shaped passages work in the styling:{% include warning.html content=&quot;Like a red morn that ever yet betokened,Wreck to the seaman, tempest to the field,Sorrow to the shepherds, woe unto the birds,Gusts and foul flaws to herdmen and to herds.William Shakespeare&quot; %}A caution, which, as per GDD &quot;Suggests proceeding with caution&quot;:{% include caution.html content=&quot;Do not be tricked into thinking that there areno crocodiles just because the water is still. - Malaysian proverb&quot; %}A note (GDD: &quot;An ordinary note or tip&quot;):{% include note.html content=&quot;Try using a small LIMIT when testing out a newquery.&quot; %}A note with a long link to see if it breaks/wraps:{% include note.html content=&quot;This is an example note to see how the line breakis handled by css.https://starburstdata.atlassian.net/wiki/spaces/PM/pages/1080688861/Starburst+Galaxy+UX+Research-let+saddareallylong+link&quot; %}You can also use [capture to create a large section ofcontent](https://jekyllrb.com/docs/includes/#passing-parameter-variables-to-includes)and then pass that to the admonition.## Side navigationTo include multiple levels of navigation you will need to use the following ymlsyntax:```nav:  - title: SEP Overview    context: /starburst-enterprise/index  - title: Platform administrators    id: admins    collapse: /platform-administrator    subnavigation:      - title: Home        context: /platform-administrator/index      - title: SEP clusters        id: test        collapse: /platform-administrator/cluster        subfolderitems:         - title: Test           context: /platform-administrator/cluster/test```"
 },
 {
  "title": "Materialized views",
  "url": "/data-consumer/materialized-views.html",
  "content": "# {{ page.title }}[Starburst Enterprise platform](../starburst-enterprise/index.html)({{site.terms.sep}}) supports materialized views with the[Hive](../latest/connector/starburst-hive.html#materialized-views) and[Iceberg](../latest/connector/iceberg.html#materialized-views) connectors. Youcan use  to take a federated query and store the results local to{{site.terms.sep}}Like regular views, materialized views standardize complex, analytical queries.With ({{site.terms.sep}} you can use these materialized with any query,including a federated query that accesses data in multiple catalogs and schemas.They provide a pre-computed result set to query against in physical storage,increasing query performance.Materialized views created in a Hive catalog may be automatically refreshed, asdiscussed in this document.{% include note.html content=&quot;There is no support at this time for automaticrefresh of materialized views in Iceberg.&quot; %}## PrerequisitesYour {{site.terms.sep}} platform administrator or a data engineer withadministrative access to {{site.terms.sep}} has to first enable materializedviews in one or more Hive or Iceberg catalogs. Check with them to learn whichcatalogs are enabled, and which schemas may be used.You also must have the necessary access privileges to create data in the schemadesignated in each enabled catalog.## Create and use a materialized viewAny federated query that runs successfully in {{site.terms.sep}} can be used tocreate a materialized view.  Materialized views in {{site.terms.sep}} arecreated in the same way as in other data platforms, with a [CREATE MATERIALIZEDVIEW](../latest/sql/create-materialized-view.html) statement. In the followingexample, the `mysalescatalog` has been configured to allow materialized views:```sqlCREATE MATERIALIZED VIEW mysalescatalog.mysalesschema.mv_cust_tot_return AS    SELECT      sr_customer_sk ctr_customer_sk,      sr_store_sk ctr_store_sk,      sum(sr_return_amt) ctr_total_return    FROM    tpcds.sf1.store_returns,    tpcds.sf1.date_dim    WHERE ( (sr_returned_date_sk = d_date_sk) AND (d_year &gt; 2000) )    GROUP BY sr_customer_sk, sr_store_sk;```Once a materialized view exists, you can query it like any regular table:```sqlSELECT * FROM mycatalog.mysalesschema.mv_cust_tot_return;```## Automatically refresh materialized views in HiveHive catalogs in {{site.terms.sep}} configured to allow materialized viewsprovide several `WITH` clause properties to configure refresh schedules and hownew data is imported:* `refresh_interval` and `cron`: Choose one method to specify a refresh  frequency.* `max_import_duration`: Specifies how long to allow a refresh to complete  before failing.* `grace_period`: Specifies the amount of time in-flight queries can run against  an expiring snapshot.* `incremental_column`: Specifies the column to be used to identify new data  since the last refresh. If you do not use this field, {{site.terms.sep}} performs a full refresh.We suggest that you review our reference documentation, which has [moreinformation about theseproperties](../latest/connector/starburst-hive.html#automated-materialized-view-management).In the following example, the `refresh_interval` property is used toautomatically refresh the data every 24 hours from the time the `CREATE`statement initially runs:```sqlCREATE MATERIALIZED VIEW myhive.mysalesschema.mv_cust_tot_returnWITH (  refresh_interval = &#39;24.0h&#39;,  grace_period = &#39;5.00m&#39;,  max_import_duration = &#39;30.00m&#39;) AS    SELECT      sr_customer_sk ctr_customer_sk,      sr_store_sk ctr_store_sk,      sum(sr_return_amt) ctr_total_return    FROM    tpcds.sf1.store_returns,    tpcds.sf1.date_dim    WHERE ( (sr_returned_date_sk = d_date_sk) AND (d_year &gt; 2000) )    GROUP BY sr_customer_sk, sr_store_sk;```In this example, the refresh runs for a maximum of 30 minutes. Unless the`cron` property is specified, the time at which data in a materialized view isrefreshed is based on the moment the `CREATE MATERIALIZED VIEW` statement firstruns, plus the `refresh_interval`.You can run refreshes on a set schedule by using the `cron` property instead.The `cron` property uses normal cron expressions. Here is the same materializedview, created with a `cron` schedule and an incremental column:```sqlCREATE MATERIALIZED VIEW myhive.mysalesschema.mv_cust_tot_returnWITH (  cron = &#39;30 2 * * *&#39;  grace_period = &#39;5.00m&#39;,  max_import_duration = &#39;30.00m&#39;,  incremental_column = &#39;sr_returned_date_sk&#39;) AS    SELECT      sr_customer_sk ctr_customer_sk,      sr_store_sk ctr_store_sk,      sum(sr_return_amt) ctr_total_return    FROM    tpcds.sf1.store_returns,    tpcds.sf1.date_dim    WHERE ( (sr_returned_date_sk = d_date_sk) AND (d_year &gt; 2000) )    GROUP BY sr_customer_sk, sr_store_sk;```This causes the refresh to execute at 2:30 AM daily, and loads only new data asdetermined by the `sr_returned_date_sk` date column."
 },
 {
  "title": "Metabase",
  "url": "/data-consumer/clients/metabase.html",
  "content": "# {{ page.title }}"
 },
 {
  "title": "Metastore overview",
  "url": "/starburst-galaxy/metadata/metastores.html",
  "content": "# {{ page.title }}There are two places in the {{site.terms.galaxy_first}} where you can addmetastores:1. **Metadata** page2. [Data sources](/starburst-galaxy/data-sources/index.html) page### Add metastore from the Metadata pageAdd an AWS Glue metastore or an external Hive metastore starting from the**Metadata** page. #### Add an AWS Glue metastoreAdding an AWS Glue metastore connection details allows access to metadata andmapping information about the objects stored in AWS to {{site.terms.galaxy}}. 1. Click **Admin** and select **Metadata**.2. In the **Metastores** section, click **+ New**.3. Select the **AWS Glue** metastore location.4. Enter a **Name for your AWS Glue metastore**.5. Select the **AWS Glue region** from the drop down menu.6. Select **Secret key** or **Assumed IAM role**.   * **Secret key**: Enter the **Access key ID** and your      **Secret access key name**. The secret key ID is      the name of your secret key in your secret manager.   * **Assumed IAM role**: Enter the **IAM role ARN**.7. Click **Finish** to save and return to the **Metadata** page.Review metastore information on the **Metadata** page.#### Add an external Hive metastoreAdding an external Hive metastore connection details allows access to metadataand mapping information about the objects stored in AWS to {{site.terms.galaxy}}. 1. Click **Admin** and select **Metadata**.2. In the **Metastores** section, click **+ New**.3. Select the **External Hive metastore** metastore location.4. Enter the **Hive metastore URI**. 5. Click **Show authentication options** to add other authentication    details, such as:   * Hive metastore username.   * Thrift client SSL enable (check the **True** box).   * Thrift client SSL trust certificate.   * Thrift client SSL trust certificate password.6. Click **Next**.Review metastore information on the **Metadata** page.### Add a new metastore while adding a data sourceInstead of adding a metastore from the **Metadata** page, add one while addinga data source {{site.terms.galaxy}}.* Add an AWS Glue metastore* Add an external Hive metastoreLearn about [AWS Glue metastores](https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro)and [external Hive metastores](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html)."
 },
 {
  "title": "Migrating your analytics to Starburst",
  "url": "/data-consumer/migration.html",
  "content": "# {{ page.title }}In some cases, migrating your analytics to {{site.terms.sb}} is as easy asswapping out your client, such as a JDBC driver, and changing the sources inyour `FROM` clauses. But because not every data source implements the SQLstandard the same, there could be slight differences between your data sources’native SQL and {{site.terms.sb}}&#39;s SQL. The information on this page will getyou started moving your tools and workflows to {{site.terms.sb}}.## Getting and installing {{site.terms.sb}} clients{{site.terms.sb}} has JDBC and ODBC drivers to connect your favorite tools to yourfavorite data. We have an [entire documentsection](./clients/index.html) that covers downloading, installing andconnecting clients.## Migrating queries from ANSI-standard SQL implementationsFor SQL implementations that follow the ANSI standard closely, only minor queryedits are likely. Rather than changing these production queries on the flythough, we suggest using your favorite SQL client or our own[CLI](./clients/cli.html) to test your existing queriesbefore making changes to production.Our full {{site.terms.sb}} [SQL referencemanual](../latest/language.html) is available to help youresolve any small implementation differences as you migrate your queries to{{site.terms.sb}}.## Migrating queries from HiveIn other cases where a SQL implementation deviates significantly from the ANSIstandard, such as with Hive&#39;s HiveQL, there some things you&#39;ll want tokeep in mind. Our documentation covers the syntactic and semantic differencesbetween HiveQL and other non-ANSI standard implementations such as:* Array syntax and handling* Syntax for strings and identifiers* CAST considerations* Differences in datediff* Complex expressions and subqueries* INSERT and OVERWRITE operationsRead the [Hive migration](../../latest/appendix/from-hive.html) page in ourreference documentation for detailed information on these topics.## Reducing ETLWith {{site.terms.sb}}, there are many opportunities to both reduce yourreliance on increasingly complex ETL pipelines and the intermediate storage tocentralize data that you must transform and clean up tech debt while you do so.These are complex problems and it can be hard to know where to begin. Thefollowing sections offer some strategies that make approaching the problemeasier.### Reduce or remove duplicative or similar processingIf you have multiple pipelines that ultimately produce the similar data with thesame grain, start there by combining those queries. For example, if youcalculate budget for ads at an ad_id level, and someone else is calculatinginvoices for the same ad_id grain, combine those queries. You save the doublecompute and storage demands so that you don’t scan the data twice, then move itaround twice, and finally write it twice with just one column being different.### Optimize around end usageAs you are doing your due diligence for this migration, whether you choose to leave some pipelines in your current framework or not, take the opportunity to review your pipeline design against their purpose:* For end usage purely for downstream ETL, optimize for writes* For end usage for dashboards and reports, optimize for readsFor in-depth information on optimizing queries, dive into our [trainingvideo](../data-engineer/query-performance.html) onoptimizing query performance, and reference the [queryoptimizer](../latest/optimizer.html) section of our referencedocumentation.### Reduce or remove the need for temp tables and intermediate disk storageData products that are derived from disparate sources often need to land in anintermediate schema where they can be combined locally. With {{site.terms.sb}},you can simplify this type of processing and remove the need for temp tables andschemas by taking advantage of {{site.terms.sb}}&#39;s powerful query federationabilities. This has the additional, positive side effect of obviating the needfor managing temp table cleanup jobs, too.### Reduce or remove little-used queries and data productsIf you are not already, start measuring the usage for dashboards and reports. Have any been abandoned? Has the usage shrunk enough on any of them so that there is no longer a justifiable ROI on maintaining them? If so, work with your stakeholders on a sunset plan, and remove the surfaces and pipelines.## Next stepsOnce you have cleaned up your pipelines, start looking at where you can delightyour customers with more aggressive data landing times. Upstream data ETL andavailability will continue to set a lower bound on your landing times, but thereis a positive, domino effect from reducing pipeline complexity and the needto wait for slow compute and storage operations."
 },
 {
  "title": "ODBC driver",
  "url": "/data-consumer/clients/odbc.html",
  "content": "# {{ page.title }}The Open Database Connectivity (ODBC) driver enables anyapplication supporting an ODBC driver to connect to {{site.terms.sb}} clusters.The client application can then issue SQL queries to a {{site.terms.sep_first}}cluster and receive results.{% include note.html    content=&quot;For more detailed information, refer to the    [complete ODBC driver documentation](https://starburstdata-downloads.s3.us-east-2.amazonaws.com/odbc/2.0.0.1001-docs/Starburst+ODBC+Driver+Install+and+Configuration+Guide.pdf).&quot;%}## InstallationTo install the ODBC driver, request the appropriate[driver and version](#driver-version-requirements) from {{site.terms.support}}and run the installer for your operating system.Typically you have to install the driver and add it to your clientapplication. Refer to your client&#39;s documentation for more details.### Supported platformsThe ODBC driver is supported with the followed operating systems and drivermanagers:**Windows*** Windows 7 SP1, 8.1, or 10* Windows Server 2008 R2 SP1, 2012, or 2016The ODBC driver can be managed on Windows with Windows ODBC Administrator.**MacOS*** OSX version 10.13, or 10.14The ODBC driver can be managed on MacOS with the following driver managers:* iODBC 3.52.9 or later* unixODBC 2.2.14 or later**Linux*** Red Hat® Enterprise Linux® (RHEL) 6 or 7* CentOS 6 or 7* SUSE Linux Enterprise Server (SLES) 11 or 12* Debian 8 or 9* Ubuntu 14.04, 16.04 or 18.04The ODBC driver can be managed on Linux with the following driver managers:* iODBC 3.52.9 or later* unixODBC 2.2.14 or laterOlder versions of the driver may be subject to different platform requirements.### Driver version requirementsIn general, you should download the newest version of the {{site.terms.sb}} ODBCdriver. Some ODBC-based clients may require earlier versions ofthe {{site.terms.sb}} ODBC driver, contact {{site.terms.support}} to requestspecific versions.Note that version 1.2.14 and earlier of the ODBC driver require a license file,which you can also request from {{site.terms.support}}. Install this licensefile in the ``lib`` directory of the ODBC driver installation.## Connect to a clusterWith the ODBC driver installed, you can now configure a connection to a cluster.Typically you only need the[connection information](./index.html#basic-connection-information).Other details such as the ODBC connection are automatically configured in manyapplications, or can be seen and updated in a user interface dialog.## Start queryingYou can now open a connection and start running queries.For example, if your application allows you to write and run queries you canrun the following command to see what catalogs are available:```sqlSHOW CATALOGS;```Many client applications provide a user interface to see the list of catalogs,schema, tables, and much more.## Next stepsNow that you can connect to {{site.terms.sep}} with a client application,learn more about {{site.terms.sb}}&#39;s [SQL support](../starburst-sql.html)."
 },
 {
  "title": "Red Hat OpenShift deployment guide",
  "url": "/ecosystems/redhat/openshift-deployment.html",
  "content": "# {{page.title}}{{site.terms.sep_first}} is available as an operator on OpenShift directlythrough the Red Hat Marketplace.## PrerequisitesBefore you get started, here are some things you need:* Access to an OpenShift cluster with with [correctly-sized  nodes]({{site.sep_rhm_url}}k8s/requirements.html#k8s-cluster-requirements),  using IAM credentials, and with sufficient Elastic IPs* Previously installed and configured Kubernetes, including access to  `kubectl`* An editor suitable for editing YAML files* Your {{site.terms.sep}} license fileBefore you get started installing {{site.terms.sep}}, we suggest that you readour [reference documentation](../../latest/k8s/overview.html) and our helpful[customization guide](../../latest/k8s/sep-configuration.html).## Quick startAfter you have signed up through RHM, download the latest OpenShift ContainerPlatform (OCP) client for your platform from [the OpenShift mirrorsite](https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/), andcopy the `oc` executable into your path, usually `/usr/local/bin`. Once thisis done, you are ready to install the operator in OCP4.Using your administrator login for Red Hat OCP, log in to the OCP web consoleand click **Operators &gt; OperatorHub** in the left-hand menu.Once there, select &quot;Starburst Enterprise&quot; from the **Project:** drop-down menu, and navigatethrough the projects to **All Items &gt; Big Data &gt; {{site.terms.sb}}** until yousee **{{site.terms.sep_full}}**. Click on that tile, then click the **Install**button.When the **Create Operator Subscription** page appears, select the**{{site.terms.sb}}** project as the specific namespace on the cluster, leaveall other options as default, and click **Subscribe**. When the operation is complete, you are subscribed to the {{site.terms.sep}}operator, and it is installed and accessible to you in OCP4.## Getting up and running### InstallationOnce you have your operator subscription in place, it&#39;s time to install. Thereare several steps to getting {{site.terms.sep}} installed and deployed:* Installing the {{site.terms.sep}} cluster* Installing the Hive Metastore Service (HMS)You must install the HMS to connect and query any objects storage with the Hiveconnector. This is typically a core use case for {{site.terms.sep}}, and then arequired step. The HMS is used by {{site.terms.sep}} to manage the metadata ofany objects storage.Follow these steps to **Install Starburst Enterprise Operator** via Red HatMarketplace:1. On the main menu, click **Workspace** &gt; **My Software** &gt; **Product** &gt;  **Install Operator**2. On the **Update Channel** section, select an option, &#39;stable&#39; or  &#39;alpha&#39;3. On the **ApprovalStrategy** section, select either **Automatic** or  **Manual**, the approval strategy corresponds to how you want to process  operator upgrades)4. On the **TargetClustersection**:  * Click the checkbox next to the clusters where you want to install the    **Starburst Enterprise Operator**  * For each cluster you selected, under** Namespace Scope**, on the **Select    Scope** list, select an option5. Click **Install**,  it may take several minutes for installation to completeOnce installation is complete, the status changes from *Installing* to *Up to date*.### ConfigurationWhen the operator installation is complete, you can proceed to deploy two customresources:* [Starburst Enterprise](./samples/starburst-enterprise-values.yaml)* [Starburst Hive](./samples/starburst-hive.yaml)Just like with installation, there are several steps to configuring{{site.terms.sep_full}}:* [Configuring SEP]({{site.sep_rhm_url}}k8s/sep-configuration.html)* [Configuring the Hive metastore]({{site.sep_rhm_url}}k8s/hms-configuration.html)Each of these steps uses a specific Helm chart `values.yaml` configuration.Click on the links for detailed instructions on configuring each of the customresources.The following setup steps are required:* Configure the resource requirements based on your cluster and node sizes* Update the image repository and tags to use the RedHat registry:  * Starburst Enterprise: `registry.connect.redhat.com/starburst/starburst-enterprise:354-e-ubi`  * Starburst Enterprise Init: `registry.connect.redhat.com/starburst/starburst-enterprise-init:354.0.0-ubi`  * HMS: `registry.connect.redhat.com/starburst/hive:354.0.0-ubi`## Next stepsYour cluster is now operational! You can now connect to it with your [clienttools](../../data-consumer/clients/index.html), and start querying your datasources.Follow these steps to quickly test your deployed cluster:1. Create a route to the default &#39;starburst&#39; service. If you changed the name in  the `expose` section, use the new name.2. Run the following command using the [CLI](../../data-consumer/) with the  configured route:  ```  trino --server  --catalog tpch  ```3. Run `SHOW SCHEMAS;` in the CLI, and you can see a list of schemas  available to query with names such as `tiny`, `sf1`, `sf100`, and others.We&#39;ve created an [operations guide]({{site.sep_rhm_url}}k8s/operation.html) toget you started with common first steps in cluster operations.It includes some great advice about starting with a small, initial configurationthat is built upon in our [cluster sizing and performance videotraining](../../videos/2020-09-09-cluster-sizing-and-performance-video.html).## Troubleshooting{{site.terms.sep}} is powerful, enterprise-grade software with many movingparts. As such, if you find you need help troubleshooting, here are some helpfulresources:* [LDAP authentication]({{site.sep_rhm_url}}security/ldap.html?#troubleshooting)* [Data consumer guide with clients, SQL and other tips](../../data-consumer/index.html)## FAQ**Q: Once it&#39;s deployed, how do I access my cluster?**A: You can use the [CLI on aterminal](../../data-consumer/clients/cli.html) or the [WebUI]({{site.sep_rhm_url}}admin/web-interface.html) to access your cluster. Forexample:* {{site.terms.oss}} CLI command: `./trino --server  example-starburst-enterprise.apps.demo.rht-sbu.io --catalog hive`* Web UI URL: `http://example-starburst-enterprise.apps.demo.rht-sbu.io`* [Many other client applications](../../data-consumer/clients/index.html) can  be connected, and used to run queries, created dashboards and more.**Q: I need to make administrative changes that require a shell prompt. How do Iget a command line shell prompt in a container within my cluster?**A: On OCP, you&#39;ll get a shell prompt for a pod. To get a shellprompt for a pod, you&#39;ll need the name of the pod you want to work from. To doso, log in to your cluster as per your RHM documentation. For example:```shelloc login -u kubeadmin -p XXXXX-XXXXX-XXXXX-XXXX https://api.demo.rht-sbu.io:6443```Get the list of running pods:```shell❯ oc get pod -o wideNAME                                                 READY   STATUS    RESTARTS   AGE   IP            NODE                                         NOMINATED NODE   READINESS GATEShive-XXXXXXXXX-lhj7l        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal               starburst-enterprise-coordinator-example-XXXXXXXXX-4bzrv   1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal                starburst-enterprise-operator-7c4ff6dd8f-2xxrr                     1/1     Running   0          41m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal               starburst-enterprise-worker-example-XXXXXXXXX-522j8        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal               starburst-enterprise-worker-example-XXXXXXXXX-kwxhr        1/1     Running   0          27m   10.130.2.XX   ip-10-0-162-XXX.us-west-2.compute.internal              starburst-enterprise-worker-example-XXXXXXXXX-phlqq        1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal                ```The `pod name` is the first value in a record. Use the `pod name` to open ashell:```shell❯ oc rsh starburst-enterprise-coordinator-example-XXXXXXXXX-4bzrv```A shell prompt will appear. For example, on OCP 4.4:```shellsh-4.4$```**Q: Is there a way to get a shell prompt through the OCP web console?**A: Yes. Log in to your OCP web console and navigate to**Workloads &gt; Pods**. Select the pod you want a terminal for, and click the**Terminal** tab.**Q: I&#39;ve added a new data source. How do I update the configuration torecognize it?**A: Using the [making configurationchanges](#configuration) section to edit your YAML configuration,find `additionalCatalogs`, and add an entry for your new data source. Forexample, to add a PostgreSQL data source called `mydatabase`:```yaml    mydatabase: |      connector.name=postgresql      connection-url=jdbc:postgresql://172.30.XX.64:5432/pgbench      connection-user=pgbench      connection-password=postgres123```Once your changes are complete, click `Save` and then `Reload` to deployyour changes. Note that this restarts the coordinator and all workers on thecluster, and might take a little while."
 },
 {
  "title": "Starburst Admin operation guides",
  "url": "/starburst-enterprise/starburst-admin/operation-guides.html",
  "content": "# {{ page.title }}After satisfying the [initial requirements](./index.html#requirements) for thecontrol node and all the managed nodes in the cluster, you can get a cluster upand running with the help of the[installation guide](./installation-guide.html).A fully configured deployment requires a bit more configuration and work. It istypically performed incrementally and you use {{site.terms.sbadmin}} to help youfor a number of scenarios. They range from simple tasks like changing aconfiguration in a catalog file to more complex ones such as adding a newcatalog file, scaling the cluster up or down, or adding security configuration.The following sections includes these and many other scenarios and refers to the[playbook reference](./playbook-reference.html) and other sections as necessary.## Targeting specific hostsBy default, Ansible runs the tasks defined in the playbooks on all cluster nodesdefined in the inventory `hosts` file.You can target specific host groups or even specific hosts with the `-l` option.The example hosts files uses the groups `coordinator` and `worker` to target thegroups. Individual hosts are targeted with the IP address.**Example: Roll out configuration changes to workers only*** Update the relevant configuration files in `files/worker`, for example changed  JVM configuration.* Push the configuration only to the workers  ```  ansible-playbook -l worker playbooks/push-configs.yml  ```* Restart only the workers  ```  ansible-playbook -l worker playbooks/restart.yml  ```**Example: Update security configuration on the coordinator*** Update the relevant configuration files in `files/coordinator`, for example  updated certificate files.* Push the configuration only to the coordinator  ```  ansible-playbook -l coordinator playbooks/push-configs.yml  ```* Restart only the coordinator  ```  ansible-playbook -l coordinator playbooks/restart.yml  ```**Example: Restart or remove a misbehaving worker only*** Determine the worker that has problem, for example from [collecting the  logs](./playbook-reference.html#collect-logs) or [checking the service  status](./playbook-reference.html#check-status)* Attempt a restart of the one worker that seems to have problems  ```  ansible-playbook -l 172.28.0.3 playbooks/restart.yml  ```* Check status after restart  ```  ansible-playbook -l 172.28.0.3 playbooks/check-status.yml  ```* Check the logs after restart  ```  ansible-playbook -l 172.28.0.3 playbooks/collect-logs.yml  ```## Adding, removing and updating catalogsYou can add a new catalog, update an existing catalog or even remove a catalogand roll that change out across the cluster with the following steps:* Perform the desired changes in the `files/catalog` directory.* [Stop the cluster](./playbook-reference.html#stop).* [Push the configuration](./playbook-reference.html#push-configs).* [Start the cluster](./playbook-reference.html#start).A full cluster restart of the coordinator and all workers is necessary to applythe updated configuration.## Changing other configuration* Perform the desired changes in the `files` subdirectories and files.* [Stop the cluster](./playbook-reference.html#stop) or selectively the  coordinator or all workers.* [Push the configuration](./playbook-reference.html#push-configs).* [Start the cluster](./playbook-reference.html#start) or selectively the  coordinator or all workers..The safest option is to restart the complete cluster. Depending on theconfiguration changes it can be possible to just restart all the workers or thecoordinator only. For example, for authentication configuration to{{site.terms.sep}} a coordinator restart can be sufficient.## Upgrading {{site.terms.sep_full }}Upgrading SEP using Ansible is similar to initial installation. The process hasto be performed without any users running queries.Use the following steps to perform the update:* Download the binary package for the new version and place it in `files`.* Alternatively place the binary package on a server and update the installer URL.* Update the version number in `vars.yml`.* Update all the configuration and add any new configuration files.* [Stop the cluster](./playbook-reference.html#stop).* [Install the new packages to the cluster](./playbook-reference.html#install).* [Push the configuration](./playbook-reference.html#push-configs).* [Start the cluster](./playbook-reference.html#start).```ansible-playbook playbooks/stop.ymlansible-playbook playbooks/install.ymlansible-playbook playbooks/push-configs.ymlansible-playbook playbooks/start.yml```To rollback the upgrade, revert any configuration changes, change the valueof `version` to the previous one, and execute the same playbooks above.Note that because all nodes in the cluster must use the same version,it is not possible to perform a rolling restart and avoid interruptions.To minimize down time, we recommend installing the new version on a setof new hosts, and reconfigure clients to connect to this cluster or updateDNS entries to point to it. Old cluster can be decommissioned after all queriesrunning on it are done.To manage separate installations on more than one sets of hosts, copythe `hosts` file and update values inside it. Then, to use the new file,add the `-i ` parameter when running the `ansible-playbook` command.### Blue/Green deployments and upgradesWith sufficient resources you can significantly improve your upgrade process bymanaging two productions cluster behind a load balancer (LB) with a definedfully qualified domain name (FQDN). The upgrade process can then follow ablue-green deployment process:* Current production cluster in use is *green* and LB uses the FQDN to point  users to it.* Update all configuration and resources for the inactive *blue* cluster to the  new version and configuration.* Install, push configuration and start the *blue* cluster.* Test the *blue* cluster with the direct IP address or an alternative FQDN  configured on the LB.* Switch the LB configuration to point the *blue* cluster.* Shut down the *green* cluster.For the next upgrade the process is the identical, just the role of the twoclusters is reversed.The inactive cluster can be kept running for failover and high availabilityusage or decommissioned and recreated again for future upgrades.## Adding or removing workersYou can use {{site.terms.sbadmin}} to help with scaling your cluster up anddown by adding or removing workers with the following steps.**Scaling up*** Provision the new machine with [necessary requirements for cluster  nodes](./index.html#requirements).* Add the IP address and access details to the `worker` section in the `hosts`  file.* Install on the new node* Push configuration to the new node* Start the new node* Check the log of the new nodeFor example, if you add the new host at `172.28.0.5` as worker you can run theinstallation just to this node:```[coordinator]172.28.0.2 ansible_user=root ansible_password=changeme[worker]172.28.0.3 ansible_user=root ansible_password=changeme172.28.0.4 ansible_user=root ansible_password=changeme172.28.0.5 ansible_user=root ansible_password=changeme```Playbook invocations:```ansible-playbook -l 172.28.0.5 playbooks/install.ymlansible-playbook -l 172.28.0.5 playbooks/push-configs.ymlansible-playbook -l 172.28.0.5 playbooks/start.yml```Specifying the host is optional and can be omitted since the playbooks can runwithout side effects if the desired state is already reached. As a result youcan add a number of workers and then just install, push configuration and startthem all together.**Scaling down*** Stop the worker or perform a graceful shutdown.  ```  # Hard stop  ansible-playbook -l 172.28.0.5 playbooks/stop.yml  # Graceful shutdown  ansible-playbook -l 172.28.0.5 playbooks/graceful-shutdown.yml  ```* Optionally run [uninstall](#uninstall) for the specific worker only  ```  ansible-playbook -l 172.28.0.5 playbooks/uninstall.yml  ```* Remove the worker from `hosts`## Configuring TLSYou can configure [TLS for the coordinator](../../latest/security/tls.html) aswell as [cluster internal authentication andTLS](../../latest/security/internal-communication.html) as usual.For the functionality of the playbooks you need to additionally update the portsconfigured in `vars.yml` so that they can continue to interoperate with thenodes for status checks and other aspects.Using 8443 on the coordinator and workers:```coordinator_port: 8443worker_port: 8443```## Using secretsYou can use [secrets in environmentvariables](../../latest/security/secrets.html) to avoid clear text password andother sensitive data in the configuration files.To inject a secret value into an environment variable, you can modify the`env.sh` shell scripts in `files/worker` and `files/coordinator`. The script isexecuted before {{site.terms.sep}} is started. For example, you can insert codeto retrieve secret values from a secret manager such as [Hashicorp Vault](https://www.hashicorp.com/products/vault),[Keywhiz](https://github.com/square/keywhiz), or a secret manager from yourcloud provider."
 },
 {
  "title": "Tuning your cluster performance",
  "url": "/platform-administrator/performance-tuning.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is a more feature-rich version of{{site.terms.oss_first}} providing enhanced query performance, security,connectivity, and ease of use.Learn how to size your cluster and the machines in it to ensure the bestperformance possible for your workload in this training video presented by oneof our founders, Dain Sundstrom. For your convenience, we&#39;ve divided the videotraining course up into topic sections, and provided links to the relevant partsof our documentation below.## General tuning strategy &amp; baseline advice            {% include youtubeSnippet.html id=page.youtubeId1 start=346 end=896 %}              Topics:              Starting big        Stabilizing, then tuning        Options to disable            Running time: ~9 min.      ## Cluster sizing, and how {{site.terms.sep}} uses CPU and memory resources                 {% include youtubeSnippet.html id=page.youtubeId1 start=897 end=2056 %}              Topics:              How memory affects JOIN, GROUP BY, ORDER BY and window functions        Availability        Concurrency            Running time: ~19 min.      ## Machine sizing and its impact             {% include youtubeSnippet.html id=page.youtubeId1 start=2086 end=4393 %}              Topics:              Memory and memory allocation        Shared join hash        Distributed join        Skew        Machine sizes and types        Spilling        Small clusters            Running time: ~38 min.      Additional resources on memory management and spilling in {{site.terms.sep}}:* [Memory management properties](../latest/admin/properties-memory-management.html?highlight=memory%20allocation)* [Spilling properties](../latest/admin/properties-spilling.html)* [Spill to disk](../latest/admin/spill.html)* [JVM Settings](../latest/admin/tuning.html)## Tuning the workload            {% include youtubeSnippet.html id=page.youtubeId1 start=5025 end=5965 %}              Topics:              Query plan        Precomputing        Connectors            Running time: ~16 min.      ## Hive data organization             {% include youtubeSnippet.html id=page.youtubeId1 start=5981 end=6936 %}              Topics:              Organize your data for the Hive connector        Hive partitioning and bucketing        ORC and Parquet        File size        Bad parquet files        Rewrite table with the ORC writer            Running time: ~16 min.      ## Making queries faster             {% include youtubeSnippet.html id=page.youtubeId1 start=6937 end=7713 %}              Topics:              What to look for in a query        Using more hardware        Underutilization        Hive caching            Running time: ~13 min.      For more in-depth information on this topic, watch our [query optimizationtraining video](../../data-engineer/query-performance.html).## Sharing resources, and resource groups             {% include youtubeSnippet.html id=page.youtubeId1 start=7714 end=7899 %}              Topics:              Concurrency        User experience, expectations and satisfaction        Social engineering            Running time: ~3 min.      "
 },
 {
  "title": "Personas used for documentation and design",
  "url": "/internal/personas.html",
  "content": "# {{ page.title }}There are three primary audiences we write and design for:- Starburst platform administrators- Data engineers- Data consumers: analysts &amp; scientistsThere is a single, secondary audience: &quot;Data leaders.&quot; This persona is largely acheck writer for our purposes (VP, CDO, CIO), and does not actually use SEP anydifferently than the primary personas. They are included here for completeness,and should be kept in mind when creating marketing content such as case studies,white papers and ROI-focused materials.This document describes these audiences as personas - fictional amalgamations -that you can empathize with and solve problems on behalf of. In practice,product personas are given names, faces and backgrounds to aid in discussingtheir needs as if they were real people, representative of our customers.## Primary personas### Data consumers - analysts and scientistsData analysts and scientists will approach Starburst in very similar ways.However, their backgrounds and skill sets are different, so we will separatethose out. Their pain points will be treated together.              Chris Consumer    Early career    BSc in physics from University of Toledo, currently working on online MBA    Chris is a Business Analyst. He&#39;s responsible for delivering    visualizations and reports to ensure that his leadership is making    well-informed, data-driven decisions. Chris cares very deeply that not only    are the right questions being asked (and answered), but that the right    data is being used to answer the questions. With the wealth of data    available, it can be easy to overlook and misuse data. The quality of    Chris&#39;s work ultimately rests on the quality and reliability of the data he    uses, so Chris keeps good working relationships with his data engineering    team and often communicates discrepancies and SLAs issues to them. Chris has    some solid SQL chops and is often able to prototype a new data source to be    productionalized by data engineers. Chris feels that he has just the right    combination of technical skills and business acumen.  When you write, design and build for Chris, here are some of the skillsets you can expect him to have:* A reasonable level of skill with SQL, with some knowledge of more advanced  queries* Limited programming skills and methodologies* Tells stories with data* Expert with data visualization tools* Excellent spreadsheet skills, including some modeling* A good ability to detect and articulate issues with data, even if he cannot  remedy them or trace the cause              Cameron Consumer    Early career    PhD in statistics, Stanford    Cameron is a data scientist. She&#39;s responsible for creating data models    and that forecast and describe the business. Cameron worries about the impact    of seasonality on sales, and feels compelled to deliver models that    reflect that impact with a high degree of accuracy. Cameron feels like she    brings the answers to &quot;Why?&quot; and &quot;How?&quot; to the table. Her machine learning    models help her find the levers that the business can pull - the &quot;how,&quot;    and her models account for why the business behaved as it did, or will.    She feels more like an academic than an engineer, and is very proud of her    scientific approach to business. Her digital sales data knowledge is    formidable, and her reputation as an SME ensures that she has a robust    stream of opportunities in her field.  When you write, design and build for Cameron, here are some of the skillsets you can expect her to have:* A reasonable level of SQL skills, with some knowledge of more advanced queries* Reasonable programming skills* Expert in statistical methods and/or machine learning* Some understanding of code repositories* Competence with data visualization tools* A good ability to detect and articulate issues with data, even if they cannot  remedy them or trace the causeCameron&#39;s and Chris&#39;s pain points include, in no particular order:* Having to retrofit tools onto multiple data sources* Long waits for ETL to deliver useable data* Can&#39;t dive into data quality issues* Data engineers sometimes kill their queries because of resource contention* Complex, periodic reports and models are often delayed past due dates### Data engineers                Donna Data Engineer      Mid-career      BSc in computer engineering, University of Illinois at Chicago      Donna Data Engineer is responsible for designing performant data      sources that can answer a broad range of business questions at XYZ, Inc.      Donna found her way to data engineering through internships in college; it      felt like a good blend between the technical chops required for      programming jobs, and the big picture, organizational nature of data that      she is naturally drawn to. As part of her job, she must understand what      data is currently available from what sources, and what new data is needed      to fill in any gaps. Donna has to work with stakeholders to source that      new data, be it from third parties or through new log entries, message      streams or product endpoints. Donna works pretty closely with data      analysts and scientists, and tries to anticipate their needs in order to      keep up with burgeoning data demands.                  Daniel Data Engineer      Mid-career      BSc in computer science, University of New Hampshire      Daniel Data Engineer is responsible for delivering data to data      analysts and data scientists at Acme Corp. Up until a few years ago,      this mostly entailed writing complex ETL in frameworks such as Informatica      and Alteryx. Over the last few years, he&#39;s worked mostly in python-based      frameworks such as Airflow and Bonobo as well as diving into Apache Spark.      Daniel really cares about data landing times, because him and his      coworkers hear from PagerDuty way more than they would like to.   When you write, design and build for Donna and Daniel, here are some of theskill sets you can expect them to have:* Creating and monitoring pipeline health metrics to ensure SLAs are met.* Enabling automated self-service pipelines using Infrastructure as Code (IaC)* Design schemas, data lake and data warehouse solutions in collaboration with  stakeholders.* Building and managing Kafka-based streaming data pipelines* Building and managing Airflow- and Spark-based ETLs* Creating and updating data models &amp; data schemas that reduce system complexity  and cost, and increase efficiency* Preparing and cleaning data for prescriptive and predictive modeling and  descriptive analytics* Identifying, designing, and implementing internal process improvements such as  automating manual processes, optimizing data delivery for greater scalability* Creating data tools for analysts and data scientists* Building data integrations between various 3rd party systems such as Salesforce  and WorkdayDonna&#39;s and Daniel&#39;s pain points, in no particular order:* Keeping up with the changing landscape of data delivery technology* Managing SLAs for data pipelines in environments where the data growth rate  and complexity constantly increases, data pipeline and platform performance* Aligning and negotiating with upstream data sources and infrastructure SLA  owners* Sussing out detailed data requirements from folks with a wide range of data  knowledge* Long, brittle pipelines* Productionalizing non-performant analyst queries* Constantly responding to resource constraint issues* Designing ETL around siloed data* Data cleansing### Platform administrators              Art Administrator    Late career    BSc in computer science, BYU    Art Administrator is responsible for XYZ, Inc&#39;s Starburst cluster. He was    an SRE for the data team for years, and switched roles to platform    engineering after leading the SREs for a bit. Art really cares about    scalability and reliability, especially since XYZ has super aggressive    SLAs both on data landing times and of course availability. Art works    closely with his colleagues in IT to ensure that his systems adhere to    XYZ&#39;s strict access policies and support audit requirements.                   Ada Administrator      Late career      BSc in computer science, University of Washington      Ada Administrator is responsible for both Acme Corp&#39;s Starburst and      Postgres clusters. Ada was a DBA from early to mid-career, and it fell to      her at Acme to figure out the HDFS ecosystem when it came along. Now she      builds and maintains big data clusters for a living. Ada cares a lot      about the using right data platform for the data.  When you write, design and build for Art and Ada, here are some of the skillsets you can expect them to have:* Building and maintaining scalable data platform architectures to support the  ingest, storage and querying of large heterogenous datasets* Creating and monitoring cluster health metrics to ensure optimal performance  and reduce any downtime* Writing clean, production-ready code (in Java, Go etc.) with a strong focus on  quality, scalability and high performance* Using and building scalable asynchronous REST API’s* Working with cloud providers like AWS, Azure and Google Cloud* Implementing and working with persistence technologies like AWS S3, HDFS,  Kafka and ElasticSearch* Designing for data integrity and security through all environments as well as  the data lifecycle* Partnering with data engineers to enable automated self-service pipelines  using Infrastructure as Code (IaC)* Partnering with data engineers to design and improvement schemas, data lake  and data warehouse solutions in collaboration with stakeholdersArt&#39;s &amp; Ada&#39;s pain points, in no particular order:* Sorting through an overload of information to master complex data platforms* Ensuring data platforms can scale to demand and with growth* Architecting solutions that can provide disaster recovery and business  continuity for complex, critical data systems, in conjunction with IT  stakeholders* Assisting in managing budgets and licensing cycles for massive  enterprise-scale software vendors, bandwidth and hardware leases* Constantly tackling inherently complex and highly-visible tasks* Delivering against stringent infrastructure SLAs* Doing more with less, or at least the same team size* Implementing data governance requirements for all data systems## Secondary persona - data leader              Lauren Leader    Mid-to-late career    MBA, Haas School of Business    Lauren is CIO at the newly IPO&#39;ed Clouds &#39;R Us. She&#39;s responsible for    data infrastructure, data governance and delivery, as well as    enabling SOX, GDPR and CCPA compliance. Prior to stepping into her current    role, Lauren was a VP of IT at Acme Corp., where she owned the budget for    all data infrastructure. She calls this her &quot;real-life MBA,&quot; because she    learned the hard way from being caught off-guard by explosive growth in    under-specified legacy systems in multiple budget cycles. Lauren is also    sensitive to scaling, platform lock-in, and staffing around particular    technologies.  When you write for Lauren, here are some of her pain points to keep in mind, inno particular order:* Constantly fighting Shadow IT, up to and including small, narrow-scope  one-off data warehouse solutions which she inevitably must absorb* Architecting around legacy systems, particularly monolithic services* Changing regulatory climate* Staffing for innovation while keeping legacy systems running and trying to  automate* Managing, defending and demanding a budget with rapid growth* Balancing buy-vs-build, including for contracting services* Balancing private cloud vs hosted cloud solutions for cost-effectiveness,  regulatory compliance and security"
 },
 {
  "title": "Starburst Admin playbook reference",
  "url": "/starburst-enterprise/starburst-admin/playbook-reference.html",
  "content": "# {{ page.title }}{{site.terms.sbadmin}} includes numerous playbooks to perform specific actions.You can use them individually or in sequence to satisfy the needs of yourspecific use case for [installing](./installation-guide.html) and[operating](./operation-guides.html).The following playbooks are available:* [Install cluster](#install)* [Push configurations](#push-configs)* [Start cluster](#start)* [Stop cluster](#stop)* [Restart cluster](#restart)* [Check service status](#check-status)* [Graceful worker shutdown](#graceful-shutdown)* [Rolling worker restart](#rolling-restart-workers)* [Collect logs](#collect-logs)* [Uninstall cluster](#uninstall)## Install clusterThe `install` playbook installs the RPM or tarball package on all defined hosts.You are required to place the `tar.gz` or `rpm` file in the `files` directoryand specify the `version` in `vars.yml` to run the playbook:```shellansible-playbook playbooks/install.yml```Errors occurs if RPM and tarball are found or if version values mismatch.Alternatively to the archive in the `files` directory on the control machine,you can set the `installer_url` in `vars.yml` to point to HTTP or HTTPS URL ofthe  `tar.gz` or `rpm`. Comment out the `installer_file` in `vars.yml`. Ansiblethen downloads the binary from the URL directly on the hosts. This approach ismore complex to set up, but scales better since all hosts can download from theURL in parallel. The hosts need to be able to contact the specified URL.The playbook verifies the availability of the required Java runtime on the host,starting first at the value of `JAVA_HOME` and then looking at commoninstallation paths. It uses the script `files/find_java.sh` and fails if noruntime is found.The playbook installs the RPM or unpack the tarball and create the necessarydirectories specified via `vars.yml`.Ansible can operate as a non-root user, but for some operations, it requiresroot privileges. By default, `sudo` commands are used to elevate privileges.Since `sudo` prompts for the user password, run every `ansible-playbook` commandwith the `--ask-become-pass` parameter as detailed in the [become commanddocumentation](https://docs.ansible.com/ansible/latest/user_guide/become.html#become-command-line-options).The RPM installation automatically adds a dedicated system user to run{{site.terms.sep}} as a service. This user owns all configuration files, whichshould not be readable by other users on the hosts. The tarball installationuses the `installation_owner` and `installation_group` defined in `vars.yml`.The install playbook create the user and group.##  Push configurationsThe `push-configs` playbook generates the configuration files for thecoordinator and workers and distributes them to all hosts.The following resources are used to create the set of configuration files:* `files/vars.yml` - variable definitions to use in the configuration files that  are templatized with Jinja2. These files use the extension `.j2`. Ansible uses  the set values and replaces the variable placeholders. For example, the  placeholder `{{ node_environment }}` is replaced with the value `production`  from  `node_environment: production`.* `files/coordinator` - the configuration files for the coordinator* `files/worker` - the configuration files for all the workers* `files/catalog` - the [catalog properties files to define the connection to  any data sources](../../data-engineer/catalogs.html), required on the coordinator  and all workers.* `files/extra/etc` - additional files that are placed in the directory  specified by the `etc_directory` variable on all hosts, typically properties  files for event listener, security or similar configuration* `files/extra/lib` - additional files that are placed in the `lib` directory  all hosts, typically binary files or configuration files that are added to the  servers classpath such as custom user defined function implementations* `files/extra/plugin` - additional files that are placed in the `plugin`  directory all hosts, typically complete directories with binaries used for a  custom developed plugin such as a security extension or another connectorAny changes, including deletion of catalog files is synchronized to the hosts.Starburst Admin automatically uses the folder name `starburst` for {{site.terms.sep}}deployments, `trino` for {{site.terms.oss}} deployments, and `presto` for Prestodeployments.Other file deletions are not synchronized but can be performed with the [Ansiblefilemodule](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/file_module.html).For example, if you made a file `files/extra/etc/foo.sh`, the file is copiedinto `/etc/starburst/foo.sh` on the hosts. You can use the followingcommand to delete it:```ansible all -m file -a &quot;path=/etc/starburst/foo.sh state=absent&quot;```The supported files and configuration methods are [identical to the generalconfiguration files and properties from SEP](../../latest/index.html) for theidentical version you deploy with {{site.terms.sbadmin}}.After you created or updated all the configuration as desired, you can run theplaybook with the following command:```shellansible-playbook playbooks/push-configs.yml```Keep in mind that most of the configuration files need to be distributed to allnodes in the cluster and that they need to be identical for all workers.The best approach to apply any new configuration involves the following steps:* Ensure no users are active and [stop](#stop) the cluster.* Alternatively [shut it down gracefully](#graceful-shutdown).* Update the configuration files.* Push the configuration.* [Start](#start) the cluster.* Verify the changes work.## Start clusterThe `start` playbook start the coordinator and all worker processes on thehosts. It starts {{site.terms.sep}} the `service` command for RPM-basedinstallations or with the `launcher` script for tarball installation.You need to [install the cluster](#install) and [push theconfiguration](#push-configs) before starting the cluster the first time.```ansible-playbook playbooks/start.yml```The playbook relies on the hosts to be up and running and available to Ansible.When restarting the hosts on the operating system or hardware level, theRPM-based installation automatically starts the {{site.terms.sep}} processes.The tarball installation does not start automatically. You can however configureit to perform the start by using the `launcher` script as daemon script. Referto the documentation of your used Linux distribution for details.##  Stop clusterThe `stop` playbook stops the coordinator and all worker processes on the hosts.It does not take into account if {{site.terms.sep}} actively processes anyworkload, and simply terminates.```shellansible-playbook playbooks/stop.yml```You can use the [graceful shutdown](#graceful-shutdown) as an alternative.##  Restart clusterThe `restart` playbook stops and then starts the coordinator and all workerprocesses on the hosts.```shellansible-playbook playbooks/restart.yml```It is equivalent to running the `stop` and the `start` playbook sequentially.##  Check service statusThe `check-status` playbook checks the status of the the coordinator and allworker processes and displays the results.```shellansible-playbook playbooks/check-status.yml```The playbook uses the `init.d` script for the RPM-based installation or the`launcher` script for a tarball installation to get the status of each service.If the service is running, you see a log for each address in your inventory filestating `Running as `:```TASK [Print status] ********ok: [172.28.0.2] =&gt; {    &quot;msg&quot;: &quot;Running as 1965&quot;}ok: [172.28.0.3] =&gt; {    &quot;msg&quot;: &quot;Running as 1901&quot;}ok: [172.28.0.4] =&gt; {    &quot;msg&quot;: &quot;Running as 1976&quot;}```If a service is not active, you see `Not running`:```TASK [Print status] ****ok: [172.28.0.2] =&gt; {    &quot;msg&quot;: &quot;Not running&quot;}ok: [172.28.0.3] =&gt; {    &quot;msg&quot;: &quot;Not running&quot;}ok: [172.28.0.4] =&gt; {    &quot;msg&quot;: &quot;Not running&quot;}```##  Graceful worker shutdownThe `graceful-shutdown` playbook stops the worker processes after all tasks arecompleted.```shellansible-playbook playbooks/graceful-shutdown.yml```Using [graceful shutdown](../../latest/admin/graceful-shutdown.html) takeslonger than using the `stop` playbook, because it allows workers to complete anyassigned work. If all workers are shut down, no further query processing isperformed by the cluster. The coordinator remains running at all times, untilmanually shut down.##  Rolling worker restartThe `rolling-restart-workers` playbook stops and starts all worker processessequentially one after the other using a [graceful shutdown](#graceful-shutdown)and a new start.```shellansible-playbook playbooks/rolling-restart-workers.yml```You can configure the following variables in `files/vars.yml` to managegraceful shutdowns:* `graceful_shutdown_user` - user name to pass to the `X-Presto-User` or  `X-Trino-User` header when issuing the graceful shutdown request via the HTTP  API* `graceful_shutdown_retries` -  number of times to check for successful  shutdown before failing* `graceful_shutdown_delay`- inactive duration between shutdown status checks* `rolling_restart_concurrency` - number of workers to restart at the same timeBy default, the playbook waits up to 10 minutes for any individual worker tostop and start. Each operation has a 10 minute timeout. If this timeout isreached, then the playbook execution fails. If you have a longer shutdowngrace period configured, you may want to extend this timeout.Keep in mind that this playbook does not change any configuration of theworkers. If you push configuration changes to the cluster before a rollingrestart, the cluster can be in an inconsistent state until the restart iscompleted. This can lead to query failures and other issues. A simple additionof a catalog file is possible. The new catalog only becomes usable after allworkers are restarted. Updates to catalog and other configuration filestypically result in problems.## Collect logsThe `collect-logs` playbook downloads the log files from all hosts to thecontrol machine.```shellansible-playbook playbooks/collect-logs.yml```The server, HTTP request, and launcher logs files from each host are copied into`logs/coordinator-` for the coordinator or `logs/worker-`for the workers in the current directory.## Uninstall clusterThe `uninstall` playbook removes all modifications made on the hosts by otherplaybooks. [Stop the cluster](#stop) before running the playbook.```shellansible-playbook playbooks/uninstall.yml```The playbook deletes all data, configuration and log files. It also removes thedeployed binary packages and the created user accounts and groups."
 },
 {
  "title": "Microsoft Power BI",
  "url": "/data-consumer/clients/powerbi.html",
  "content": "# {{ page.title }}You can use the popular analytics platform [Microsoft PowerBI](https://powerbi.microsoft.com/) with your {{site.terms.sb}} cluster,using Power BI DirectQuery.  {{site.terms.sep}} can be accessed via DirectQuery with the following tools:* Microsoft Power BI Desktop* Microsoft Power BI ServicePower BI lets {{site.terms.sep}} perform all queryprocessing. This combines the scalability and power of {{site.terms.sep}} withthe reporting features of Power BI.## Requirements* Power BI 2.87.720.0 and higher. Older versions of Power BI only support import  mode.* The [OBDC driver](./odbc.html) for {{site.terms.sep}}.* A `.lic` license file for the {{site.terms.sep}} ODBC driver. Install this  file in the `lib` directory of the driver&#39;s program files.{% include note.html  content=&quot;Power BI in DirectQuery and Import modes require version 1.2.14 of  the [ODBC driver](./odbc.html). To use the ODBC driver version 2.0.0 or later,  use a [standard ODBC connection](#standard-odbc).&quot;%}## Data connectivity modesPower BI can access data from {{site.terms.sep}} in the following connectionmodes:* **DirectQuery**: query data using the {{site.terms.sep}} query engine without  importing that data to the Power BI client. This mode is recommended for most  use cases as it has the least impact on network or client storage resources,  regardless of the amount of data queried.* **Import**: import queried data directly to the client machine, for further  local analysis with tools like quick insights and calculated tables. Note that  import mode is limited to 1GB of data per query.* **Standard ODBC**: use a generic ODBC connection. Allow for custom SQL which  is unsupported by DirectQuery. Note that a standard ODBC connection is limited  to 1GB of data per query.## Connect with Power BI DesktopThe following sections describe how to connect to {{site.terms.sep}} fromPower BI Desktop.### DirectQuery and import modeTo connect to {{site.terms.sep}} from Power BI Desktop using either DirectQueryor import mode, follow these steps:1. In Power BI Desktop, select **Get Data** &gt; **More**.2. Select **All** &gt; **Starburst Enterprise** and click **Connect**.  &amp;nbsp; {% include image.html  url=&#39;../../assets/img/general/power-bi-sb-source.png&#39;  img-id=&#39;Data source selection&#39;  alt-text=&#39;Selecting Starburst Enterprise as a data source&#39;  descr-text=&#39;Image showing Starburst Enterprise selected as a data source&#39;  pxwidth=&#39;500&#39;  screenshot=&#39;true&#39;  %}3. Configure the necessary [connection and authentication details](./index.html#basic-connection-information)  to access your cluster. You must include a port number, which your network  administrator can provide.4. Select either **DirectQuery** or **Import** as your **Data Connectivity**  mode.5. Click **OK**.6. Select an authentication method and click **Connect** to establish a  connection.7. After the connection is established, use the **Data Navigator** to  browse catalogs, query data sources and more.### Standard ODBCThe standard ODBC connection mode requires you to add the {{site.terms.sep}}ODBC driver to Windows as an ODBC data source. To set up the data source, followthese steps:1. Open the **ODBC Data Source Administrator** utility.2. Add the {{site.terms.sep}} ODBC driver as a **User** or **System Data  Source**.3. Click **Apply**.To connect to {{site.terms.sep}} from Power BI Desktop using a standard ODBCconnection, follow these steps:1. In Power BI Desktop, select **Get Data** &gt; **More**.2. Select **All** &gt; **ODBC** and click **Connect**.  &amp;nbsp; {% include image.html  url=&#39;../../assets/img/general/power-bi-odbc-source.png&#39;  img-id=&#39;Data source selection&#39;  alt-text=&#39;Selecting ODBC as a data source&#39;  descr-text=&#39;Image showing ODBC selected as a data source&#39;  pxwidth=&#39;500&#39;  screenshot=&#39;true&#39;  %}3. Under **Data source name (DNS)**, select the {{site.terms.sep}} data source.4. Under **Advanced options** &gt; **Connection string** add ``host=&quot;hostname:port&quot;``,  replacing &quot;hostname:port&quot; with the [connection details](./index.html#basic-connection-information)  for your cluster.5. Configure the necessary [authentication details](./index.html)  for your cluster.7. After the connection is established, use the **Data Navigator** to  browse catalogs, query data sources and more.### Authentication and SecurityPower BI Desktop always tries to connect with an encrypted connection first. Ifyou are connecting without TLS/SSL, the connector offers the option to connectusing an unencrypted connection afterwards.To use a TLS-encrypted connection with your cluster, make sure the serveruses a globally trusted certificate.If this is not the case, add the server’s certificate to thesystem trust store (**Certificates** &gt; **Trusted Root CertificationAuthorities**) ​before connecting. The certificate can be added for the machine,or for each user running the Power BI connector.  In many organizations thisis handled automatically as part of the operating system and browserconfiguration.### LDAP authentication with Power BI DesktopIf your cluster is configured to use LDAP authentication, select **LDAP** in theauthentication window when connecting and provide your username and passwordcredentials.### Kerberos authentication with Power BI DesktopTo use Kerberos authentication, Kerberos must be installed for the user andinitialized using ``kinit``, before using the driver. This establishes your usercredentials on the machine.Select **Kerberos** in the authentication window, when connecting, and providethe Kerberos **Service name**.## Connect with Power BI serviceUsing the web-based Power BI service requires you to have the [on-premises datagateway](https://powerbi.microsoft.com/en-us/gateway/), and the{{site.terms.sep}} ODBC driver added to that installation with the appropriatepermissions.To add the ODBC driver, follow these steps:1. Copy the ODBC driver file into `%USER%DocumentsPower BI  DesktopCustom Connectors` for each user. Create the directory, if it does not  exist.2. Grant the gateway service account permissions to access the  `Custom Connectors` directory from the previous step.3. Update the directory properties in the **Security - Advanced** tab. Add  access that grants **Basic Permissions - Full Control** to  **Everyone/Authenticated Users**. Alternatively, you can also create a new  group and add the gateway service account to a new group.4. In the **Connectors** configuration of the gateway, update the value for  **Load custom data connectors from folders** to point at the  `Custom Connectors` directory.5. Log in to Power BI Online and update the **Gateway Cluster Settings**. Enable  **Allow user&#39;s custom data connector to refresh through this gateway  cluster**.To connect to {{site.terms.sep}} as a data source, follow these steps:1. Log in to the Power BI service.2. Navigate to **Setting** &gt; **Manage gateways**.3. Select the gateway, and **Add data source**.4. Set the **Data Source Type** to **Starburst Enterprise**.5. Configure the necessary [connection and authentication details](./index.html#basic-connection-information)  for your cluster. You must include a port number, which your network  administrator can provide.6. Click **Add** to create the {{site.terms.sep}} data source.## Limitations* Self-signed certificate usage for TLS/SSL connections is not supported.* Writing and using custom SQL statements is not supported with DirectQuery.  The ODBC standard connector can be used for custom SQL but does not support  direct querying.* Authentication type and field name customization does not apply on the Power  BI service. The following is the mapping of the service field names to their  Desktop counterparts:  Service name      | Desktop name  ----------------- | -----------------------  Basic             | LDAP  Key               | Kerberos  Key: Account Key  | Kerberos: Service Name## Release notes### Version 2.0.0April 2021:* Remove beta flag.* Change name to **Starburst Enterprise** as breaking change. All existing  reports break, and all users need to recreate their queries."
 },
 {
  "title": "Create a private connection for a data source",
  "url": "/starburst-galaxy/data-sources/private-connections.html",
  "content": "# {{ page.title }}You need to create a private connection in your AWS account to enable in{{site.terms.galaxy_full}} to connect to most data sources. Each data sourcelists the specific requirements.In the AWS account that you use for {{site.terms.galaxy_full}}, you need thefollowing to configure a private connection:* The VPC ID of the {{site.terms.galaxy_full}} VPC k8s cluster* The VPC ID of target data source (MySQL or PostgreSQL)* A peering connection* Route entries in your {{site.terms.galaxy_full}} AWS account for connectivity   to the target data source* Route entries in your target data source account for connectivity to   {{site.terms.galaxy_full}}## VPC ID from {{site.terms.galaxy_full}} VPC k8s clusterIn your {{site.terms.galaxy_full}} AWS account, locate the VPC ID from your{{site.terms.galaxy_full}} VPC k8s cluster, `starburst-main-vpc`.1. To search for the VPC, click **Your VPCs** on the left menu.2. Select the row for `starburst-main-vpc`.3. Copy the **VPC ID** and store it in accessible, safe place.## VPC ID of target data sourceFind your data source and locate the VPC ID in AWS account of the data source.1. Search for **EMR** and change to the appropriate region.2. Click **Clusters** on the left menu and select **your cluster**.3. From the **Summary** tab under **Network and hardware**, select the **Subnet ID**.4. Copy the **VPC ID** and store it in accessible, safe place.## Create a peering connectionCreate a peering connection in your {{site.terms.galaxy_full}} AWS account.1. Search for **VPC**.2. Click **Peering connections** on the left menu and select **Create peering connection**.3. Enter a peering name tag (example: `galaxy-mysql-vpcx`).4. From the **VPC Requestor** dropdown menu, select the **k8s cluster VPC ID**.5. Choose the **account** and **region** for your target data source.6. From the **VPC Acceptor** dropdown menu, select the **data source VPC ID**.7. Copy the **VPC (Requester)** and **VPC (Acceptor)** **CIDR blocks** and store it   in accessible, safe place to use in the next process.8. Click **Create Peering Connection**, then click **OK**.You need to accept the peering connection in the target data source:1. Go to the AWS account that has the target data source VPC.2. Search **VPC** and select **Peering connections** from the left menu.3. Locate the **peering connection** name and right-click **Pending Acceptance**   status.4. Click **Accept Request**.5. Use the **Close** button to exit.## Add route entries to the {{site.terms.galaxy_full}} AWS accountAdd route entries to your {{site.terms.galaxy_full}} AWS account to connectto your data sources.1. From your {{site.terms.galaxy_full}} AWS account, search for **VPC**.2. Click **Subnets** from the left menu.3. Enter **starburst-main-vpc** in **Filter subnets** and click **Enter**.4. Copy the two **private subnet IDs** labelled.   `starburst-main-vpc-{az1}-private0` and `starburst-main-vpc-{az2}-private0`.5. Click **Route tables** on left menu.6. Enter the first **subnet ID** in **Add filter** and click **Enter**.7. Enter the second **subnet ID** in **Add filter** and click **Enter**.8. Select the first routing table from the list and click **Edit routes**.9. Click **Add route**.10. In **Destination**, enter the **CIDR** of the **VPC (Accepter)** for the    targeted data source.11. In the **Target** dropdown menu, choose **Peering connection** and select    the peering connection name tag.## Add route entries to the target data source accountAdd route entries to the target data source account for connectivity to{{site.terms.galaxy_full}}.1. From your data source account, search for **VPC**.2. Click **Route tables** from the left menu.3. In **Add filter**, paste the target data source **VPC ID** and click   **Enter**.4. Select the first routing table from the list.5. Under the **Routes** tab, click **Edit routes**.6. Click **Add route**.7. In **Destination**, enter the **CIDR** of the **VPC (Requestor)** for the   {{site.terms.galaxy_full}} AWS account.8. In the **Target** dropdown menu, choose **Peering connection** and select the   peering connection name tag.9. Click **Save Changes** to complete.## Next stepsThe private connection is now established and you can continue to configure thespecific data source connection."
 },
 {
  "title": "Optimizing query performance",
  "url": "/data-engineer/query-performance.html",
  "content": "# {{ page.title }}{{site.terms.sep_first}} is fast. But did you know that there are stillmany opportunities to make it even faster depending on how you write yourqueries?Learn how to use `EXPLAIN` and `ANALYZE` to improve your query performance inthis training video presented by one of our founders, Martin Traverso. For yourconvenience, we&#39;ve divided the video training course up into topic sections, andprovided links to the relevant parts of our documentation below.## The query lifecycleKnowing what&#39;s happening under the hood in SQL can help you to write queriesthat capitalize on possible optimizations and avoid approaches that will costyou performance. This section provides an overview of what happens as a query isexecuted.             {% include youtubeSnippet.html id=page.youtubeId1 start=491 end=1217 %}              Topics:              Parsing        Analysis        Planning        Optimization        Scheduling and execution            Running time: ~12 min.      ## The EXPLAIN statement in detailIf you want to understand what the {{site.terms.sep}} engine is basing its decisions onas it executes a query, you need to use the `EXPLAIN` statement. This sectionwalks you through this very informative tool in detail.             {% include youtubeSnippet.html id=page.youtubeId1 start=1218 end=2469 %}              Topics:              EXPLAIN        EXPLAIN vs EXPLAIN ANALYZE        Fragment            structure, distribution, row layout, estimates, and performance stats in EXPLAIN ANALYZE        Exchanges            Click the links to read more on that topic in our reference manual.      Running time: ~20 min.      ## General optimizationsThe content in this section is more technique-oriented, and is a complexsubject. We strongly suggest watching it all the way through thoroughly first togain a broad awareness of how you write a query can affect its performancebefore trying these on your own. For further reading, we recommend our[pushdown](../latest/optimizer/pushdown.html)documentation.The SQL engine relies on table statistics to make decisions on optimizations.Enabling dynamic filtering can take optimizations even further. We recommendreading about these powerful features to ensure you are getting the bestperformance possible out of your cluster:* [Dynamic filtering](../latest/admin/dynamic-filtering.html)* [Table statistics](../latest/optimizer/statistics)                 {% include youtubeSnippet.html id=page.youtubeId1 start=2470 end=5940 %}              Topics:              Constant folding        Predicate pushdown        Predicate pushdown into the Hive connector        Hive partition pruning        Hive bucket pruning        Row group skipping for ORC and Parquet        Limit, partial limit, and aggregation pushdown        Skew            Running time: ~58 min.      SEP offers [several properties to control how theoptimizer](../latest/admin/properties-optimizer.html) handles certainoperations.## Cost-based optimizationsThis section presents on overview of how cost-based optimizations work in {{site.terms.sep}},and provides great context for the following recommended reading:* [Cost-based optimizations](../latest/optimizer/cost-based-optimizations.html)* [Cost in EXPLAIN](../latest/optimizer/cost-in-explain.html?highlight=explain)             {% include youtubeSnippet.html id=page.youtubeId1 start=5941 end=6729 %}              Topics:              Partitioned and broadcast joins        Disabling cost-based optimizations        Join reordering        Table statistics        Computing statistics with ANALYZE            Running time: ~13 min.      "
 },
 {
  "title": "Starburst Admin release notes",
  "url": "/starburst-enterprise/starburst-admin/release-notes.html",
  "content": "# {{ page.title }}New releases of {{site.terms.sbadmin}} are created whenever significant newfeatures or bug fixes are available.## 1.2.0 (6 Aug 2021)* Add condition to status printing to detect what preliminary task was skipped* Add `OmitStackTraceInFastThrow` JVM flag* Setup limits in `/etc/security/limits.d` for tarball installation## 1.1.0 (11 Jun 2021)* Fix `installation_group` name issue* Remove `query.*` config property values in files to use automatic defaults* Move user documentation to [doc.starburst.io](https://docs.starburst.io/starburst-enterprise/starburst-admin/index.html)* Remove developer documentation from package## 1.0.0 (24 Mar 2021)* Initial supported release with full feature set to run production systems"
 },
 {
  "title": "Amazon S3",
  "url": "/ecosystems/amazon/s3.html",
  "content": "# {{page.title}}You can query data stored on the Amazon Simple Storage Service (Amazon S3) with{{site.terms.sep_first}}. The most common usage is with the [Hiveconnector](../../latest/connector/starburst-hive.html) and the dedicated[configuration options forS3-access](../../latest/connector/hive-s3.html).In addition, you need to store the meta data about the object storage. Typicallythis is done with [AWS Glue](./glue.html), but you can also use your own HiveMetastore Service (HMS).Amazon S3 can also be used as storage backend with other systems, and thenqueried with the dedicated connector:* [Delta Lake](../../latest/connector/starburst-delta-lake.html)* [Iceberg](../../latest/connector/iceberg.html)* [Snowflake](../../latest/connector/starburst-snowflake.html)Ensure the requirements for the connector are fulfilled.## RequirementsTo enable access to S3 with {{site.terms.sep_first}}, the following conditionsmust be met:* Your {{site.terms.sep}} cluster must have the necessary permissions needed to  access your S3 data.* Requirements of the connector used for the catalog you use to access S3 need  to be fulfilled. Typically this includes a meta store, either [AWS  Glue](./glue.html), or a HMS.## Ensuring {{site.terms.sep}} access to S3If your S3 data is publicly available, you do not need to do anything. However,S3 data is not typically publicly available, and you must grant{{site.terms.sep}} access to it.With the CFT support, you must [select an appropriate instanceprofile](../../latest/aws/requirements.html#aws-instance-profiles) when creating acluster.Validate that the selected instance profile is sufficient for {{site.terms.sep}}to read S3 data by opening an SSH connection to the coordinator and issuing thefollowing commands:```shell$ aws s3 ls s3://your-bucket/path/to/dataset/$ aws s3 cp s3://your-bucket/path/to/dataset/data-file - &gt; /dev/null```### Configuring a catalog and meta storeA [catalog must be created](../../data-engineer/catalogs.html) for your S3bucket.Users of the CFT can  [create a new HMS](../../latest/aws/metastore.html)via a CloudFormation template. It must have an instance profilegranting access to S3. Alternatively, you can use an existing HMS:1. Set the **MetastoreType** parameter to ``External Hive Metastore  Service`` and **ExternalMetastoreHost** to IP address of your HMS.Users of the [Kubernetes support(../../latest/k8s.html) can perform similarconfiguration.If the HMS uses authentication, review the Hive connector security[documentation](../../latest/connector/hive-security.html) to configure theconnector for your environment.## Reading data from S3If you chose an existing HMS instance when configuring the Hive catalog, chancesare that your S3 data is already mapped to SQL tables in the HMS. In that case,you should be able to query it immediately.If you created a new HMS instance, it is likely that your S3 data is not yetmapped. In the HMS you must provide the schema of the data, the file format, andthe data location. For example, if you have ORC or Parquet files in an S3bucket, ``my_bucket``, create a queryable table in the desired schema of yourcatalog.```shell  USE hive.default;  CREATE TABLE orders (       orderkey bigint,       custkey bigint,       orderstatus varchar(1),       totalprice double,       orderdate date,       orderpriority varchar(15),       clerk varchar(15),       shippriority integer,       comment varchar(79)    ) WITH (      external_location = &#39;s3://my_bucket/path/to/folder&#39;,      format = &#39;ORC&#39; -- or &#39;PARQUET&#39;    );```Query the newly mapped table as usual:```shell  SELECT * FROM orders;```## StatisticsIf your queries are complex and include joining large data sets, you may runinto performance issues. This is because {{site.terms.sep}} does not know thestatistical properties of the data necessary for the cost-based optimizer&#39;sdecisions.To gather table statistics, execute the following command:```shell  ANALYZE orders;```## Writing data to S3After configuring  {{site.terms.sep}} to read data from S3, you can write to it.If your HMS contains schema(s) mapped to S3 locations, you can usethem to export data to S3. If you don&#39;t want to use existing schemas (or thereare no appropriate schemas in the HMS), create a new one.```shell  CREATE SCHEMA hive.s3_export WITH (location = &#39;s3://my_bucket/some/path&#39;);```Once you have a schema pointing to a location where you want to export the data,export data using a ``CREATE TABLE AS`` statement that specifies yourdesired file format:```shell  CREATE TABLE hive.s3_export.my_table  WITH (format = &#39;ORC&#39;)  AS ;```Data is written to one or more files within the``s3://my_bucket/some/path/my_table`` namespace. The number of files depends onthe size of the data being exported and possible parallelization of the sourceof the data."
 },

 {
  "title": "Search",
  "url": "/searchresults.html",
  "content": "  {{page.title}}                                    Filter:                                                  "
 },
 {
  "title": "Create a secret in AWS",
  "url": "/starburst-galaxy/data-sources/secrets.html",
  "content": "# {{ page.title }}A secret consists of a set of credentials, user name and password, and theconnection details used to access a secured service.{% include note.html content=&quot;S3 requires that you create an [IAM policy](../data-sources/iam-policies-users.html) before you add a secret.&quot; %}## Create a secretCreate a secret in the AWS console or in the AWS CLI. Secrets must be saved with`starburst-galaxy-` as the prefix.### AWS consoleCreate a secret in the AWS console. You need the account credentialsto create a secret.1. Open the [AWS console](https://console.aws.amazon.com/secretsmanager/) and   navigate to Secrets Manager.2. Confirm or change your region if necessary.3. On either the service introduction page or the **Secrets list** page, choose   **Store a new secret**.4. Select **Other types of secrets**.5. Click the **Plaintext** tab and delete any existing characters in the   plaintext area. Enter your password (secret value). 6. Select **DefaultEncryptionKey** to encrypt your secret information.7. Click **Next**.8. Name the secret `starburst-galaxy-`, names must be lowercase and   include dashes and normal characters. Special characters and underscores are   not supported. It is important that you always use the `starburst-galaxy-`   prefix. 9. Navigate through the next screens and create the secret.### AWS CLICreate a secret from the AWS CLI. You need the account credentialsto create a secret.1. In the AWS CLI, access the account credentials.2. Run the following command:```    aws secretsmanager create-secret --region us--    --name starburst-galaxy-    --secret-string ```The output of the following command includes the ARN, name, and versionID."
 },
 {
  "title": "Securing Starburst Enterprise",
  "url": "/platform-administrator/security.html",
  "content": "# {{ page.title }}Learn how to safeguard your data with {{site.terms.sep_first}}&#39;s securitytoolkit in this training video presented by one of our founders, Dain Sundstrom.For your convenience, we&#39;ve divided the video training course up into topicsections, and provided links to the relevant parts of our documentation below.## Introduction             {% include youtubeSnippet.html id=page.youtubeId1 start=306 end=943 %}              Topics:              SEP security process        What to secure        Preparing: Verifying HTTP            Running time: ~11 min.      ## Client to server encryption             {% include youtubeSnippet.html id=page.youtubeId1 start=944 end=2096 %}              Topics:              Approaches for HTTPS, including proxies and load balancers        Adding SSL/TLS certificates        Handling PEM and JKS files        Verifying HTTPS for SEP            Running time: ~19 min.      ## Authentication and authorization in {{site.terms.sep}}             {% include youtubeSnippet.html id=page.youtubeId1 start=2097 end=4141 %}              Topics:              Password file authentication        LDAP authentication (See also: group providers)        Kerberos authentication (See also: passthrough)        Client certificate authentication        JSON Web Token authentication        Using multiple authenticators        Authentication with user mapping        Overview of authorization        File-based system access control            Running time: ~34 min.      ## Securing {{site.terms.sep}}&#39;s internal communications and management endpointsDocumentation for the material covered in this section is found[here](../latest/security/internal-communication.html).             {% include youtubeSnippet.html id=page.youtubeId1 start=4694 end=5608 %}              Topics:              Securing the Starburst cluster itself        Shared secret        Internal HTTPS        Secrets management        Management endpoints            Running time: ~16 min.      ## Hive catalog securityWe recommend the following additional reading, which covers enabling{{site.terms.sep}}&#39;s powerful role-based global access control:* [Access control overview](../latest/security/access-control.html)* [Global access control with Apache Ranger](../latest/security/global-ranger.html)* [Global access control with Privacera](../latest/security/global-privacera.html)* [Built-in system access control](../latest/security/built-in-system-access-control.html)While we strongly recommend implementing global access control, you can stillsecure Hive at the catalog level if your particular situation makes thatnecessary. Documentation covering the various options for securing Hive at thecatalog level can be found as follows:* [Configuring Hive security](../latest/connector/hive-security.html)* [Hive-level security with Apache Ranger](../latest/security/hive-ranger.html)* [Hive-level security with Privacera](../latest/security/hive-privacera.html)* [Hive-level security with Apache Sentry](../latest/security/hive-sentry.html)             {% include youtubeSnippet.html id=page.youtubeId1 start=5609 end=6650 %}              Topics:              Authorization        Metastore authentication        HDFS authentication        Kerberos debugging        S3 authentication        Google Cloud authentication            Running time: ~18 min.      "
 },
 {
  "title": "Sitemap",
  "url": "/sitemap.xml",
  "content": "            https://docs.starburst.io/universe-sitemap.xml              https://docs.starburst.io/latest/sitemap.xml              https://docs.starburst.io/356-e/sitemap.xml                https://docs.starburst.io/350-e/sitemap.xml                https://docs.starburst.io/345-e/sitemap.xml                https://docs.starburst.io/359-e/sitemap.xml                https://docs.starburst.io/358-e/sitemap.xml      "
 },
 {
  "title": "SQuirrel SQL Client",
  "url": "/data-consumer/clients/squirrel-sql.html",
  "content": "# {{ page.title }}[SQuirrel SQL](http://www.squirrelsql.org) is a Java-based graphical databaseclient that allows you to view the structure of your database, browse the datain tables, and issue SQL commands. The client is installed as alocal application on your workstation. You can use the client to access datasources from {{site.terms.sb}} clusters, since it supports the[JDBC driver](./jdbc.html).  ## ConfigurationFollow these steps to access your {{site.terms.sep}} cluster with SQuirrel SQL:1. Get the necessary [connection   information](./index.html#connection-information) for your cluster.2. Download the [JDBC driver](../../data-consumer/clients/jdbc.html#download-the-jdbc-driver).1. Copy the JDBC driver `.jar` into the desired directory.2. Start the SQuirrel SQL client.3. Add and configure the Trino JDBC driver:   a. Select **New Driver**.   b. Enter the following information in each field:    * **Name**: Trino    * **Example URL**: `jdbc:trino://host:port/catalog/schema`    * **Website URL**: `https://trino.io`   c. Select the **Extra Class Path** tab.   d. Click **Add** and navigate to the JDBC driver `.jar` you downloaded.   e. Click **List Drivers**.   f. Set the **Class Name** to `io.trino.jdbc.TrinoDriver`.   g. Click **OK**. Look for a success message in the client logs if the      driver setup was successful.      &amp;nbsp; {% include image.html      url=&#39;../../assets/img/general/squirrel-sql-driver.png&#39;      img-id=&#39;SQuirrel SQL driver setup&#39;      alt-text=&#39;SQuirrel SQL driver setup&#39;      descr-text=&#39;Image depicting SQuirrel SQL driver setup&#39;      pxwidth=&#39;500&#39;      screenshot=&#39;true&#39;      %}4. Create an alias for {{site.terms.sb}}:   a. Select **Add Alias**.   b. Enter the following information in each field:      * **Name**: A name for the connection to {{site.terms.sep}}      * **Driver**: Trino (as created during the driver configuration steps)      * **URL**: `jdbc:trino://host:port`. You can optionally specify a        catalog and schema in this URL so you do not have to fully qualify        table names in queries. Replace `host:port` with the connection        details for your cluster.        You must fill in a port number, which your network        administrator can provide, or enter one of the [default        ports](./index.html#default-ports) for {{site.terms.sep}} clusters.      * **User Name/Password**: Credentials to access your {{site.terms.sep}}        coordinator. If no authentication is configured on the coordinator,        leave the password field blank. You must still specify a username so        {{site.terms.sep}} can report the initiator for any queries.   c. Click **OK**.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/squirrel-sql-alias.png&#39;   img-id=&#39;SQuirrel SQL alias setup&#39;   alt-text=&#39;SQuirrel SQL alias setup&#39;   descr-text=&#39;Image depicting SQuirrel SQL alias setup&#39;   pxwidth=&#39;500&#39;   screenshot=&#39;true&#39;   %}5. (Optional) For a TLS-enabled cluster, add the `SSL=true` property and value.   a. Select **Modify** for your {{site.terms.sep}} alias.   b. Click **Properties**.   c. Select the **Driver properties** tab.   d. Right-click a header in the properties table, and select      **Add property**.    &amp;nbsp; {% include image.html    url=&#39;../../assets/img/general/squirrel-sql-connect.png&#39;    img-id=&#39;SQuirrel SQL connection prompt&#39;    alt-text=&#39;SQuirrel SQL connection prompt&#39;    descr-text=&#39;Image showing where to add a property to a SQuirrel SQL alias&#39;    pxwidth=&#39;300&#39;    screenshot=&#39;true&#39;    %}    e. Enter the following information in each field:      * **Property Name**: `SSL`      * **Property Value**: `true`      * **Property Description**: &quot;Set to true in order to enable TLS&quot;    f. Click **Add**.    g. Select the **Use driver properties** and **Specify** checkboxes.    h. Click **OK**, and **OK** again to save your changes to the alias.6. Test your connection:    a. Select the {{site.terms.sep}} alias.    b. Click the **Connect** button.    c. Enter your credentials in the **User** and **Password** fields.    d. Click **Connect**.{% include image.htmlurl=&#39;../../assets/img/general/squirrel-sql-properties.png&#39;img-id=&#39;SQuirrel SQL connection prompt&#39;alt-text=&#39;SQuirrel SQL connection prompt&#39;descr-text=&#39;Image depicting the SQuirrel SQL connection prompt&#39;pxwidth=&#39;500&#39;screenshot=&#39;true&#39;%}{% include note.html  content=&quot;If the connection appears to be successful but the returned  object doesn&#39;t contain any tables, this likely indicates that your client has  reached the cluster but is unable to establish a connection. Check the error  logs, your credentials, port information, and TLS configuration to  troubleshoot.&quot;%}## HTTPS/TLSAny {{site.terms.sep}} cluster that requires authentication is also required touse [TLS/HTTPS](../../latest/security/tls.html). If you&#39;re using globallytrusted certificate best practices, use the cluster&#39;s HTTPS URL in thedriver and alias URLs as shown in the steps above.If you&#39;re not using a globally trusted certificate, you may have to configurethe trust store on your client machine. Consult your site&#39;s networkadministrators for guidance.To use TLS, you must specify the JDBC parameter setting `SSL=true` as shown inthe steps above.## QueryingClick on the {{site.terms.sep}} alias in the **Aliases** tab to connect to your{{site.terms.sep}} coordinator. The initial connection downloads all metadataabout catalogs, schema, tables, columns and more.{% include warning.html  content=&quot;This download can take a significant amount of  time for large catalogs (thousands of tables or more).&quot;%}Under the **Objects** tab, youcan browse your data sources and their metadata.{% include image.html  url=&#39;../../assets/img/general/squirrel-sql-objects.png&#39;  alt-text=&#39;SQuirrel SQL database navigator&#39;  descr-text=&#39;SQuirrel SQL database navigator&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}Under the **SQL** tab, write and execute queries to inspect your tables&#39;contents.{% include image.html  url=&#39;../../assets/img/general/squirrel-sql-query.png&#39;  alt-text=&#39;SQuirrel SQL editor&#39;  descr-text=&#39;SQuirrel SQL editor&#39;  pxwidth=&#39;800&#39;  screenshot=&#39;true&#39;%}"
 },
 {
  "title": "User personas",
  "url": "/get-started/starburst-personas.html",
  "content": "# {{ page.title }}No matter what their job title is, most {{site.terms.sb}} users fit one of ouruser personas:* *Data consumers* query data through existing catalogs.* *Data engineers* create catalogs that connect data sources to Starburst.* *Platform administrators* run and maintain the Starburst cluster.These personas embody a set of focused workflows and assume a nominal skillset. Your role may comprise some or all of one or more of these personas&#39;workflows, and that is ok! In fact, we can easily describe this overlap:* All users in the course of their job are data consumers at some level.* Data engineers might do a bit of platform administration.* Platform administrators might do a bit of data engineering.Rather than repeat information or throw every possible workflow at every user,{{site.terms.sb}} user guides are organized to answer a very important question:&quot;Where do I start?!&quot; We use personas to accomplish this.Let&#39;s meet the {{site.terms.sb}} personas so that you can start with the oneclosest to your day-to-day workflows.## Data consumerData analysts, data scientists, and casual report wranglers all use{{site.terms.sb}} in very similar ways. In a nutshell, a data consumer focuseson one or more of the following:* Delivering visualizations and reports.* Making well-informed, data-driven decisions.* Creating forecast and machine learning models that describe the business.* Performing ad hoc analyses.In our guides, we assume a reasonable level of skill with[SQL](../glossary.html#sql), including some knowledge of more advanced queries,and some combination of the following:* Limited to reasonable programming skills* Excellent spreadsheet skills, including some modeling* Knowledge of statistical methods and/or machine learning* Competence with data visualization and/or reporting tools* Ability to detect and articulate issues with data, even if unable to  remedy them or trace the causeIt&#39;s worth noting that downstream users who only consume data through generatedreports and visualizations and don&#39;t actively query data themselves directly aredirect customers of data consumers. Our documentation does not teach basic SQLskills.Our [data consumer user guide](../data-consumer/index.html) provides muchinformation and in-depth training that covers:* Starburst clients* Query federation* Query optimization* Migrating queries to Starburst SQL              Learn more from the data consumer guides                          ## Data engineerData engineers deliver data to data consumers in a performant and suitableformat, and in a timely manner with an expectation of a given level of dataquality. Often they source new data from a variety of relational databases,object stores, log entries, message streams or product endpoints. In a nutshell,a data engineer focuses on:* Creating and updating data models &amp; data schemas.* Building and managing ETLs.* Identifying, designing, and implementing internal process improvements such as  automating manual processes, optimizing data delivery for greater scalability.* Building and managing streaming data pipelines.* Building data integrations between various 3rd party systems such as Salesforce  and Workday.Data engineers are customers to platform administrators and the servicesthey provide. Data consumers are the direct customers of data engineers.{{site.terms.sb}} lets data engineers decouple compute from and simplifiesdelivering the data that your users need. Our [data engineer userguides](../data-engineer/index.html) gets you started with detailed information andtraining on topics such as:* Creating catalogs to connect data sources.* Diagnosing and fixing query performance issues.* Developing custom connectors.              Learn more from the data engineer guides                          ## Platform administrator{{site.terms.sb}} platform administrators care about the scalability,performance and reliability of the {{site.terms.sb}} cluster. They balance SLAsfor both data landing times and availability; implement access and datagovernance policies; and support audit requirements. In a nutshell, platformadministrators focus on:* Building and maintaining scalable data platform architectures to support the  ingest, storage and querying of large heterogenous datasets.* Creating and monitoring cluster health metrics to ensure optimal performance  and reduce any downtime.* Implementing and working with cloud providers like AWS, Azure, and Google  Cloud.{{site.terms.sb}} runs on [COTS](../glossary.html#cots) hardware, and uses memoryinstead of disk, making it fast and more cost-effective. We&#39;ve put together muchin-depth information and training in our [platform administratorguide](../platform-administrator/index.html) for you,covering:* Security* Performance tuning* Cluster setup and configuration              Learn more from the platform administrator guide                          "
 },
 {
  "title": "Using SQL in Starburst",
  "url": "/data-consumer/starburst-sql.html",
  "content": "# {{ page.title }}{{site.terms.oss}}&#39;s open source distributed SQL engine runs fast analyticqueries against various data sources ranging in size from gigabytes topetabytes. {{site.terms.sb}} brings that SQL engine to even more data sources,with more robust features. Because {{site.terms.sb}}&#39;s SQL is ANSI-compliant andsupports most of the SQL language features you depend on, you can hit the groundrunning.Business intelligence users and data scientists can continue to use theirfavorite [client tools](./clients/index.html) such as Tableau, Qlik and ApacheSuperset to access and analyze virtually any data source, or multiple datasources in a single query.## The basicsWe know you want to jump right in, and we know you already have awesomeanalytics skills. It&#39;s just a matter of harnessing the power of{{site.terms.sb}} products to take your analytics even further. With that inmind, you can browse our latest reference materials to learn just how familiar{{site.terms.sb}} SQL is:* [SQL language](../latest/language.html)* [SQL statement syntax](../latest/sql.html)* [Functions and operators](../latest/functions.html)### CatalogsOne key difference worth highlighting is the concept of *catalogs*. Each of yourdata sources is defined as a catalog in {{site.terms.sb}}, and that catalog inturn contains schemas. Using a SQL client such as our CLI, you can discover whatcatalogs are available:```sqlpresto&gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows)```From there, you can use the familiar `SHOW SCHEMAS` command to drill furtherdown.### Fully-qualified table namesTable names are fully qualified when they include the catalog name:```..```This becomes critical when creating [federated queries](./data-mesh.html).### General SQL featuresJust in case you&#39;d like a review, here&#39;s a walkthrough of some basic SQLfeatures in {{site.terms.sb}} from one of our founders, David Phillips:             {% include youtubeSnippet.html id=page.youtubeId1 start=585 end=1060 %}              Topics:              Formatting        CASE and searched CASE expressions        IF expressions        TRY expressions        Lambda expressions            Click on the links to read more on that topic in our reference manual.      Running time: ~8 min.      ## Advanced SQLReady to move past the basics? For your convenience, we&#39;ve divided the AdvancedSQL for {{site.terms.sb}} video training course up into topic sections, andprovided links to the relevant parts of our documentation below.### Advanced aggregation techniques             {% include youtubeSnippet.html id=page.youtubeId1 start=1970 end=3654 %}              Topics:              count() with DISTINCT        Approximations, including counting and percentiles        max_by() values        Pivoting with count_if() and FILTER        Complex aggregations        Checksums        ROLLUP        CUBE        GROUPING SETS            Running time: ~28 min.      ### Window functions             {% include youtubeSnippet.html id=page.youtubeId1 start=5280 end=6768 %}              Topics:              Row numbering        Ranking        Ranking and numbering without ordering        Bucketing and percentage ranking        Partitioning        Accessing leading and training rows with lead() and lag()         Window frames        Accessing first, last and Nth values        ROWS vs RANGE using array_agg()        Using aggregations in window functions            Running time: ~25 min.      ### Array and map functionsMany data stores allow to to create arrays, but it isn&#39;t always easy.{{site.terms.sb}} allows you to easily create arrays and maps with your data.Creating arrays with your data is easy:```sqlSELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]```SQL array indexes are 1-based. Learn more about how to use and manipulate themin this in-depth video.             {% include youtubeSnippet.html id=page.youtubeId1 start=4067 end=5214 %}              Topics:              Accessing array and map elements with element_at()        Sorting arrays with array_sort()        matching elements with any_match(), all_match() and none_match()        Filtering elements        Transforming elements        Converting arrays to strings        Computing array products        Unnesting arrays and maps        Creating maps from keys and values and an array of entry rows            Running time: ~19 min.      ### Using JSON             {% include youtubeSnippet.html id=page.youtubeId1 start=1116 end=1969 %}              Topics:              The JSON data type        Extraction using json_extract() and json_extract_scalar()        Casting and partial casting from JSON        Formatting as JSON            Running time: ~14 min.      "
 },

 {
  "title": "Apache Superset",
  "url": "/data-consumer/clients/superset.html",
  "content": "# {{ page.title }}You can use [Apache Superset™](https://superset.apache.org/) as well as thehosted cloud service [Preset Cloud](https://preset.io/product/) with{{site.terms.sep_first}} to explore and visualize your data. Superset is a dataexploration and visualization web application. It is typically run on a serverand used by many users accessing the application. It enables users to write SQLqueries, create new tables, visualizations and dashboards and download data intofiles.  {% include video.html content=&quot;Check out the Trino CommunityBroadcast interview with the Apache Superset team to learn more and see ademo.&quot; %}## RequirementsApache Superset has built in support for {{site.terms.sep}}.{{site.terms.sep}} Versions 350-e and higher can use the preferred mechanism ofa [trino connection](https://superset.apache.org/docs/databases/trino). It ispowered by the [SQLAlchemy driver](https://pypi.org/project/sqlalchemy-trino/)and allows full usage of SQL as supported by {{site.terms.oss}}, and therefore{{site.terms.sep}}.Older versions need to use the legacy support with [prestoconnection](https://superset.apache.org/docs/databases/presto). It uses the[PyHive driver](https://pypi.org/project/PyHive/) and therefore does not supportall supported SQL syntax.{% include caution.html content=&quot;Users are strongly advised to upgrade toSEP 350-e or higher to take advantage of the superior Trino support.&quot; %}## ConnectionGet the necessary [connection information](./index.html#connection-information)for your cluster:* Username* Password* Host* PortSuperset understands the {{site.terms.sep}} concept of catalogs. You can createa connection with or without a specified catalog:* trino://{username}:{password}@{hostname}:{port}/{catalog}* trino://{username}:{password}@{hostname}:{port}Once you have gathered this information, you can [create a newconnection](https://superset.apache.org/docs/creating-charts-dashboards/first-dashboard#connecting-to-a-new-database)in Superset:* Select **Data &gt; Databases**, and click the **+ Database** button.* Enter the connection string formed from your connection information.* Test the connection, if desired.* Click **Add**.## TLS/HTTPSAny {{site.terms.sep}} cluster that requires authentication is required to useTLS/HTTPS. If the best practice of using a globally trusted certificate isimplemented, you can use the HTTPS URL of the cluster in the connectionstring.If the certificate is not globally trusted, you need to [configure the{{site.terms.sep}} certificate for TLS/SSL access from Superset on the Supersetserver](https://superset.apache.org/docs/databases/extra-settings).## AuthenticationYou can configure the same authentication provider for Superset and{{site.terms.sep}}. As a result, the same credentials can be used to log in toeither system.Authentication providers supported by both systems include LDAP and OAuth2:* [Apache Superset](https://superset.apache.org/docs/security)* [{{site.terms.sep_full}}](../../latest/security.html)## Resources* [Apache Superset website](https://superset.apache.org/)* [Apache Superset documentation](https://superset.apache.org/docs/intro)* [Preset](https://preset.io/), Powerful, easy to use data exploration and  visualization platform, powered by Apache Superset* [Trino Community Broadcast interview with the Apache Superset team, including  live demo](https://trino.io/episodes/12.html)"
 },
 {
  "title": "Starburst Support",
  "url": "/support.html",
  "content": "                    {{site.terms.support}}        Don&#39;t panic. Help is just a click away!                                      We love our customers!      At {{site.terms.sb}}, we offer our customers dedicated resources      for your organization with up to a 30 minute response time and 24×7      support from our team of experts.      Not a customer just yet? When you are ready to engage, Starburst is      here to help you chart a course, launch successfully, check in      regularly while you get started, and when you are in production.      Check it out!                                Get help with {{site.terms.sep_full}}                                                                              Your support cases              Our intuitive support portal offers customers fast ticket              submission and tracking, user-friendly forums, a knowledge              base, and easy downloads.                                                                                                    Our support policies              Learn more about support with fast response times for problems              and critical issues. Our expert engineers are ready to help you              whenever needed.                                                                                                    No login, no problem!              If you do not have a support portal login, contact us! And take              advantage of all the useful resources available in the              documentation.                                                              {{site.terms.sep_full}} releases and downloads                                                                              Supported releases              Learn about what versions we support, our release cycles,              and important support dates for specific versions.                                                                                                    Latest downloads              Get the latest long-term support (LTS) and short-term support              (STS) versions, and their clients.                                                             Use {{site.terms.sb}} on any of these ecosystems:                                                              Amazon AWS                                                                        Google Cloud                                                                        Microsoft Azure                                                                        Red Hat OpenShift                              "
 },
 {
  "title": "Tableau",
  "url": "/data-consumer/clients/tableau.html",
  "content": "# {{ page.title }}[Tableau](https://www.tableau.com) is a popular analytics tool with powerfuldata visualization capabilities. There are three ways to connect recent releasesof Tableau products to recent releases of a {{site.terms.sep_first}} cluster:  * **[Tableau data connector](#tableau-connector-with-jdbc-driver)**: The  recommended connection method uses a Tableau data connector file paired with  the {{site.terms.sb}} JDBC driver.* **[ODBC connection](#odbc-driver-connection)**: {{site.terms.sb}} also  supports connecting to {{site.terms.sep}} clusters with the {{site.terms.sb}}  ODBC driver.* **[Legacy JDBC connection](#legacy-jdbc-driver-connection)**: For sites using  a plain JDBC connection using the legacy &quot;Other Databases (JDBC)&quot; method,  {{site.terms.sb}} strongly recommends migrating to the **[Tableau data  connector](#tableau-data-connector-with-jdbc-driver)** method, but  {{site.terms.sb}} also supports continued use of the legacy method.## Before you begin1. Determine the [connection information](./index.html#connection-information)   for your {{site.terms.sep}} cluster, including its network name, listening   port (or [default port](./index.html#default-ports)), and login credentials.2. Instructions to connect Tableau products to {{site.terms.sep}} clusters vary   slightly, depending on the {{site.terms.sep}} version. See [Determine cluster   version](./index.html#determine-cluster-version).## Tableau data connector with JDBC driverYou can connect recent releases of Tableau Desktop to recent {{site.terms.sep}}clusters with a combination of a Tableau data connector file and JDBC driver.This method requires:* {{site.terms.sep}} 354-e or later* Tableau Desktop 2020.4 or laterNote that a Tableau data connector is not the same as an {{site.terms.sep}}Connector. The Tableau data connector is a bridge between Tableau and thestandard {{site.terms.sb}} JDBC driver. The two files work together to enableread-only access between Tableau and one or more {{site.terms.sep}} clusters.The Tableau data connector is a JAR file with name similar to``StarburstEnterprise.taco``.The procedure to install and use the Tableau data connector for{{site.terms.sep}} depends on your Tableau Desktop version:* For Tableau Desktop 2021.2.0 and later, select {{site.terms.sep_full}}  directly from the **To a Server** menu and follow the [standard data connector  procedure](#standard-data-connector-procedure).* For Tableau Desktop 2020.4 to 2021.1, you must first download the data  connector file yourself, as described in  [download data connector](#download-data-connector).## Standard data connector procedureFor Tableau Desktop 2021.2.0 or later, follow these steps:1. If Tableau Desktop is open, close it and exit.2. Download the latest {{site.terms.sb}} [JDBC   driver](../../latest/installation/jdbc.html).3. Place the JDBC driver file in the Tableau drivers directory:        Windows     C:Program FilesTableauDrivers     macOS     /Users/username/Library/Tableau/Drivers      Do not store more than one {{site.terms.sb}} JDBC driver in this directory.   Delete any older drivers when you update to a newer version. Connections to   all {{site.terms.sep}}-connected data sources are made through the   {{site.terms.sb}} JDBC driver.4. Start Tableau Desktop. In the left column, under **To a Server**, click   **More**.5. In the list of server types, select **{{site.terms.sep_full}} by Starburst**.   This opens a **Connector Details** dialog that describes the data connector.6. Click **Install**. This opens a connection dialog:   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_initial-login-screen.png&#39;   img-id=&#39;tableau-initial-login&#39;   alt-text=&#39;Tableau Connector login dialog&#39;   descr-text=&#39;Image depicting Tableau Connector login dialog&#39;   screenshot=&#39;true&#39;   %}7. Fill in the connection dialog with the following parameters:                            Field          Value                                      Server          Hostname or IP address of your SEP cluster.                          Port          Port used by your cluster.                          Authentication          Use the drop-down list to select Username for a              cluster with no authentication or Username and              Password for a cluster with TLS enabled and a              password-based authentication method such as LDAP.                          Username          Your SEP username.                          Password          (If selected) the password for the specified              Username.                          Require SSL          Select this check box if your cluster has TLS enabled. When              selected, the following field appears.                          SSL Verification          Select an entry in the drop-down list to specify how rigorously              the server’s certificate is to be validated.                     The values for **SSL Verification** have the following meanings:        FULL     Confirm that the certificate&#39;s validity is chained all the way back         to a root Certificate Authority (CA).         CA     Confirm that the certificate is valid as far back as the included         intermediate CA.         NONE     Confirm that the server&#39;s certificate matches the DNS name and private         key of the server.          8. When the connection is made, the **Connections** panel shows a list of the   catalogs (data sources) configured in your cluster.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_connections_catalogs.png&#39;   img-id=&#39;tableau-initial-catalogs-list&#39;   alt-text=&#39;Tableau Connector list of catalogs&#39;   descr-text=&#39;Image depicting Tableau Connector list of catalogs&#39;   screenshot=&#39;true&#39;   %}9. Select a catalog and subsequent schema from that catalog, to see the   available tables.   &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_connections_catalog_schema_table.png&#39;   img-id=&#39;tableau-list-of-tables&#39;   alt-text=&#39;Tableau list of tables&#39;   descr-text=&#39;Image depicting Tableau Connector list of tables&#39;   screenshot=&#39;true&#39;   %}If your cluster and catalogs support it, you can use the **Initial SQL** panelto specify one or more SQL statements to run on connection to the cluster. Seethe **Learn More** link in Tableau for information on the limitations of thisfeature.### Customized JDBC connectionsIf you need to set additional connection properties that are not included inTableau&#39;s connection dialog, customize the connection using a properties file.For more information, see [Customize JDBC Connections Using a PropertiesFile](https://community.tableau.com/s/question/0D54T00000F339uSAB/customize-jdbc-connections-using-a-properties-file?_ga=2.13850551.1453931082.1617841062-354148074.1617841061)in the Tableau Community and the [list of available parameters for the JDBCdriver](../../latest/installation/jdbc.html#parameter-reference).## {{site.terms.sb}} advantageRemember that {{site.terms.sep_full}} is not a database, it&#39;s a SQL queryengine that can connect to multiple data sources at the same time. Each{{site.terms.sep}} cluster can query multiple catalogs in a wide range ofdifferent data sources.Although Tableau is typically configured for one specific catalog and schema,it is possible to query more than one data source with a single Tableauconnection.To query multiple catalogs, select **New Custom SQL** in Tableau, and thenreference the fully-qualified name of any table in the cluster using the full`catalog.schema.table` syntax.The following example query accesses four catalogs: `postgresql`, `hive`,`mysql`, and `sqlserver`.```sqlSELECT c.custkey  , c.state  , c.estimated_income  , cp.customer_segment  , a.cc_number  , pp.cc_type  , a.mortgage_id  , a.auto_loan_idFROM postgresql.burst_bank.customer cJOIN hive.burst_bank.account a on c.custkey = a.custkeyJOIN mysql.burst_bank.product_profile pp on a.custkey = pp.custkeyJOIN sqlserver.burst_bank.customer_profile cp on c.custkey = cp.custkeyWHERE c.country = &#39;US&#39;AND c.state NOT IN (&#39;AA&#39;, &#39;AE&#39;, &#39;AP&#39;)```This approach is faster because all data access is managed by{{site.terms.sep}}, and is executed on the cluster. Tableau can also join datadirectly from multiple data sources, but this creates an unnecessary workload onTableau, and can negatively impact Tableau performance.## Download data connectorFor Tableau Desktop 2020.4 through 2021.1, the Tableau data connector for{{site.terms.sep_full}} does not appear automatically in the **AdditionalConnectors** list in the **To a Server** menu. To get {{site.terms.sep_full}} toappear in that list, either:* Upgrade Tableau Desktop to version 2021.2.0 or later, or* Prepare your Tableau installation as described here before attempting to  connect to an {{site.terms.sep}} cluster.Follow these steps to manually download the Tableau data connector for{{site.terms.sep_full}}:1. Close and exit Tableau Desktop.2. From the [Starburst Enterprise   page](https://extensiongallery.tableau.com/connectors/274) of Tableau&#39;s   web-based Extension Gallery, download the Tableau data connector file whose   name is similar to ``StarburstEnterprise.taco``. Use the **Download** button   on the upper right of the page. This requires logging into Tableau&#39;s site   with a free login.3. Move the data connector file to:        Windows     C:UsersusernameDocumentsMy Tableau RepositoryConnectors     macOS     /Users/username/Documents/My Tableau Repository/Connectors   4. Proceed from here to the [standard data connector   procedure](#standard-data-connector-procedure).## Legacy JDBC driver connectionTableau provides a generic connection method titled **Other Databases (JDBC)**.Although {{site.terms.sb}} strongly recommends using a [Tableau dataconnector](#tableau-data-connector-with-jdbc-driver), this generic JDBCconnection method is still available. Follow these steps:1. Download the {{site.terms.sb}} JDBC driver according to your   {{site.terms.sep}} version.   * For {{site.terms.sep}} 354-e and later, download the [latest JDBC driver     version](../../latest/installation/jdbc.html).   * For {{site.terms.sep}} 350-e or older, download     [``presto-jdbc-350.jar``](../../350-e/installation/jdbc.html).2. Place the JDBC driver file in the Tableau drivers directory:        Windows     C:Program FilesTableauDrivers     macOS     /Users/username/Library/Tableau/Drivers      Do not store more than one {{site.terms.sb}} JDBC driver in this directory.   Delete any older drivers when you update to a newer version. Connections to   all {{site.terms.sep}}-connected data sources are made through the   {{site.terms.sb}} JDBC driver.3. Start Tableau Desktop and select **Connect to a server** using the   **Other Databases (JDBC)** connector in Tableau.4. Fill in the connection dialog as shown in the following table. For the URL   field, if you downloaded the Trino JDBC driver file to connect to newer   {{site.terms.sep}} versions, use a JDBC connection string in this format:   ```shell   jdbc:trino://cluster.example.com:8080/catalog?SSL=true   ```   If you downloaded the PrestoSQL JDBC driver file to connect to older   {{site.terms.sep}} versions, use a JDBC connection string in this format:   ```shell   jdbc:presto://cluster.example.com:8080/catalog?SSL=true   ```   For either driver, the JDBC connection string must include the initial   catalog to connect to. Once connected, you can select schemas and tables   within that catalog, or select a different catalog.   If your cluster has TLS enabled, append the property ``?SSL=true`` to the   connection string.                        Field         Value                                 URL         Full JDBC connection string for your cluster. Must include a catalog             name.                       Dialect         Must be ‘SQL92’                       Username         Your SEP username                       Password         Your SEP password                       Properties File         Specify or browse to the path of a JDBC properties file containing             further specifications for this connection. See            Customized JDBC Connections.                           &amp;nbsp; {% include image.html   url=&#39;../../assets/img/general/Tableau_other-databases-dialog-catalog.png&#39;   img-id=&#39;tableau-other-databases&#39;   alt-text=&#39;Tableau Other Databases login dialog&#39;   descr-text=&#39;Image depicting Tableau Other Databases login dialog&#39;   screenshot=&#39;true&#39;   %}## ODBC driver connectionContact [Starburst Support](../../support.html) to obtain access to the[Starburst ODBC driver](./odbc.html). The Presto ODBC driver provided by Tableauis not supported.{% include note.html content=&quot;Instead of connecting with ODBC, the preferredconnection method is to use the Tableau data connector plus JDBC driver, asdescribed in the preceding section, [standard data connectorprocedure](#standard-data-connector-procedure).&quot; %}Open Tableau and begin the ODBC configuration. On Tableau&#39;s startup page, select**Other Databases (ODBC)**, and configure as follows:* Driver: Starburst ODBC Driver* Username: ``* String Extras: `Driver=Starburst ODBC    Driver;Catalog=;Host=;Port=;`The `String Extras` field supports any of the ODBC connection properties fromthe Starburst ODBC driver.Select **Sign In** to establish the connection. If you are prompted for apassword, the {{site.terms.sep}} server has authentication enabled.The [Tableau ODBC documentation](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases.htm)contains further information."
 },
 {
  "title": "Metadata tags for page front matter",
  "url": "/internal/tag-list.html",
  "content": "# {{page.titles}}The tags in this document are adopted for use at this time. They are used asmetadata to collect and list pages.## searchindex tagIf set to `false`, the page is not included in the simple search, see`search.json`.## persona tagValues:* platform-administrator* data-engineer* data-consumer* data-leader## product tagValues:* sep* galaxy* marketplace* amazon-marketplace* azure-marketplace* google-marketplace* redhat-marketplace## media-type tagValues:* text* video* graphics* screenshot"
 },
 {
  "title": "Tags overview",
  "url": "/starburst-galaxy/metadata/tag-resources.html",
  "content": "# {{ page.title }}Tags are a label, or key-value pair, that you assign to organize and applymetadata to a [cluster](/starburst-galaxy/clusters.html).Create a tag by defining a key and assigning value to it. For example, define akey as a cost center and assign an associated region: `cost-center =boston`The {{site.terms.galaxy_first}} allows you to use tags from your AWS resources.Similar to AWS, you must manually assign a tag to a resource, it won&#39;tautomatically assign itself. Learn about [resource tagging on AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html).## Add tags to {{site.terms.galaxy}}There are two places where you can add tags:1. The **Saved items** page2. In the **Configure additional parameters** section of the    [cluster creation process](/starburst-galaxy/clusters.html)   under the **Add tags** section#### Add tags from the Saved items pageAdd tags from the **Saved items** page.1. On the **Saved items** page, click **+ New**.3. In the **Key** field, type to add a new key or select an existing key.4. Assign a **value** to your key.5. Click **Save**.Navigate to **Saved items** to edit, delete, orview the tag usage at any time.#### Add a tag during clusters creationWhen you create a cluster, you have the option to add existing tags to thecluster or create a new tag.1. On the **Clusters** page, select **+ New**.2. Open **Configure additional parameters (optional)** and    select **Create a new tag**.3. In the **Key** field, type to add a new key or select an existing key.4. Assign a **value** to your key.5. Click **Save**.Navigate to **Saved items** to edit, delete, orview the tag usage at any time.## Edit a tag or view tag usageNavigate to **Saved items** to edit, delete, orview the tag usage at any time.#### Edit a tagAny updates you make to a tag apply to all of the clusters that use that tag.1. Click **Menu** next to the tag you want to update and select **Edit tag**.2. Change the **Key** or **Value**.3. Click **Save tag** to input your updates.Your tag is updated and will apply the new tag info to the clusters that use it.#### View tag usageSee which clusters are associated with a tag. 1. Click **Menu** next to the tag whose usage you want to view    and select **View usage**.2. The tag usage displays and shows how many items use the tag.While you&#39;re viewing a tag, you have the option to remove items that use it."
 },
 {
  "title": "Universe Sitemap",
  "url": "/universe-sitemap.xml",
  "content": "            https://docs.starburst.io        {% for post in site.posts %}                    https://docs.starburst.io{{ post.url }}            {% endfor %}    {% for page in site.pages %}        {% if page.url == &#39;/search.json&#39;            or page.url == &#39;/sitemap.xml&#39;            or page.url == &#39;/universe-sitemap.xml&#39;            or page.url == &#39;/404.html&#39;            or page.url == &#39;/assets/css/style.css&#39;            or page.sitemap_exclude == &#39;y&#39;        %}             {% continue %}         {% endif %}                    https://docs.starburst.io{{ page.url | replace:&#39;/index.html&#39;,&#39;/&#39; }}            {% endfor %}"
 },
 {
  "title": "Verify server with Web UI",
  "url": "/starburst-enterprise/try/verify.html",
  "content": "# {{page.title}}To verify that your locally run {{site.terms.sep_full}} server is running, usethe Web UI.Open the Web UI---------------With any modern browser, go to [http://localhost:8080](http://localhost:8080).At the login screen, enter your current OS login name (or any string).{% include image.html  url=&#39;../../assets/img/general/web-ui-login.png&#39;  img-id=&#39;webuilogin&#39;  alt-text=&#39;WebUI login dialog&#39;  descr-text=&#39;Image depicting WebUI login dialog&#39;  pxwidth=&#39;250&#39;  screenshot=&#39;true&#39;%}The default Web UI screen shows the version number, environment, and uptime ofthe server. The statistics fields show zeros until a query is run against theserver.{% include image.html  url=&#39;../../assets/img/general/web-ui-empty.png&#39;  img-id=&#39;webuiempty&#39;  alt-text=&#39;Empty Web UI screen&#39;  descr-text=&#39;Empty Web UI screen&#39;  pxwidth=&#39;650&#39;  modal=&#39;true&#39;%}Run queries-----------To run queries against the local server with the {{site.terms.oss}} CLI, see[CLI](../../data-consumer/clients/cli)."
 },

 {
  "title": "",
  "url": "",
  "date": "",
  "content": ""
 }
]
